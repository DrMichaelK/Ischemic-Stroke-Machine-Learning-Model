{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2633de9f-40d1-42be-b895-fa9cebb9f188",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2633de9f-40d1-42be-b895-fa9cebb9f188",
        "outputId": "4ab7718f-ca83-4284-a5fc-34a6f6dd5cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.19.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.70.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigtable 2.26.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-iam 2.16.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-pubsub 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install datasets==2.20.0 pandas transformers jiwer==2.2.0 accelerate==0.34.2 wandb==0.12.0 huggingface-hub==0.23.4 python-dotenv torch numpy==1.26.3 librosa soundfile torchaudio protobuf==3.20 seaborn bitsandbytes  einops-exts transformers>=4.28.1 torch==2.0.1 pillow open_clip_torch>=2.16.0  sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5eeb1ed-ba46-4760-8940-6397b7cb4e2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5eeb1ed-ba46-4760-8940-6397b7cb4e2b",
        "outputId": "5bba7851-b41a-4e3a-da50-3f8ff3e562fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.3.1\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba3c611-b0bb-46ea-b097-729f4f20b5e9",
      "metadata": {
        "id": "bba3c611-b0bb-46ea-b097-729f4f20b5e9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset, Audio, concatenate_datasets\n",
        "import os\n",
        "import re\n",
        "import wandb\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from datasets import load_dataset, load_metric, Audio\n",
        "from transformers import (     Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,\n",
        "    Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
        ")\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from dotenv import load_dotenv\n",
        "from transformers import EarlyStoppingCallback\n",
        "from huggingface_hub import login\n",
        "import bitsandbytes\n",
        "# Set processors (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4552691c-b9c2-4d96-bf49-75d3495c9b6f",
      "metadata": {
        "id": "4552691c-b9c2-4d96-bf49-75d3495c9b6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25463fd0-939c-465b-889d-df9df15657cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "HF_API_TOKEN = \"\"\n",
        "\n",
        "login(HF_API_TOKEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaeebaad-ef12-488b-b016-d4873c275721",
      "metadata": {
        "id": "aaeebaad-ef12-488b-b016-d4873c275721",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b73d195-57fd-4d97-c204-51bf0ab33017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git-lfs\n",
            "  Downloading git_lfs-1.6-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Downloading git_lfs-1.6-py2.py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: git-lfs\n",
            "Successfully installed git-lfs-1.6\n"
          ]
        }
      ],
      "source": [
        "! pip install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b53fd8-ca72-4bee-a0b7-85c8e58d2b02",
      "metadata": {
        "id": "05b53fd8-ca72-4bee-a0b7-85c8e58d2b02",
        "outputId": "e7cb697b-6916-402a-8646-44527e43f809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-7-5bff6ae2d3bb>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-5bff6ae2d3bb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    = # ensure Git LFS is installed for handling large files\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# ensure Git LFS is installed for handling large files\n",
        "!git clone https://huggingface.co/KYAGABA/BRAINDATA23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc4ffcb-743e-4340-8b01-535ac1e81476",
      "metadata": {
        "id": "4dc4ffcb-743e-4340-8b01-535ac1e81476",
        "outputId": "4ca9ce7a-269c-4c23-de9f-7bae219194f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-03 09:49:13--  https://huggingface.co/KYAGABA/processed_patient_data/resolve/main/processed_images.zip\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.210.61, 13.35.210.66, 13.35.210.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.210.61|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/23/fa/23faa404fc748645236d09a8406dd1bf3fcf2d1c8ab48cfe44d875bf7ea45258/4375f58a5f2615aa97b0f39cd7381ac0a86720bb09764be4be48c8c08d2d4bda?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27processed_images.zip%3B+filename%3D%22processed_images.zip%22%3B&response-content-type=application%2Fzip&Expires=1730886553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDg4NjU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzIzL2ZhLzIzZmFhNDA0ZmM3NDg2NDUyMzZkMDlhODQwNmRkMWJmM2ZjZjJkMWM4YWI0OGNmZTQ0ZDg3NWJmN2VhNDUyNTgvNDM3NWY1OGE1ZjI2MTVhYTk3YjBmMzljZDczODFhYzBhODY3MjBiYjA5NzY0YmU0YmU0OGM4YzA4ZDJkNGJkYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=Ij60C5FIEZPTHbHDQwpQYNESZ4JzTNzrASSmY5QDTQGXVsNaJpwJ-XKpML9lowhvK%7ExKyrEW79RilXeqMMJvW4lkhGq0-RCUDik-X9kqM1VFDleDtXd7PxRGH16xitzGGsE%7EinyUiQnCBjWTp%7EyJlmAv%7EqwWQ4beQb3PhWOiGkZRstmGuTGUwlhYzwczGxzXihYbVlXcx5PErbCAmiylUPaJkO6dOruNPhm%7Ee8OQmVIzkQDJmeaenPlsB57I5ZtiOs9DjLlvtLnnfgiui5o8TvSuLU-91KotvwwwvrS8vubzaJKE%7E%7EMQcFTf4svN68Dw0M7VL5LL70JmtKW0j8VTAA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-11-03 09:49:13--  https://cdn-lfs-us-1.hf.co/repos/23/fa/23faa404fc748645236d09a8406dd1bf3fcf2d1c8ab48cfe44d875bf7ea45258/4375f58a5f2615aa97b0f39cd7381ac0a86720bb09764be4be48c8c08d2d4bda?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27processed_images.zip%3B+filename%3D%22processed_images.zip%22%3B&response-content-type=application%2Fzip&Expires=1730886553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDg4NjU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzIzL2ZhLzIzZmFhNDA0ZmM3NDg2NDUyMzZkMDlhODQwNmRkMWJmM2ZjZjJkMWM4YWI0OGNmZTQ0ZDg3NWJmN2VhNDUyNTgvNDM3NWY1OGE1ZjI2MTVhYTk3YjBmMzljZDczODFhYzBhODY3MjBiYjA5NzY0YmU0YmU0OGM4YzA4ZDJkNGJkYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=Ij60C5FIEZPTHbHDQwpQYNESZ4JzTNzrASSmY5QDTQGXVsNaJpwJ-XKpML9lowhvK%7ExKyrEW79RilXeqMMJvW4lkhGq0-RCUDik-X9kqM1VFDleDtXd7PxRGH16xitzGGsE%7EinyUiQnCBjWTp%7EyJlmAv%7EqwWQ4beQb3PhWOiGkZRstmGuTGUwlhYzwczGxzXihYbVlXcx5PErbCAmiylUPaJkO6dOruNPhm%7Ee8OQmVIzkQDJmeaenPlsB57I5ZtiOs9DjLlvtLnnfgiui5o8TvSuLU-91KotvwwwvrS8vubzaJKE%7E%7EMQcFTf4svN68Dw0M7VL5LL70JmtKW0j8VTAA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.155.68.103, 18.155.68.50, 18.155.68.65, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.155.68.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3678459786 (3.4G) [application/zip]\n",
            "Saving to: ‘processed_images.zip’\n",
            "\n",
            "processed_images.zi 100%[===================>]   3.43G  23.4MB/s    in 2m 30s  \n",
            "\n",
            "2024-11-03 09:51:43 (23.4 MB/s) - ‘processed_images.zip’ saved [3678459786/3678459786]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header=\"Authorization: Bearer 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\" \"https://huggingface.co/KYAGABA/processed_patient_data/resolve/main/processed_images.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3285b3-9d27-461e-9d68-614a4d6d8dba",
      "metadata": {
        "id": "5c3285b3-9d27-461e-9d68-614a4d6d8dba",
        "outputId": "566e24f9-f3c8-43ad-b280-117bd87a1d8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.3)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.5.1-py3-none-any.whl (890 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/890.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "# Define the path to your zip file and extraction folder\n",
        "zip_file_path = \"/content/processed_images.zip\"  # Replace with your zip file path\n",
        "extract_to = \"/content/extracted_folder\"  # Folder where you want to extract the contents\n",
        "\n",
        "# Create the extraction folder if it doesn't exist\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(f\"Extracted all files to {extract_to}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3c87-q1bsyA",
        "outputId": "f799299a-22be-466b-e6e4-6e3dbcf4c6fe"
      },
      "id": "f3c87-q1bsyA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted all files to /content/extracted_folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths to class folders and images\n",
        "class_paths = {\n",
        "    \"acute\": \"/content/extracted_folder/acute\",  # Replace with actual path\n",
        "    \"normal\": \"/content/extracted_folder/normal\",\n",
        "    \"chronic\": \"/content/extracted_folder/chronic\",\n",
        "    \"lacunar\": \"/content/extracted_folder/lacunar\"\n",
        "}\n",
        "\n",
        "# Define transforms for images\n",
        "image_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((112, 112)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.43216, 0.394666, 0.37645],\n",
        "            std=[0.22803, 0.22145, 0.216989]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Initialize video model\n",
        "video_model = models.video.r3d_18(pretrained=True)\n",
        "video_model.fc = torch.nn.Linear(video_model.fc.in_features, 512)  # Adjust embedding size\n",
        "video_model = video_model.to(device)\n",
        "\n",
        "# Initialize report generator (e.g., BioBART)\n",
        "report_generator_tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-v2-base\")\n",
        "report_generator = AutoModelForSeq2SeqLM.from_pretrained(\"GanjinZero/biobart-v2-base\").to(device)\n",
        "\n",
        "# Project image embeddings to match the report generator's expected embedding size\n",
        "class ImageToTextProjector(torch.nn.Module):\n",
        "    def __init__(self, image_embedding_dim, text_embedding_dim):\n",
        "        super(ImageToTextProjector, self).__init__()\n",
        "        self.fc = torch.nn.Linear(image_embedding_dim, text_embedding_dim)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the projector\n",
        "projector = ImageToTextProjector(512, report_generator.config.d_model).to(device)\n",
        "\n",
        "# Custom model that combines video embeddings and generates classification and report\n",
        "class CombinedModel(torch.nn.Module):\n",
        "    def __init__(self, video_model, report_generator, num_classes, projector):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.video_model = video_model\n",
        "        self.report_generator = report_generator\n",
        "        self.classifier = torch.nn.Linear(512, num_classes)  # Adjust input size based on video embeddings\n",
        "        self.projector = projector  # Project image embeddings to text embedding size\n",
        "\n",
        "    def forward(self, images, labels=None):\n",
        "        # Process video input\n",
        "        video_embeddings = self.video_model(images)  # Output shape: (batch_size, 512)\n",
        "\n",
        "        # Classification output\n",
        "        class_outputs = self.classifier(video_embeddings)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        # Project image embeddings to match report generator's expected input size\n",
        "        projected_embeddings = self.projector(video_embeddings)  # Shape: (batch_size, d_model)\n",
        "\n",
        "        # Expand dimensions to simulate sequence length of 1\n",
        "        encoder_inputs = projected_embeddings.unsqueeze(1)  # Shape: (batch_size, seq_len=1, d_model)\n",
        "\n",
        "        # Generate report using the report generator\n",
        "        if labels is not None:\n",
        "            # During training, compute loss using teacher forcing\n",
        "            outputs = self.report_generator(\n",
        "                inputs_embeds=encoder_inputs,\n",
        "                labels=labels\n",
        "            )\n",
        "            gen_loss = outputs.loss\n",
        "            generated_report = None  # Do not generate report during training\n",
        "        else:\n",
        "            # During inference, generate report\n",
        "            generated_report_ids = self.report_generator.generate(\n",
        "                inputs_embeds=encoder_inputs,\n",
        "                max_length=200,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            generated_report = report_generator_tokenizer.batch_decode(\n",
        "                generated_report_ids, skip_special_tokens=True\n",
        "            )\n",
        "            gen_loss = None\n",
        "\n",
        "        return class_outputs, generated_report, gen_loss\n",
        "\n",
        "# Initialize the combined model\n",
        "num_classes = len(class_paths)\n",
        "combined_model = CombinedModel(\n",
        "    video_model, report_generator, num_classes, projector\n",
        ").to(device)\n",
        "\n",
        "# Dataset class\n",
        "class MultiModalPatientDataset(Dataset):\n",
        "    def __init__(self, class_paths, image_transform, n_frames=16):\n",
        "        self.image_transform = image_transform\n",
        "        self.data = self._load_data(class_paths)\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def _load_data(self, class_paths):\n",
        "        data = []\n",
        "        for class_label, class_path in class_paths.items():\n",
        "            for patient_folder in os.listdir(class_path):\n",
        "                patient_path = os.path.join(class_path, patient_folder)\n",
        "                image_paths = [\n",
        "                    os.path.join(patient_path, f)\n",
        "                    for f in os.listdir(patient_path)\n",
        "                    if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "                ]\n",
        "                report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "                if os.path.exists(report_path) and image_paths:\n",
        "                    with open(report_path, \"r\") as file:\n",
        "                        report = file.read().strip()\n",
        "                    data.append(\n",
        "                        {\n",
        "                            \"images\": image_paths,\n",
        "                            \"report\": report,\n",
        "                            \"label\": class_label,\n",
        "                        }\n",
        "                    )\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_paths = item[\"images\"]\n",
        "        n_images = len(image_paths)\n",
        "\n",
        "        # Sample or pad images to fixed number of frames\n",
        "        if n_images >= self.n_frames:\n",
        "            # Evenly sample n_frames images\n",
        "            indices = np.linspace(0, n_images - 1, self.n_frames, dtype=int)\n",
        "            image_paths_sampled = [image_paths[i] for i in indices]\n",
        "        else:\n",
        "            # Pad the image list by repeating the last image\n",
        "            pad_count = self.n_frames - n_images\n",
        "            image_paths_sampled = image_paths + [image_paths[-1]] * pad_count\n",
        "\n",
        "        images = [\n",
        "            self.image_transform(Image.open(img_path).convert(\"RGB\"))\n",
        "            for img_path in image_paths_sampled\n",
        "        ]\n",
        "        images = torch.stack(images).permute(\n",
        "            1, 0, 2, 3\n",
        "        )  # Shape: (Channels, Frames, Height, Width)\n",
        "\n",
        "        # Tokenize target report for labels (BioBART)\n",
        "        target_inputs = report_generator_tokenizer(\n",
        "            item[\"report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        labels = target_inputs[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        label = torch.tensor(list(class_paths.keys()).index(item[\"label\"]), dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"images\": images,\n",
        "            \"label\": label,\n",
        "            \"labels\": labels,  # For Seq2Seq model, labels are tokenized target sequences\n",
        "            \"report\": item[\"report\"],  # Include report for evaluation\n",
        "        }\n",
        "\n",
        "# Initialize dataset and split\n",
        "dataset = MultiModalPatientDataset(\n",
        "    class_paths=class_paths, image_transform=image_transform, n_frames=16\n",
        ")\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [item[\"images\"] for item in batch]\n",
        "    labels = torch.stack([item[\"label\"] for item in batch])\n",
        "    seq_labels = [item[\"labels\"] for item in batch]\n",
        "    reports = [item[\"report\"] for item in batch]\n",
        "\n",
        "    # Pad sequences to the maximum length in the batch\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        seq_labels, batch_first=True, padding_value=report_generator_tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    # Stack images\n",
        "    images_stacked = torch.stack(images)\n",
        "\n",
        "    return {\n",
        "        \"images\": images_stacked,\n",
        "        \"label\": labels,\n",
        "        \"labels\": labels_padded,\n",
        "        \"report\": reports,\n",
        "    }\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 150\n",
        "learning_rate = 1e-5\n",
        "optimizer = AdamW(combined_model.parameters(), lr=learning_rate)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Metrics for evaluation\n",
        "# Metrics for evaluation\n",
        "accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
        "precision_metric = Precision(task='multiclass', average='macro', num_classes=num_classes).to(device)\n",
        "recall_metric = Recall(task='multiclass', average='macro', num_classes=num_classes).to(device)\n",
        "f1_metric = F1Score(task='multiclass', average='macro', num_classes=num_classes).to(device)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    combined_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = batch[\"images\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        decoder_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        class_outputs, _, gen_loss = combined_model(\n",
        "            images,\n",
        "            labels=decoder_labels,\n",
        "        )\n",
        "        class_loss = torch.nn.CrossEntropyLoss()(class_outputs, labels)\n",
        "\n",
        "        # Total loss (classification + report generation)\n",
        "        total_batch_loss = class_loss + gen_loss\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss += total_batch_loss.item()\n",
        "        scaler.scale(total_batch_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Validation and Test\n",
        "for loader, split in zip([val_loader, test_loader], [\"Validation\", \"Test\"]):\n",
        "    total_accuracy, total_precision, total_recall, total_f1, total_bleu = (\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "    )\n",
        "    combined_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader):\n",
        "            images = batch[\"images\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            reports = batch[\"report\"]\n",
        "\n",
        "            # Forward pass without labels (inference mode)\n",
        "            outputs, generated_reports, _ = combined_model(\n",
        "                images,\n",
        "            )\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            total_accuracy += accuracy_metric(preds, labels)\n",
        "            total_precision += precision_metric(preds, labels)\n",
        "            total_recall += recall_metric(preds, labels)\n",
        "            total_f1 += f1_metric(preds, labels)\n",
        "\n",
        "            # Evaluate the generated report using BLEU score\n",
        "            for ref_report, gen_report in zip(reports, generated_reports):\n",
        "                total_bleu += sentence_bleu(\n",
        "                    [ref_report.split()], gen_report.split()\n",
        "                )\n",
        "\n",
        "    num_batches = len(loader)\n",
        "    print(f\"{split} Accuracy: {total_accuracy / num_batches:.4f}\")\n",
        "    print(f\"{split} Precision: {total_precision / num_batches:.4f}\")\n",
        "    print(f\"{split} Recall: {total_recall / num_batches:.4f}\")\n",
        "    print(f\"{split} F1 Score: {total_f1 / num_batches:.4f}\")\n",
        "    print(f\"{split} BLEU Score (Generated Report): {total_bleu / num_batches:.4f}\")\n",
        "\n",
        "# Save the model if needed\n",
        "# torch.save(combined_model.state_dict(), 'combined_model.pth')\n",
        "# report_generator_tokenizer.save_pretrained(\"combined_multimodal_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STIdgMv31bE3",
        "outputId": "e6736de6-9fe4-451e-a50d-1ce56c6fcf5d"
      },
      "id": "STIdgMv31bE3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/150], Loss: 6.2981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/150], Loss: 4.4395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/150], Loss: 3.4345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/150], Loss: 2.9472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/150], Loss: 2.7561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/150], Loss: 2.5084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/150], Loss: 2.3457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/150], Loss: 2.2065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/150], Loss: 2.0803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/150], Loss: 2.0285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/150], Loss: 2.0182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/150], Loss: 1.8363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/150], Loss: 1.8062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/150], Loss: 1.7366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/150], Loss: 1.6481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/150], Loss: 1.5557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/150], Loss: 1.6025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/150], Loss: 1.5170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/150], Loss: 1.4614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/150], Loss: 1.4489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/150], Loss: 1.3783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/150], Loss: 1.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/150], Loss: 1.2525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/150], Loss: 1.1727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/150], Loss: 1.2146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/150], Loss: 1.2194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/150], Loss: 1.1173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/150], Loss: 1.0700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/150], Loss: 1.0805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/150], Loss: 0.9764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/150], Loss: 0.9945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/150], Loss: 0.9367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/150], Loss: 0.8886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/150], Loss: 0.8498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/150], Loss: 0.8241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/150], Loss: 0.7885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/150], Loss: 0.8706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/150], Loss: 0.7531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/150], Loss: 0.7696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/150], Loss: 0.7138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/150], Loss: 0.7799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/150], Loss: 0.7294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/150], Loss: 0.7164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/150], Loss: 0.6185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/150], Loss: 0.5905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/150], Loss: 0.6186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/150], Loss: 0.5100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/150], Loss: 0.7252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/150], Loss: 0.7815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/150], Loss: 0.6475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [51/150], Loss: 0.5809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [52/150], Loss: 0.5165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [53/150], Loss: 0.6660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [54/150], Loss: 0.5534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [55/150], Loss: 0.6132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [56/150], Loss: 0.5455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [57/150], Loss: 0.5741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [58/150], Loss: 0.5564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [59/150], Loss: 0.4301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [60/150], Loss: 0.4645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [61/150], Loss: 0.4715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [62/150], Loss: 0.5211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [63/150], Loss: 0.4065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [64/150], Loss: 0.5075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [65/150], Loss: 0.5681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [66/150], Loss: 0.4366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [67/150], Loss: 0.4645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [68/150], Loss: 0.4329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [69/150], Loss: 0.5534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [70/150], Loss: 0.4990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [71/150], Loss: 0.4432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [72/150], Loss: 0.3829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [73/150], Loss: 0.3268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [74/150], Loss: 0.3867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [75/150], Loss: 0.4775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [76/150], Loss: 0.4369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [77/150], Loss: 0.3998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [78/150], Loss: 0.3733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [79/150], Loss: 0.4621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [80/150], Loss: 0.3355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [81/150], Loss: 0.2839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [82/150], Loss: 0.3983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [83/150], Loss: 0.4609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [84/150], Loss: 0.3709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [85/150], Loss: 0.3357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [86/150], Loss: 0.2918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [87/150], Loss: 0.2640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [88/150], Loss: 0.3003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [89/150], Loss: 0.3294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [90/150], Loss: 0.3071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [91/150], Loss: 0.2965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [92/150], Loss: 0.2291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [93/150], Loss: 0.2401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [94/150], Loss: 0.2870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [95/150], Loss: 0.2570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [96/150], Loss: 0.2038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [97/150], Loss: 0.2381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [98/150], Loss: 0.2092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [99/150], Loss: 0.1895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/150], Loss: 0.1726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [101/150], Loss: 0.2599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [102/150], Loss: 0.2507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [103/150], Loss: 0.1871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [104/150], Loss: 0.1929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [105/150], Loss: 0.2305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [106/150], Loss: 0.2289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [107/150], Loss: 0.1798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [108/150], Loss: 0.1800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [109/150], Loss: 0.2478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [110/150], Loss: 0.1152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [111/150], Loss: 0.1518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [112/150], Loss: 0.1280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [113/150], Loss: 0.1569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [114/150], Loss: 0.1975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [115/150], Loss: 0.1161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [116/150], Loss: 0.2591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [117/150], Loss: 0.2138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [118/150], Loss: 0.1347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [119/150], Loss: 0.1800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [120/150], Loss: 0.1345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [121/150], Loss: 0.1800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [122/150], Loss: 0.1819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [123/150], Loss: 0.1152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [124/150], Loss: 0.1981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [125/150], Loss: 0.0993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [126/150], Loss: 0.0851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [127/150], Loss: 0.2301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [128/150], Loss: 0.1369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [129/150], Loss: 0.1960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [130/150], Loss: 0.1392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [131/150], Loss: 0.1331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [132/150], Loss: 0.1018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [133/150], Loss: 0.1070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [134/150], Loss: 0.1541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [135/150], Loss: 0.0926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [136/150], Loss: 0.0806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [137/150], Loss: 0.0858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [138/150], Loss: 0.0831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [139/150], Loss: 0.0798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [140/150], Loss: 0.0712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [141/150], Loss: 0.1117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [142/150], Loss: 0.0932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [143/150], Loss: 0.0736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [144/150], Loss: 0.0778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [145/150], Loss: 0.1102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [146/150], Loss: 0.0653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [147/150], Loss: 0.0956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [148/150], Loss: 0.0798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:17<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [149/150], Loss: 0.0869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:18<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [150/150], Loss: 0.0760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            " 22%|██▏       | 2/9 [00:05<00:17,  2.55s/it]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 9/9 [00:23<00:00,  2.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5556\n",
            "Validation Precision: 0.5370\n",
            "Validation Recall: 0.4537\n",
            "Validation F1 Score: 0.4815\n",
            "Validation BLEU Score (Generated Report): 0.4503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:51<00:00,  2.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5250\n",
            "Test Precision: 0.4750\n",
            "Test Recall: 0.4750\n",
            "Test F1 Score: 0.4667\n",
            "Test BLEU Score (Generated Report): 0.4127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_save_path = \"combined_multimodal_model\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Save the model's state_dict\n",
        "torch.save(combined_model.state_dict(), os.path.join(model_save_path, \"pytorch_model.bin\"))\n",
        "\n",
        "# Save the tokenizer\n",
        "report_generator_tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Save any custom configurations if necessary\n",
        "config = {\n",
        "    \"num_classes\": num_classes,\n",
        "    # Include other necessary configurations\n",
        "}\n",
        "import json\n",
        "with open(os.path.join(model_save_path, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l08KJ6QmQTIz"
      },
      "id": "l08KJ6QmQTIz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "4aab476f3cc3459789aff9e7b782d35f",
            "b98bb087ba41450d8c2b55e2e7c6dba2",
            "743adea0ea094ecd8cb99e205d722af0",
            "01cbbd23b1d747a99d06ba82bbe621e1",
            "7e6a921009464110a87887c0044e323d",
            "d8682fe007bf48f6b0d58e849d194fc9",
            "34e803869b984e11a15e4e8fbb39003b",
            "7a5943f7466a4fb99eff195af41214a5",
            "e68a2472e36a45d289b46eace8626a8f",
            "2aeafbce1c0d4ea986dbbb2ee712ae8d",
            "c525623b4f16408b8a5e800978fc2632",
            "52fecc4e20a04aecb67ab6c5dd7e4f7f",
            "37a612ddd86749c2b77e5ab613c7b627",
            "65149b73c4a24a5d8940bdb6fa62b5ff",
            "2c36978ec8c74bbaa779b2074c0422be",
            "d96096b5e76749348aacb17fcb159041",
            "02d00321720643fbb186fa50c199389f",
            "8a2d786652994bcba321f6275fefebbc",
            "5c151a87404e4a6f82a8b728a41dfe93",
            "0d2b81ca1d6043adb8952d72a8d1a215",
            "9eb498ce7f93411ea79db847e00661c6",
            "eb01d8caed9745c8b8f19f9f219e5ade",
            "3b32465add24494c9daccb8fa368ff06",
            "53e4599e8b8d494da7da6debe5a4b67d",
            "4d2a202af9e0415fb2e1ff26b085aedf",
            "8cbf03cca12f416d85d9cf09567f234b",
            "24b9f5a89e924897a74880a48a777491",
            "9c38d3a176ac488f938282b1de57fcdc",
            "33342a7ec1f946fa8c55b207f71f54b7",
            "8d426284d16f4d8fa9cca0524b0e4637",
            "818bc7bebd2848619496648303eb8155",
            "402d057a99804a6e95bd50c895f1dd84"
          ]
        },
        "id": "VpUWqyLiQu0S",
        "outputId": "fdd5560d-70a8-4bca-e7dd-c0c3a7e1ca59"
      },
      "id": "VpUWqyLiQu0S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4aab476f3cc3459789aff9e7b782d35f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paths to class folders and images\n",
        "class_paths = {\n",
        "    \"acute\": \"/content/extracted_folder/acute\",  # Replace with actual path\n",
        "    \"normal\": \"/content/extracted_folder/normal\",\n",
        "    \"chronic\": \"/content/extracted_folder/chronic\",\n",
        "    \"lacunar\": \"/content/extracted_folder/lacunar\"\n",
        "}\n",
        "\n",
        "# Define transforms for images (including augmentation)\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.43216, 0.394666, 0.37645],\n",
        "        std=[0.22803, 0.22145, 0.216989]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Define a separate transform for validation and test (without augmentation)\n",
        "image_transform_val = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.43216, 0.394666, 0.37645],\n",
        "        std=[0.22803, 0.22145, 0.216989]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Initialize video model\n",
        "video_model = models.video.r3d_18(pretrained=True)\n",
        "video_model.fc = torch.nn.Linear(video_model.fc.in_features, 512)  # Adjust embedding size\n",
        "video_model = video_model.to(device)\n",
        "\n",
        "# Initialize report generator (e.g., BioBART)\n",
        "report_generator_tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-v2-base\")\n",
        "report_generator = AutoModelForSeq2SeqLM.from_pretrained(\"GanjinZero/biobart-v2-base\").to(device)\n",
        "\n",
        "# Initialize paraphrase model for text augmentation\n",
        "paraphrase_tokenizer = AutoTokenizer.from_pretrained('ramsrigouthamg/t5_paraphraser')\n",
        "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained('ramsrigouthamg/t5_paraphraser').to(device)\n",
        "\n",
        "# Project image embeddings to match the report generator's expected embedding size\n",
        "class ImageToTextProjector(torch.nn.Module):\n",
        "    def __init__(self, image_embedding_dim, text_embedding_dim):\n",
        "        super(ImageToTextProjector, self).__init__()\n",
        "        self.fc = torch.nn.Linear(image_embedding_dim, text_embedding_dim)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)  # Added dropout for regularization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the projector\n",
        "projector = ImageToTextProjector(512, report_generator.config.d_model).to(device)\n",
        "\n",
        "# Custom model that combines video embeddings and generates classification and report\n",
        "class CombinedModel(torch.nn.Module):\n",
        "    def __init__(self, video_model, report_generator, num_classes, projector):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.video_model = video_model\n",
        "        self.report_generator = report_generator\n",
        "        self.classifier = torch.nn.Linear(512, num_classes)  # Adjust input size based on video embeddings\n",
        "        self.projector = projector  # Project image embeddings to text embedding size\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)  # Added dropout for regularization\n",
        "\n",
        "    def forward(self, images, labels=None):\n",
        "        # Process video input\n",
        "        video_embeddings = self.video_model(images)  # Output shape: (batch_size, 512)\n",
        "        video_embeddings = self.dropout(video_embeddings)  # Apply dropout\n",
        "\n",
        "        # Classification output\n",
        "        class_outputs = self.classifier(video_embeddings)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        # Project image embeddings to match report generator's expected input size\n",
        "        projected_embeddings = self.projector(video_embeddings)  # Shape: (batch_size, d_model)\n",
        "\n",
        "        # Expand dimensions to simulate sequence length of 1\n",
        "        encoder_inputs = projected_embeddings.unsqueeze(1)  # Shape: (batch_size, seq_len=1, d_model)\n",
        "\n",
        "        # Generate report using the report generator\n",
        "        if labels is not None:\n",
        "            # During training, compute loss using teacher forcing\n",
        "            outputs = self.report_generator(\n",
        "                inputs_embeds=encoder_inputs,\n",
        "                labels=labels\n",
        "            )\n",
        "            gen_loss = outputs.loss\n",
        "            generated_report = None  # Do not generate report during training\n",
        "        else:\n",
        "            # During inference, generate report\n",
        "            generated_report_ids = self.report_generator.generate(\n",
        "                inputs_embeds=encoder_inputs,\n",
        "                max_length=512,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            generated_report = report_generator_tokenizer.batch_decode(\n",
        "                generated_report_ids, skip_special_tokens=True\n",
        "            )\n",
        "            gen_loss = None\n",
        "\n",
        "        return class_outputs, generated_report, gen_loss\n",
        "\n",
        "# Initialize the combined model\n",
        "num_classes = len(class_paths)\n",
        "combined_model = CombinedModel(\n",
        "    video_model, report_generator, num_classes, projector\n",
        ").to(device)\n",
        "\n",
        "# Text augmentation function using paraphrasing model\n",
        "def augment_text(text):\n",
        "    # Prepare the text for the model\n",
        "    preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
        "    t5_input_text = \"paraphrase: \" + preprocess_text + \" </s>\"\n",
        "    tokenized_text = paraphrase_tokenizer.encode(t5_input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    # Generate paraphrase\n",
        "    generated_ids = paraphrase_model.generate(\n",
        "        tokenized_text,\n",
        "        num_beams=5,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        max_length=512,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    output = paraphrase_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return output\n",
        "\n",
        "# Image augmentation function\n",
        "def augment_image(img):\n",
        "    # Apply additional augmentations\n",
        "    if random.random() < 0.5:\n",
        "        img = ImageOps.mirror(img)\n",
        "    if random.random() < 0.5:\n",
        "        enhancer = ImageEnhance.Contrast(img)\n",
        "        img = enhancer.enhance(random.uniform(0.8, 1.2))\n",
        "    if random.random() < 0.5:\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(random.uniform(0.8, 1.2))\n",
        "    return img\n",
        "\n",
        "# Function to balance dataset by augmenting data and saving to new patient folders\n",
        "def balance_dataset(class_paths):\n",
        "    # Compute class counts\n",
        "    class_counts = {}\n",
        "    data = []\n",
        "    for class_label, class_path in class_paths.items():\n",
        "        if not os.path.exists(class_path):\n",
        "            continue\n",
        "        patient_folders = os.listdir(class_path)\n",
        "        class_counts[class_label] = len(patient_folders)\n",
        "        data.append((class_label, class_path, patient_folders))\n",
        "\n",
        "    # Determine maximum class count\n",
        "    max_count = max(class_counts.values())\n",
        "\n",
        "    # Augment data for classes with fewer samples\n",
        "    for class_label, class_path, patient_folders in data:\n",
        "        num_to_add = max_count - class_counts[class_label]\n",
        "        if num_to_add > 0:\n",
        "            print(f\"Augmenting {num_to_add} samples for class '{class_label}'\")\n",
        "            for _ in range(num_to_add):\n",
        "                # Select a random patient folder\n",
        "                patient_folder = random.choice(patient_folders)\n",
        "                original_folder_path = os.path.join(class_path, patient_folder)\n",
        "\n",
        "                # Create a new patient folder\n",
        "                augmented_folder_name = f\"{patient_folder}_aug_{random.randint(1000,9999)}\"\n",
        "                augmented_folder_path = os.path.join(class_path, augmented_folder_name)\n",
        "                os.makedirs(augmented_folder_path, exist_ok=True)\n",
        "\n",
        "                # Copy and augment images\n",
        "                image_files = [f for f in os.listdir(original_folder_path) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
        "                for image_file in image_files:\n",
        "                    original_image_path = os.path.join(original_folder_path, image_file)\n",
        "                    img = Image.open(original_image_path).convert(\"RGB\")\n",
        "                    img = augment_image(img)\n",
        "                    augmented_image_path = os.path.join(augmented_folder_path, image_file)\n",
        "                    img.save(augmented_image_path)\n",
        "\n",
        "                # Augment text report\n",
        "                original_report_path = os.path.join(original_folder_path, \"report.txt\")\n",
        "                if os.path.exists(original_report_path):\n",
        "                    with open(original_report_path, \"r\") as file:\n",
        "                        report = file.read().strip()\n",
        "                    augmented_report = augment_text(report)\n",
        "                    augmented_report_path = os.path.join(augmented_folder_path, \"report.txt\")\n",
        "                    with open(augmented_report_path, \"w\") as file:\n",
        "                        file.write(augmented_report)\n",
        "                else:\n",
        "                    print(f\"Report not found in {original_folder_path}, skipping report augmentation.\")\n",
        "\n",
        "# Balance the dataset by augmenting data\n",
        "balance_dataset(class_paths)\n",
        "\n",
        "# Now proceed with the dataset loading and training as before\n",
        "\n",
        "# Dataset class\n",
        "class MultiModalPatientDataset(Dataset):\n",
        "    def __init__(self, class_paths, image_transform, n_frames=16):\n",
        "        self.image_transform = image_transform\n",
        "        self.n_frames = n_frames\n",
        "        self.data = self._load_data(class_paths)\n",
        "\n",
        "    def _load_data(self, class_paths):\n",
        "        data = []\n",
        "        for class_label, class_path in class_paths.items():\n",
        "            if not os.path.exists(class_path):\n",
        "                continue\n",
        "            for patient_folder in os.listdir(class_path):\n",
        "                patient_path = os.path.join(class_path, patient_folder)\n",
        "                image_paths = [\n",
        "                    os.path.join(patient_path, f)\n",
        "                    for f in os.listdir(patient_path)\n",
        "                    if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "                ]\n",
        "                report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "                if os.path.exists(report_path) and image_paths:\n",
        "                    with open(report_path, \"r\") as file:\n",
        "                        report = file.read().strip()\n",
        "                    data.append(\n",
        "                        {\n",
        "                            \"images\": image_paths,\n",
        "                            \"report\": report,\n",
        "                            \"label\": class_label,\n",
        "                        }\n",
        "                    )\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_paths = item[\"images\"]\n",
        "        n_images = len(image_paths)\n",
        "\n",
        "        # Sample or pad images to fixed number of frames\n",
        "        if n_images >= self.n_frames:\n",
        "            # Evenly sample n_frames images\n",
        "            indices = np.linspace(0, n_images - 1, self.n_frames, dtype=int)\n",
        "            image_paths_sampled = [image_paths[i] for i in indices]\n",
        "        else:\n",
        "            # Pad the image list by repeating the last image\n",
        "            pad_count = self.n_frames - n_images\n",
        "            image_paths_sampled = image_paths + [image_paths[-1]] * pad_count\n",
        "\n",
        "        images = []\n",
        "        for img_path in image_paths_sampled:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            img = self.image_transform(img)\n",
        "            images.append(img)\n",
        "        images = torch.stack(images).permute(1, 0, 2, 3)  # Shape: (Channels, Frames, Height, Width)\n",
        "\n",
        "        # Tokenize target report for labels (BioBART)\n",
        "        report = item[\"report\"]\n",
        "        target_inputs = report_generator_tokenizer(\n",
        "            report,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        labels = target_inputs[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        label = torch.tensor(list(class_paths.keys()).index(item[\"label\"]), dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"images\": images,\n",
        "            \"label\": label,\n",
        "            \"labels\": labels,  # For Seq2Seq model, labels are tokenized target sequences\n",
        "            \"report\": report,  # Include report for evaluation\n",
        "        }\n",
        "\n",
        "# Initialize dataset and split\n",
        "dataset = MultiModalPatientDataset(\n",
        "    class_paths=class_paths, image_transform=image_transform, n_frames=16\n",
        ")\n",
        "\n",
        "# Get class labels for computing class weights\n",
        "class_labels = [item[\"label\"] for item in dataset.data]\n",
        "class_indices = [list(class_paths.keys()).index(label) for label in class_labels]\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(class_indices),\n",
        "    y=class_indices\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Update transforms for validation and test datasets\n",
        "val_dataset.dataset.image_transform = image_transform_val\n",
        "test_dataset.dataset.image_transform = image_transform_val\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [item[\"images\"] for item in batch]\n",
        "    labels = torch.stack([item[\"label\"] for item in batch])\n",
        "    seq_labels = [item[\"labels\"] for item in batch]\n",
        "    reports = [item[\"report\"] for item in batch]\n",
        "\n",
        "    # Pad sequences to the maximum length in the batch\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        seq_labels, batch_first=True, padding_value=report_generator_tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    # Stack images\n",
        "    images_stacked = torch.stack(images)\n",
        "\n",
        "    return {\n",
        "        \"images\": images_stacked,\n",
        "        \"label\": labels,\n",
        "        \"labels\": labels_padded,\n",
        "        \"report\": reports,\n",
        "    }\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 200\n",
        "learning_rate = 1e-4\n",
        "optimizer = AdamW(combined_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Loss function with class weights\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Metrics for evaluation\n",
        "accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
        "precision_metric = Precision(task='multiclass', average='macro', num_classes=num_classes).to(device)\n",
        "recall_metric = Recall(task='multiclass', average='macro', num_classes=num_classes).to(device)\n",
        "f1_metric = F1Score(task='multiclass', average='macro', num_classes=num_classes).to(device)\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    combined_model.train()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = batch[\"images\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        decoder_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with autocast():\n",
        "            # Forward pass\n",
        "            class_outputs, _, gen_loss = combined_model(\n",
        "                images,\n",
        "                labels=decoder_labels,\n",
        "            )\n",
        "            class_loss = criterion(class_outputs, labels)\n",
        "\n",
        "            # Total loss\n",
        "            alpha = 1.0\n",
        "            beta = 1.0\n",
        "            total_batch_loss = alpha * class_loss + beta * gen_loss\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(total_batch_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += total_batch_loss.item()\n",
        "\n",
        "        # Compute training accuracy\n",
        "        preds = torch.argmax(class_outputs, dim=1)\n",
        "        total_accuracy += accuracy_metric(preds, labels)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_accuracy = total_accuracy / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    # Validation step\n",
        "    combined_model.eval()\n",
        "    val_loss = 0\n",
        "    val_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images = batch[\"images\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            decoder_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                class_outputs, _, gen_loss = combined_model(\n",
        "                    images,\n",
        "                    labels=decoder_labels,\n",
        "                )\n",
        "                class_loss = criterion(class_outputs, labels)\n",
        "                total_batch_loss = alpha * class_loss + beta * gen_loss\n",
        "\n",
        "            val_loss += total_batch_loss.item()\n",
        "\n",
        "            preds = torch.argmax(class_outputs, dim=1)\n",
        "            val_accuracy += accuracy_metric(preds, labels)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = val_accuracy / len(val_loader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Save the best model\n",
        "        torch.save(combined_model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "combined_model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Test the model\n",
        "total_accuracy = 0\n",
        "total_precision = 0\n",
        "total_recall = 0\n",
        "total_f1 = 0\n",
        "total_bleu = 0\n",
        "smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "combined_model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = batch[\"images\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        reports = batch[\"report\"]\n",
        "\n",
        "        # Forward pass without labels (inference mode)\n",
        "        with autocast():\n",
        "            outputs, generated_reports, _ = combined_model(\n",
        "                images,\n",
        "            )\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        total_accuracy += accuracy_metric(preds, labels)\n",
        "        total_precision += precision_metric(preds, labels)\n",
        "        total_recall += recall_metric(preds, labels)\n",
        "        total_f1 += f1_metric(preds, labels)\n",
        "\n",
        "        # Evaluate the generated report using BLEU score\n",
        "        for ref_report, gen_report in zip(reports, generated_reports):\n",
        "            ref_tokens = ref_report.lower().split()\n",
        "            gen_tokens = gen_report.lower().split()\n",
        "            total_bleu += sentence_bleu(\n",
        "                [ref_tokens], gen_tokens, smoothing_function=smoothing_function\n",
        "            )\n",
        "\n",
        "num_batches = len(test_loader)\n",
        "print(f\"Test Accuracy: {total_accuracy / num_batches:.4f}\")\n",
        "print(f\"Test Precision: {total_precision / num_batches:.4f}\")\n",
        "print(f\"Test Recall: {total_recall / num_batches:.4f}\")\n",
        "print(f\"Test F1 Score: {total_f1 / num_batches:.4f}\")\n",
        "print(f\"Test BLEU Score (Generated Report): {total_bleu / num_batches:.4f}\")\n",
        "\n",
        "# Save the final model and tokenizer\n",
        "model_save_path = \"combined_multimodal_model\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "torch.save(combined_model.state_dict(), os.path.join(model_save_path, \"pytorch_model.bin\"))\n",
        "report_generator_tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Save the model configuration\n",
        "config = {\n",
        "    \"num_classes\": num_classes,\n",
        "    \"model_state_dict\": \"pytorch_model.bin\",\n",
        "    \"tokenizer\": \"report_generator_tokenizer\",\n",
        "    # Add other configuration parameters as needed\n",
        "}\n",
        "with open(os.path.join(model_save_path, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b818c2a2ec84fa5b2bcdaeff3e1951f",
            "b923f3e98eaa4b55ab57d3681b16a778",
            "c41114cae39c4749805eae6732908050",
            "7aaee5b635624a7e8a054348147a4dfc",
            "0d82a1c6f1d947cda5fd01856cafbf18",
            "8f22b1be2f844d88b3feb8655598f282",
            "514a1a57239e41d0ae5e85f3dfec87a6",
            "bb2a9f0f8d72468aaa128ef371f05b55",
            "f81dea79c2864b81a339f8aad14680e4",
            "f09a2f543a7142d3a69d191c89b1cfa6",
            "5cff68ba2354493f8345c7892c578266",
            "2933be84cbb94033a746d9526d114b4e",
            "440964b0315f4403a8a965080db0aeb9",
            "b81d0c2ec9da499c8794a61fb80f79f5",
            "edaf51af71054f92a2b5dd9089e2a6ba",
            "3394147e203f4b3897f966b08a07682c",
            "586aad82bbe742e5ba988356b95c8334",
            "b89abddffe9240c3ba9ea2f3a7c6106e",
            "4f710bfb411c493896fbe7b9904f1813",
            "29eb193602824589906983b3ea2186cc",
            "9628daf58fc746ada4b11fe5b3e089a4",
            "54039a27dcc843f1940a3e19ddb7c951",
            "60411715624948a69a700f9f2b67ac74",
            "2b1709ca0e9f4c2a9489848b3675f7e1",
            "18f70a526fa3459dafd23cb83148afba",
            "289d94a583104b98b2b3c89f8aa7034c",
            "72f3c1366ca24756815a77dc095ee56a",
            "062d907970c74093bc988e53bd03485e",
            "c63cc6f06f9b469e88f413ba323d1627",
            "a46148d9ff1d45809203955eaa15008f",
            "f68727d59ff34c28a6c705ebfc50fc41",
            "5e3e3832f6a34ccfb0cbac057528c862",
            "e38aeaa66b7b406991d7940e5e7e9223",
            "45eb1dd2f5d44304aa8077a739a4893f",
            "90a85571dd0647af8f74d09cfd836de1",
            "c5a28f7771a344c9bdfbd6b78a96dbbb",
            "aaf9e066b5e7466cb8b688140e35bff0",
            "e71bef3141974586a3b7a787db4aeb73",
            "a86b2416a0224e80a2ca73307d311c11",
            "340198907c4b47fc9cd43359d7827069",
            "fcc5b9c09219462dbf8fda0bff3b9599",
            "ba9e0b9affd2444fabe8c6a9b0b3b5d0",
            "b6991e5aa1cd420480b5e4d1bd657d51",
            "5ffc83bac84a452e8ed0dc0e5dfcc763",
            "b742378c928c405e823a20078f2337eb",
            "4084be2ed06241eda0853e1686d7af18",
            "b42ea3cb39bd4333b3121ae4d1843c2f",
            "a6b569fb3bb84e0fbf79f183583864e3",
            "7748bdff37b94b8ab64ecaffd6027c86",
            "7f655f7337cb401ca9f81f18980e64c6",
            "4089f0e8fc0e4af4a5471a49ed06e61c",
            "922fd44ed2fc4e56ac99ddc1b8d772f1",
            "86d2b4c060ce4488821d13a7f30f9c11",
            "96956ebbbd3d4e20a099e5070e1f1475",
            "b02ecb311e594ef18ac5f3df03f586f5"
          ]
        },
        "id": "GtPvMQcxSgAg",
        "outputId": "eadcd33b-7547-4cfc-8503-8980b4a15f13"
      },
      "id": "GtPvMQcxSgAg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b818c2a2ec84fa5b2bcdaeff3e1951f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2933be84cbb94033a746d9526d114b4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60411715624948a69a700f9f2b67ac74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45eb1dd2f5d44304aa8077a739a4893f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b742378c928c405e823a20078f2337eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting 41 samples for class 'acute'\n",
            "Report not found in /content/extracted_folder/acute/Patient 102, skipping report augmentation.\n",
            "Report not found in /content/extracted_folder/acute/Patient 102, skipping report augmentation.\n",
            "Report not found in /content/extracted_folder/acute/Patient 102, skipping report augmentation.\n",
            "Augmenting 81 samples for class 'chronic'\n",
            "Augmenting 75 samples for class 'lacunar'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 5.4422, Accuracy: 0.2836\n",
            "Validation Loss: 2.3737, Validation Accuracy: 0.6250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/200], Loss: 2.0340, Accuracy: 0.5896\n",
            "Validation Loss: 1.5090, Validation Accuracy: 0.8500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/200], Loss: 1.3082, Accuracy: 0.7799\n",
            "Validation Loss: 0.9240, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/200], Loss: 0.8905, Accuracy: 0.9104\n",
            "Validation Loss: 0.6909, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/200: 100%|██████████| 67/67 [00:37<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/200], Loss: 0.6360, Accuracy: 0.9328\n",
            "Validation Loss: 0.6000, Validation Accuracy: 0.9500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/200], Loss: 0.5596, Accuracy: 0.9254\n",
            "Validation Loss: 0.4856, Validation Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/200: 100%|██████████| 67/67 [00:37<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/200], Loss: 0.4388, Accuracy: 0.9590\n",
            "Validation Loss: 0.6593, Validation Accuracy: 0.9250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/200], Loss: 0.4377, Accuracy: 0.9216\n",
            "Validation Loss: 0.5710, Validation Accuracy: 0.9250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/200], Loss: 0.5107, Accuracy: 0.8806\n",
            "Validation Loss: 0.4869, Validation Accuracy: 0.9500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/200], Loss: 0.3924, Accuracy: 0.9403\n",
            "Validation Loss: 0.5190, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/200], Loss: 0.3175, Accuracy: 0.9403\n",
            "Validation Loss: 0.4369, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/200: 100%|██████████| 67/67 [00:37<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/200], Loss: 0.2027, Accuracy: 0.9925\n",
            "Validation Loss: 0.4338, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/200], Loss: 0.2056, Accuracy: 0.9776\n",
            "Validation Loss: 0.4243, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/200: 100%|██████████| 67/67 [00:37<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/200], Loss: 0.1658, Accuracy: 0.9963\n",
            "Validation Loss: 0.4652, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/200], Loss: 0.1615, Accuracy: 0.9813\n",
            "Validation Loss: 0.4222, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/200: 100%|██████████| 67/67 [00:37<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/200], Loss: 0.1551, Accuracy: 0.9888\n",
            "Validation Loss: 0.4053, Validation Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/200], Loss: 0.1326, Accuracy: 0.9888\n",
            "Validation Loss: 0.4167, Validation Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/200], Loss: 0.1208, Accuracy: 0.9963\n",
            "Validation Loss: 0.4403, Validation Accuracy: 0.9500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/200], Loss: 0.1276, Accuracy: 0.9888\n",
            "Validation Loss: 0.5303, Validation Accuracy: 0.9500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/200], Loss: 0.1465, Accuracy: 0.9813\n",
            "Validation Loss: 0.4290, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/200], Loss: 0.1170, Accuracy: 0.9925\n",
            "Validation Loss: 0.4449, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/200: 100%|██████████| 67/67 [00:36<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/200], Loss: 0.0968, Accuracy: 0.9963\n",
            "Validation Loss: 0.4742, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/200], Loss: 0.1787, Accuracy: 0.9701\n",
            "Validation Loss: 0.4502, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/200], Loss: 0.1645, Accuracy: 0.9701\n",
            "Validation Loss: 0.4423, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/200], Loss: 0.0955, Accuracy: 0.9963\n",
            "Validation Loss: 0.4495, Validation Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/200: 100%|██████████| 67/67 [00:36<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/200], Loss: 0.1189, Accuracy: 0.9813\n",
            "Validation Loss: 0.4292, Validation Accuracy: 0.9750\n",
            "Early stopping!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 20/20 [01:21<00:00,  4.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8875\n",
            "Test Precision: 0.8361\n",
            "Test Recall: 0.8424\n",
            "Test F1 Score: 0.8294\n",
            "Test BLEU Score (Generated Report): 0.1735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Define the path where you want to save your model\n",
        "model_save_path = \"combined_multimodal_model\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Save the model's state_dict\n",
        "torch.save(combined_model.state_dict(), os.path.join(model_save_path, \"pytorch_model.bin\"))\n",
        "\n",
        "# Save the tokenizer\n",
        "report_generator_tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Save the model configuration\n",
        "config = {\n",
        "    \"num_classes\": num_classes,\n",
        "    \"model_state_dict\": \"pytorch_model.bin\",\n",
        "    \"tokenizer\": \"report_generator_tokenizer\",\n",
        "    # Include any other necessary configurations\n",
        "}\n",
        "\n",
        "with open(os.path.join(model_save_path, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f)\n"
      ],
      "metadata": {
        "id": "1L4mo3zln86d"
      },
      "id": "1L4mo3zln86d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "username = \"KYAGABA\"  # Replace with your username\n",
        "repo_name = \"combined-multimodal-model\"  # Name for your model repo\n",
        "repo_id = f\"{username}/{repo_name}\"\n",
        "\n",
        "# Create the repository (set exist_ok=True to avoid errors if it already exists)\n",
        "api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "njVinddwoisE",
        "outputId": "fb355146-a8bd-4559-d8da-5ed85d532275"
      },
      "id": "njVinddwoisE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RepoUrl('https://huggingface.co/KYAGABA/combined-multimodal-model', endpoint='https://huggingface.co', repo_type='model', repo_id='KYAGABA/combined-multimodal-model')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=model_save_path,\n",
        "    path_in_repo=\"\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    ignore_patterns=[\"*.pyc\", \"__pycache__\"],\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "9d205de3f9fa4bf89b60b78de378bc48",
            "b0e169d9eeb54c32849ff320f613f62f",
            "8c9be25ff6d54d2fb6babc1b29cf7991",
            "c4feb16cac6a4d46861e6fe5fc9f5b4a",
            "4a6e06bd764c447eb2b34358b533aef7",
            "05eff3d2b46048ef8818aeb1ccd32de2",
            "02e92fbb3bd84766b0b2ef60516faed1",
            "2c22eeacdb6141d0bce256b8893fbc84",
            "15751a0d866548fcbde34e149b205370",
            "90fd85015024457aa98f32b3b7b75b2e",
            "2b3e005751404b20818f68d721bfdbd6"
          ]
        },
        "id": "ihTXCVK-RH9b",
        "outputId": "fe07724b-fccc-431c-e4ac-efcd887a73ba"
      },
      "id": "ihTXCVK-RH9b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/801M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d205de3f9fa4bf89b60b78de378bc48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/KYAGABA/combined-multimodal-model/commit/8d8805e0c719b9583d8cf26f2bb844b1dbc1efb5', commit_message='Upload folder using huggingface_hub', commit_description='', oid='8d8805e0c719b9583d8cf26f2bb844b1dbc1efb5', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "nxGAp-oisdcx"
      },
      "id": "nxGAp-oisdcx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "d586ba33fd7d4d96a1fdfb607acf3203",
            "c314d8aa493b4985a39c8f1f23c4974f",
            "8026fb3ec04a4af882e214fb59074677",
            "2908a3d1d1ef4d018fb396f18e62e483",
            "ca5c0d3febaf40938a2733000b971538",
            "e13c8c64c89e4497be871a151444c64a",
            "59da7e71ee1148cda8691d3aa2cc8d75",
            "392011f6c6d147ce8c3cc2c046b9f792",
            "28ff4396a7d845658dcba0298f731a06",
            "8cbb80f263324328a1c72fa3b7702d9f",
            "dbf69d1194284d9fbe4dd6788a22253e",
            "337b7060be854d24999b81419d1ab716",
            "8cd73d0a97344bc2ae87990887390bda",
            "14ab8a6552624ea6be419d28581ae334",
            "024e27ff65f14f1a87da7c66f8cf0add",
            "f1deacca8769482fbfc647f2423ad3dc",
            "328786752d9e479181eb46a23faf111b",
            "5f8dbf8d093142f9a8d96190efdb4010",
            "860f85caa09f4a3b90fe05e09a1119a7",
            "f4bc9785023e49ca8c74d60c8f664709",
            "091650b47cad473d8e1e59d1279ab9e1",
            "3a01a36f7fac4ce6b74aff3f58bc1cbf",
            "829a8ca70c5b4df9b049d33550a2aae5",
            "105e08b46f7141c7aaf78abac3e8ce48",
            "a3662fd300354dbcb628285a87cfa267",
            "ac68b443515142f598e77024c3659da7",
            "46bb7bedc1754799a17e830cbd806363",
            "9868d2afbc744636adb2c99b19bf1aa3",
            "845aec13579f43cf8d1ebdde07d7f7ab",
            "aa0953318413431f9dc56c6541c33719",
            "ae400cea311f4fa493fea95eb5fa1550",
            "b537e6908218460891073576f6c18edf"
          ]
        },
        "id": "Kl7L6Cv0taNJ",
        "outputId": "39727987-35af-4ba1-d5a3-9880c7448e20"
      },
      "id": "Kl7L6Cv0taNJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d586ba33fd7d4d96a1fdfb607acf3203"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from model import CombinedModel, ImageToTextProjector\n",
        "from torchvision import models\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"KYAGABA/combined-multimodal-model\")\n",
        "\n",
        "# Initialize models\n",
        "video_model = models.video.r3d_18(pretrained=True)\n",
        "video_model.fc = torch.nn.Linear(video_model.fc.in_features, 512)\n",
        "\n",
        "report_generator = AutoModelForSeq2SeqLM.from_pretrained(\"GanjinZero/biobart-v2-base\")\n",
        "\n",
        "projector = ImageToTextProjector(512, report_generator.config.d_model)\n",
        "\n",
        "num_classes = 4\n",
        "combined_model = CombinedModel(video_model, report_generator, num_classes, projector)\n",
        "\n",
        "# Load state dict\n",
        "state_dict = torch.hub.load_state_dict_from_url(\n",
        "    \"https://huggingface.co/KYAGABA/combined-multimodal-model/resolve/main/pytorch_model.bin\",\n",
        "    map_location=torch.device('cpu')\n",
        ")\n",
        "combined_model.load_state_dict(state_dict)\n",
        "combined_model.eval()\n",
        "\n",
        "# Use the model for inference\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "5fCXrRo4tOzm",
        "outputId": "ecfeb6c0-b8a9-4961-96e6-36b58c581c88"
      },
      "id": "5fCXrRo4tOzm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://huggingface.co/KYAGABA/combined-multimodal-model/resolve/main/pytorch_model.bin\" to /root/.cache/torch/hub/checkpoints/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 401: Unauthorized",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-c32b88be1607>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Load state dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m state_dict = torch.hub.load_state_dict_from_url(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;34m\"https://huggingface.co/KYAGABA/combined-multimodal-model/resolve/main/pytorch_model.bin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# r is Optional[Match[str]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mhash_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0mfile_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"torch.hub\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'getheaders'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 401: Unauthorized"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "# from huggingface_hub import login, HfApi  # Uncomment if needed\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths to class folders and images\n",
        "# Paths to class folders and images\n",
        "class_paths = {\n",
        "    \"acute\": \"/content/extracted_folder/acute\",  # Replace with actual path\n",
        "    \"normal\": \"/content/extracted_folder/normal\",\n",
        "    \"chronic\": \"/content/extracted_folder/chronic\",\n",
        "    \"lacunar\": \"/content/extracted_folder/lacunar\"\n",
        "}\n",
        "# Define transforms for images\n",
        "image_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((112, 112)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Initialize models\n",
        "video_model = models.video.r3d_18(pretrained=True)\n",
        "\n",
        "video_model.fc = torch.nn.Linear(video_model.fc.in_features, 256)  # Embedding layer\n",
        "video_model = video_model.to(device)\n",
        "\n",
        "# Tokenizers and models\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "text_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(device)\n",
        "\n",
        "report_generator_tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-v2-base\")\n",
        "report_generator = AutoModelForSeq2SeqLM.from_pretrained(\"GanjinZero/biobart-v2-base\").to(device)\n",
        "\n",
        "# Custom model that combines video, text, classification, and report generation\n",
        "class CombinedModel(torch.nn.Module):\n",
        "    def __init__(self, video_model, text_model, report_generator, num_classes):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.video_model = video_model\n",
        "        self.text_model = text_model\n",
        "        self.report_generator = report_generator\n",
        "        self.classifier = torch.nn.Linear(\n",
        "            256 + 768, num_classes\n",
        "        )  # Adjust input size based on output features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images,\n",
        "        text_input_ids,\n",
        "        text_attention_mask,\n",
        "        gen_input_ids=None,\n",
        "        gen_attention_mask=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        # Process video input\n",
        "        video_embeddings = self.video_model(images)\n",
        "\n",
        "        # Process text input with text_model\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=text_input_ids, attention_mask=text_attention_mask\n",
        "        )\n",
        "        text_embeddings = text_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Combine embeddings and classify\n",
        "        combined_embeddings = torch.cat((video_embeddings, text_embeddings), dim=1)\n",
        "        class_outputs = self.classifier(combined_embeddings)\n",
        "\n",
        "        # Generate report using the report generator\n",
        "        if labels is not None:\n",
        "            outputs = self.report_generator(\n",
        "                input_ids=gen_input_ids,\n",
        "                attention_mask=gen_attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "            gen_loss = outputs.loss\n",
        "            generated_report = None  # Do not generate report during training\n",
        "        else:\n",
        "            gen_loss = None\n",
        "            generated_report_ids = self.report_generator.generate(\n",
        "                input_ids=gen_input_ids,\n",
        "                attention_mask=gen_attention_mask,\n",
        "                max_length=100,\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            generated_report = report_generator_tokenizer.batch_decode(\n",
        "                generated_report_ids, skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "        return class_outputs, generated_report, gen_loss\n",
        "\n",
        "# Initialize the combined model\n",
        "num_classes = len(class_paths)\n",
        "combined_model = CombinedModel(\n",
        "    video_model, text_model, report_generator, num_classes\n",
        ").to(device)\n",
        "\n",
        "# Dataset class with summarization and fixed number of frames\n",
        "class MultiModalPatientDataset(Dataset):\n",
        "    def __init__(self, class_paths, image_transform, n_frames=128):\n",
        "        self.image_transform = image_transform\n",
        "        self.data = self._load_data(class_paths)\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def _load_data(self, class_paths):\n",
        "        data = []\n",
        "        for class_label, class_path in class_paths.items():\n",
        "            for patient_folder in os.listdir(class_path):\n",
        "                patient_path = os.path.join(class_path, patient_folder)\n",
        "                image_paths = [\n",
        "                    os.path.join(patient_path, f)\n",
        "                    for f in os.listdir(patient_path)\n",
        "                    if f.endswith(\".png\")\n",
        "                ]\n",
        "                report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "                if os.path.exists(report_path):\n",
        "                    with open(report_path, \"r\") as file:\n",
        "                        report = file.read().strip()\n",
        "                    # For simplicity, not summarizing the report\n",
        "                    summarized_report = report\n",
        "                    data.append(\n",
        "                        {\n",
        "                            \"images\": image_paths,\n",
        "                            \"report\": summarized_report,\n",
        "                            \"label\": class_label,\n",
        "                            \"raw_report\": report,  # Keep the original report for BLEU score\n",
        "                        }\n",
        "                    )\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_paths = item[\"images\"]\n",
        "        n_images = len(image_paths)\n",
        "\n",
        "        # Sample or pad images to fixed number of frames\n",
        "        if n_images >= self.n_frames:\n",
        "            # Evenly sample n_frames images\n",
        "            indices = np.linspace(0, n_images - 1, self.n_frames, dtype=int)\n",
        "            image_paths_sampled = [image_paths[i] for i in indices]\n",
        "        else:\n",
        "            # Pad the image list by repeating the last image\n",
        "            pad_count = self.n_frames - n_images\n",
        "            image_paths_sampled = image_paths + [image_paths[-1]] * pad_count\n",
        "\n",
        "        images = [\n",
        "            self.image_transform(Image.open(img_path).convert(\"RGB\"))\n",
        "            for img_path in image_paths_sampled\n",
        "        ]\n",
        "        images = torch.stack(images).permute(\n",
        "            1, 0, 2, 3\n",
        "        )  # Shape: (Channels, Frames, Height, Width)\n",
        "\n",
        "        # Tokenize report for text_model (BioBERT)\n",
        "        text_inputs = tokenizer(\n",
        "            item[\"report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        text_input_ids = text_inputs[\"input_ids\"].squeeze(0)\n",
        "        text_attention_mask = text_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Tokenize report for report_generator (BioBART)\n",
        "        gen_inputs = report_generator_tokenizer(\n",
        "            item[\"report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        gen_input_ids = gen_inputs[\"input_ids\"].squeeze(0)\n",
        "        gen_attention_mask = gen_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Tokenize target report for labels (BioBART)\n",
        "        target_inputs = report_generator_tokenizer(\n",
        "            item[\"raw_report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        labels = target_inputs[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        label = torch.tensor(list(class_paths.keys()).index(item[\"label\"]), dtype=torch.long)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"images\": images,\n",
        "            \"text_input_ids\": text_input_ids,\n",
        "            \"text_attention_mask\": text_attention_mask,\n",
        "            \"gen_input_ids\": gen_input_ids,\n",
        "            \"gen_attention_mask\": gen_attention_mask,\n",
        "            \"label\": label,\n",
        "            \"labels\": labels,  # For Seq2Seq model, labels are tokenized target sequences\n",
        "            \"raw_report\": item[\"raw_report\"],  # Include raw report for BLEU score\n",
        "        }\n",
        "\n",
        "# Initialize dataset and split\n",
        "dataset = MultiModalPatientDataset(\n",
        "    class_paths=class_paths, image_transform=image_transform, n_frames=128\n",
        ")\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [item[\"images\"] for item in batch]\n",
        "    text_input_ids = [item[\"text_input_ids\"] for item in batch]\n",
        "    text_attention_masks = [item[\"text_attention_mask\"] for item in batch]\n",
        "    gen_input_ids = [item[\"gen_input_ids\"] for item in batch]\n",
        "    gen_attention_masks = [item[\"gen_attention_mask\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "    class_labels = torch.stack([item[\"label\"] for item in batch])\n",
        "    raw_reports = [item[\"raw_report\"] for item in batch]\n",
        "\n",
        "    # Pad sequences to the maximum length in the batch\n",
        "    text_input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        text_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "    )\n",
        "    text_attention_masks_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        text_attention_masks, batch_first=True, padding_value=0\n",
        "    )\n",
        "\n",
        "    gen_input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        gen_input_ids, batch_first=True, padding_value=report_generator_tokenizer.pad_token_id\n",
        "    )\n",
        "    gen_attention_masks_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        gen_attention_masks, batch_first=True, padding_value=0\n",
        "    )\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        labels, batch_first=True, padding_value=-100  # Use -100 for ignored tokens in loss computation\n",
        "    )\n",
        "\n",
        "    # Stack images\n",
        "    images_stacked = torch.stack(images)\n",
        "\n",
        "    return {\n",
        "        \"images\": images_stacked,\n",
        "        \"text_input_ids\": text_input_ids_padded,\n",
        "        \"text_attention_mask\": text_attention_masks_padded,\n",
        "        \"gen_input_ids\": gen_input_ids_padded,\n",
        "        \"gen_attention_mask\": gen_attention_masks_padded,\n",
        "        \"label\": class_labels,\n",
        "        \"labels\": labels_padded,\n",
        "        \"raw_report\": raw_reports,\n",
        "    }\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 1e-5\n",
        "optimizer = AdamW(combined_model.parameters(), lr=learning_rate)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Metrics for evaluation\n",
        "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "precision_metric = Precision(\n",
        "    task=\"multiclass\", average=\"macro\", num_classes=num_classes\n",
        ").to(device)\n",
        "recall_metric = Recall(task=\"multiclass\", average=\"macro\", num_classes=num_classes).to(device)\n",
        "f1_metric = F1Score(task=\"multiclass\", average=\"macro\", num_classes=num_classes).to(device)\n",
        "\n",
        "# Training loop with BioBART fine-tuning\n",
        "for epoch in range(num_epochs):\n",
        "    combined_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = batch[\"images\"].to(device)\n",
        "        text_input_ids = batch[\"text_input_ids\"].to(device)\n",
        "        text_attention_mask = batch[\"text_attention_mask\"].to(device)\n",
        "        gen_input_ids = batch[\"gen_input_ids\"].to(device)\n",
        "        gen_attention_mask = batch[\"gen_attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        decoder_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        class_outputs, _, gen_loss = combined_model(\n",
        "            images,\n",
        "            text_input_ids,\n",
        "            text_attention_mask,\n",
        "            gen_input_ids=gen_input_ids,\n",
        "            gen_attention_mask=gen_attention_mask,\n",
        "            labels=decoder_labels,\n",
        "        )\n",
        "        class_loss = torch.nn.CrossEntropyLoss()(class_outputs, labels)\n",
        "\n",
        "        # Total loss (classification + report generation)\n",
        "        total_batch_loss = class_loss + gen_loss\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss += total_batch_loss.item()\n",
        "        scaler.scale(total_batch_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Validation and Test\n",
        "for loader, split in zip([val_loader, test_loader], [\"Validation\", \"Test\"]):\n",
        "    total_accuracy, total_precision, total_recall, total_f1, total_bleu = (\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "    )\n",
        "    combined_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader):\n",
        "            images = batch[\"images\"].to(device)\n",
        "            text_input_ids = batch[\"text_input_ids\"].to(device)\n",
        "            text_attention_mask = batch[\"text_attention_mask\"].to(device)\n",
        "            gen_input_ids = batch[\"gen_input_ids\"].to(device)\n",
        "            gen_attention_mask = batch[\"gen_attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            raw_reports = batch[\"raw_report\"]\n",
        "\n",
        "            outputs, generated_reports, _ = combined_model(\n",
        "                images,\n",
        "                text_input_ids,\n",
        "                text_attention_mask,\n",
        "                gen_input_ids=gen_input_ids,\n",
        "                gen_attention_mask=gen_attention_mask,\n",
        "            )\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            total_accuracy += accuracy_metric(preds, labels)\n",
        "            total_precision += precision_metric(preds, labels)\n",
        "            total_recall += recall_metric(preds, labels)\n",
        "            total_f1 += f1_metric(preds, labels)\n",
        "\n",
        "            # Evaluate the generated report using BLEU score\n",
        "            for ref_report, gen_report in zip(raw_reports, generated_reports):\n",
        "                total_bleu += sentence_bleu(\n",
        "                    [ref_report.split()], gen_report.split()\n",
        "                )\n",
        "\n",
        "    num_batches = len(loader)\n",
        "    print(f\"{split} Accuracy: {total_accuracy / num_batches:.4f}\")\n",
        "    print(f\"{split} Precision: {total_precision / num_batches:.4f}\")\n",
        "    print(f\"{split} Recall: {total_recall / num_batches:.4f}\")\n",
        "    print(f\"{split} F1 Score: {total_f1 / num_batches:.4f}\")\n",
        "    print(f\"{split} BLEU Score (Generated Report): {total_bleu / num_batches:.4f}\")\n",
        "\n",
        "# Uncomment the following lines to save and upload the model to Hugging Face\n",
        "# login(\"YOUR_HUGGING_FACE_API_TOKEN\")\n",
        "# combined_model.save_pretrained(\"combined_multimodal_model\")\n",
        "# tokenizer.save_pretrained(\"combined_multimodal_model\")\n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path=\"combined_multimodal_model\",\n",
        "#     repo_id=\"username/combined_multimodal_model_name\",\n",
        "#     repo_type=\"model\",\n",
        "#     use_auth_token=True\n",
        "# )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TEJKoR6KdtLH",
        "outputId": "1b693429-729b-4dc7-9369-215472730524"
      },
      "id": "TEJKoR6KdtLH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 1.4017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Loss: 0.7302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/100], Loss: 0.4485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:48<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/100], Loss: 0.2856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:48<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/100], Loss: 0.1794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/100], Loss: 0.0569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/100], Loss: 0.0286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/100], Loss: 0.0138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/100], Loss: 0.0110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/100], Loss: 0.0074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/100], Loss: 0.0059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/100], Loss: 0.0068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/100], Loss: 0.0076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/100], Loss: 0.0048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/100], Loss: 0.0048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/100], Loss: 0.0049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:48<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/100], Loss: 0.0035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/100], Loss: 0.0026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.0040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/100], Loss: 0.0035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/100], Loss: 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/100], Loss: 0.0018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:48<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/100], Loss: 0.0044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/100], Loss: 0.0032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:48<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/100], Loss: 0.0051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/100], Loss: 0.0028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/100], Loss: 0.0018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/100], Loss: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/100], Loss: 0.0021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/100], Loss: 0.0025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/100], Loss: 0.0020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/100], Loss: 0.0020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/100], Loss: 0.0013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/100], Loss: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/100], Loss: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/100], Loss: 0.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/100], Loss: 0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/100], Loss: 0.0008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/100], Loss: 0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/100], Loss: 0.0017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/100], Loss: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:48<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/100], Loss: 0.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/100], Loss: 0.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/100], Loss: 0.0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/100], Loss: 0.0016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/100], Loss: 0.0028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/100], Loss: 0.0032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/100], Loss: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/100], Loss: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [51/100], Loss: 0.0022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [52/100], Loss: 0.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [53/100], Loss: 0.0007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [01:49<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [54/100], Loss: 0.0008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 33/66 [00:55<00:55,  1.68s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a4fb6f72963a>\u001b[0m in \u001b[0;36m<cell line: 303>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a4fb6f72963a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mimage_paths_sampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_paths\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpad_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         images = [\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_paths_sampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a4fb6f72963a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         images = [\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_paths_sampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 )\n\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m     def reduce(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c30c9af-422f-453e-ad45-e7708cc5a95b",
      "metadata": {
        "id": "8c30c9af-422f-453e-ad45-e7708cc5a95b",
        "outputId": "d7daf1bf-bc8f-49ab-ae09-61566bda26aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "37d6cb2fb82d4faab818d950b421a9d3",
            "43b7198cb45a44789dfe30f7b7e683b1",
            "ce9bc74d0af0429d8e5c9d629fd23521",
            "e7c534de8f524a61b9191e7573e87257",
            "8c19b9bf928c43fc9085805ac441ba17",
            "3ea9ade4ffdb4510854fddc081422891",
            "541872cb778047b7ac3bff36f3ebe4cc",
            "cd42c588e1a64f4d9376aba32828accb",
            "6f4663812f4b4634851b131792659285",
            "2b2821bb30f3418c96b16774c6cee44e",
            "78389a422ba848a0a97f82e82c8a1f39",
            "e8f55a11feab4ca892a791436e31f708",
            "2ab49ce4ddff45a2bcb5274cf49823ee",
            "c2e006c0972044ce8cdc88ac4c8e092b",
            "9752c747587f48e0a1eaed1e1fe83304",
            "932cbe9e8bcb436783b15c5d6dea1b12",
            "e226cd2aaf2e40c49ab7f6aa377ac663",
            "b278e6f70f4d47319f8b75227b7baf67",
            "2c255f4335f74ee0b42313b67b582e69",
            "bc109b76f8a449f09614c4b8d0a7e4bd",
            "2850aa864bfe4af3afb68f30a2315eb5",
            "19947bd2f6f6478fbfb8b400a9097a0d",
            "5438282fda8e409c8839d0b6eddc81b9",
            "7c0818a9f21f46178aef53a816b0f8db",
            "4e1381a9d8d040729f9e2c5e1ab5521e",
            "61588f461684424087386a8e0b08bc20",
            "57078bc564b0430f8ed66ea54d05d571",
            "1530e611f8e9464a9637d7d7f4df0e90",
            "d2391845787a422986af17f1d3086daf",
            "2c2179ef7eb8440a936c6b2bfaf3dfc3",
            "b0a5087a9451480e99eeb84a381d47a9",
            "d731a2ed9d3447538f9badf0a49e62c3",
            "04629aaf51f54999b58062c594cffbe5",
            "698f4fa745724835a3faae960fa69316",
            "41032926cbb4477592bafdae8dd76a33",
            "6fee3c758f2e4c869b0356869ab509a0",
            "72505579432b4c708287f1b539045ba6",
            "23f53386d7cf47d89d1c6643e594e337",
            "9a75a532fc884c228e9340ca13626015",
            "089bd6a8d2334bf49d5502c3f62d4d09",
            "90749f43e11e42bda519e5bd4638f62d",
            "e4212ac786854d58b78e45713b8e8690",
            "8e37f5333e6c4b06b5fd4658d9164776",
            "9d52bab1378c4d27a370f65fb2e4f507",
            "c48b136cfc5e435cbcf5a4ace1923d7f",
            "631621cb0e484da3b1a2497f23d6737b",
            "412902fd22d443d8a2c7885a119ccef6",
            "2662a691f6644e8f9aa3df9d265ddc4b",
            "be06450da0484866afca5e2279a3f547",
            "b61b4e61d3744c9fb1698b92705543b7",
            "7993ef40e8454741bbcec1ec1d9901bc",
            "77e54d38cdfd460bb7a36e8a0d38a337",
            "57cdb2d05f3b4cf0bb29797b90d8e062",
            "8f46b295cf49454aa9e8cabfd6cf2be2",
            "bf8c6361c2964f8e8b59c2524c754e08"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n",
            "100%|██████████| 127M/127M [00:00<00:00, 361MB/s] \n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37d6cb2fb82d4faab818d950b421a9d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8f55a11feab4ca892a791436e31f708"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5438282fda8e409c8839d0b6eddc81b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "698f4fa745724835a3faae960fa69316"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c48b136cfc5e435cbcf5a4ace1923d7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151/151 [00:29<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.9619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151/151 [00:27<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Loss: 0.5249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151/151 [00:27<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/100], Loss: 0.4020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151/151 [00:27<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/100], Loss: 0.1395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151/151 [00:27<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/100], Loss: 0.1026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151/151 [00:27<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/100], Loss: 0.0212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/151 [00:00<00:47,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ab9464b088c0>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths to class folders and images\n",
        "class_paths = {\n",
        "    \"acute\": \"/content/extracted_folder/acute\",  # Replace with actual path\n",
        "    \"normal\": \"/content/extracted_folder/normal\",\n",
        "    \"chronic\": \"/content/extracted_folder/chronic\",\n",
        "    \"lacunar\": \"/content/extracted_folder/lacunar\"\n",
        "}\n",
        "\n",
        "# Define transforms for images\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),  # Resizing frames to 112x112\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Initialize video and text models\n",
        "video_model = models.video.r3d_18(pretrained=True)\n",
        "video_model.fc = torch.nn.Linear(video_model.fc.in_features, 256)  # Embedding layer\n",
        "video_model = video_model.to(device)\n",
        "\n",
        "# Text model (BERT for embedding report text)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "# Dataset class with multi-frame inputs and report text\n",
        "class MultiModalPatientDataset(Dataset):\n",
        "    def __init__(self, class_paths, image_transform, max_frames=16):\n",
        "        self.image_transform = image_transform\n",
        "        self.max_frames = max_frames  # Set max_frames first\n",
        "        self.data = self._load_data(class_paths)\n",
        "\n",
        "    def _load_data(self, class_paths):\n",
        "        data = []\n",
        "        for class_label, class_path in class_paths.items():\n",
        "            for patient_folder in os.listdir(class_path):\n",
        "                patient_path = os.path.join(class_path, patient_folder)\n",
        "                image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "                report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "                # Ensure report exists\n",
        "                if os.path.exists(report_path):\n",
        "                    with open(report_path, 'r') as file:\n",
        "                        report = file.read().strip()\n",
        "\n",
        "                    # Append image paths, report text, and class label\n",
        "                    data.append({\n",
        "                        \"images\": image_paths[:self.max_frames],  # Limit to max_frames if needed\n",
        "                        \"report\": report,\n",
        "                        \"label\": class_label\n",
        "                    })\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        images = [self.image_transform(Image.open(img_path).convert(\"RGB\")) for img_path in item[\"images\"]]\n",
        "        images = torch.stack(images).permute(1, 0, 2, 3)  # Shape: (C, T, H, W)\n",
        "\n",
        "        text_inputs = tokenizer(item[\"report\"], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
        "        text_inputs = {k: v.squeeze(0).to(device) for k, v in text_inputs.items()}\n",
        "\n",
        "        label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "        return {\"images\": images.to(device), \"report\": text_inputs, \"label\": label}\n",
        "\n",
        "# Initialize dataset and split\n",
        "dataset = MultiModalPatientDataset(class_paths=class_paths, image_transform=image_transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 1e-5\n",
        "optimizer = AdamW(list(video_model.parameters()) + list(text_model.parameters()), lr=learning_rate)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = torch.nn.Linear(1024, len(class_paths)).to(device)  # Adjusted for combined embeddings of size 1024\n",
        "\n",
        "# Initialize metrics with the task argument\n",
        "accuracy = Accuracy(task=\"multiclass\", num_classes=len(class_paths)).to(device)\n",
        "precision = Precision(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "recall = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "f1 = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    video_model.train()\n",
        "    text_model.train()\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = batch[\"images\"]\n",
        "        labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "        with autocast():\n",
        "            video_outputs = video_model(images)\n",
        "            text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "            combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "            outputs = classifier(combined_embeddings)\n",
        "\n",
        "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Evaluation on the test set\n",
        "video_model.eval()\n",
        "text_model.eval()\n",
        "classifier.eval()\n",
        "total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        images = batch[\"images\"]\n",
        "        labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "        video_outputs = video_model(images)\n",
        "        text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "        combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "        outputs = classifier(combined_embeddings)\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        total_accuracy += accuracy(preds, labels)\n",
        "        total_precision += precision(preds, labels)\n",
        "        total_recall += recall(preds, labels)\n",
        "        total_f1 += f1(preds, labels)\n",
        "\n",
        "# Average metrics over the test set\n",
        "num_batches = len(test_loader)\n",
        "print(f\"Test Accuracy: {total_accuracy / num_batches:.4f}\")\n",
        "print(f\"Test Precision: {total_precision / num_batches:.4f}\")\n",
        "print(f\"Test Recall: {total_recall / num_batches:.4f}\")\n",
        "print(f\"Test F1 Score: {total_f1 / num_batches:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267f2334-ea2b-473d-a6ca-28abbd14efe3",
      "metadata": {
        "id": "267f2334-ea2b-473d-a6ca-28abbd14efe3",
        "outputId": "8b746f93-83a5-403f-f75f-b853a7eb2446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip  install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f908cf5-c199-4049-bc5a-d01aefa79f3c",
      "metadata": {
        "id": "7f908cf5-c199-4049-bc5a-d01aefa79f3c",
        "outputId": "92122df5-f031-4a5b-d1f1-d7456e1130a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.6)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c50650-603b-4596-b559-3d87b9795b57",
      "metadata": {
        "id": "03c50650-603b-4596-b559-3d87b9795b57",
        "outputId": "b2eef7c5-be51-4fa4-ce89-93c2efc92825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.0)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
            "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
            "  Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Using cached transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.12.1\n",
            "    Uninstalling huggingface-hub-0.12.1:\n",
            "      Successfully uninstalled huggingface-hub-0.12.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.29.0\n",
            "    Uninstalling transformers-4.29.0:\n",
            "      Successfully uninstalled transformers-4.29.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "video-transformers 0.0.9 requires huggingface-hub<0.12.0,>=0.11.0, but you have huggingface-hub 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.26.2 tokenizers-0.20.1 transformers-4.46.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247bc689-71c8-40f3-a40f-19013d6ac9e8",
      "metadata": {
        "id": "247bc689-71c8-40f3-a40f-19013d6ac9e8",
        "outputId": "c9957ad4-5e64-479b-cd34-a1fcc12ad4fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (18.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.3)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install transformers\n",
        "!pip install torchmetrics\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d49ecd-91e6-4b1b-baca-21be175d8b20",
      "metadata": {
        "id": "d1d49ecd-91e6-4b1b-baca-21be175d8b20",
        "outputId": "98d59516-807e-49ef-d552-b18dbdc7f9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b17fd63-9938-4eb5-b4b7-03d2db7ee72d",
      "metadata": {
        "id": "3b17fd63-9938-4eb5-b4b7-03d2db7ee72d",
        "outputId": "1f0b1001-a16c-4c8e-a929-dd94a7bda1cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.26.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.0.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (9.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision==0.15.2) (68.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision==0.15.2) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision==0.15.2) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision==0.15.2) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision==0.15.2) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision==0.15.2) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torchvision==0.15.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba34bd60-05e2-4ff5-a050-9b9bce6581ee",
      "metadata": {
        "id": "ba34bd60-05e2-4ff5-a050-9b9bce6581ee",
        "outputId": "d75b707f-2588-4f38-8e23-20777a89734e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Collecting torch\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.5.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m178.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m163.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m190.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m178.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m198.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m173.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m205.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m167.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.0.0\n",
            "    Uninstalling triton-2.0.0:\n",
            "      Successfully uninstalled triton-2.0.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2\n",
            "    Uninstalling torchvision-0.15.2:\n",
            "      Successfully uninstalled torchvision-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2 requires torch==2.0.1, but you have torch 2.5.1 which is incompatible.\n",
            "video-transformers 0.0.9 requires huggingface-hub<0.12.0,>=0.11.0, but you have huggingface-hub 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 torchvision-0.20.1 triton-3.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3f56b7-f673-40e7-96b0-5bb76bf9d4e8",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ac18615f6d9b4256bde905fd9cdb807c",
            "4be2dcf4699345998ccee07ff4dd6344"
          ]
        },
        "id": "db3f56b7-f673-40e7-96b0-5bb76bf9d4e8",
        "outputId": "0cffe8e7-5b6a-4682-80f4-9b86d84699dd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac18615f6d9b4256bde905fd9cdb807c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4be2dcf4699345998ccee07ff4dd6344",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "text_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caccbab3-1cfd-4c6c-b002-0322e0756e93",
      "metadata": {
        "id": "caccbab3-1cfd-4c6c-b002-0322e0756e93",
        "outputId": "6ba20180-2947-42ee-d736-0112b780c3cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1\n",
            "Uninstalling torch-2.5.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.10/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch-2.5.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? ^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72315dcb-9ee5-47ea-a5f5-31bfe60691cc",
      "metadata": {
        "id": "72315dcb-9ee5-47ea-a5f5-31bfe60691cc",
        "outputId": "5d9de386-fa37-4e0b-f056-7cf081eb1cdb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==2.0.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1843.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2+cu117\n",
            "  Using cached https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu117) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu117) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu117) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu117) (3.1.2)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu117)\n",
            "  Using cached https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu117) (1.26.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu117) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu117) (9.3.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu117) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu117) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1+cu117) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu117) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu117) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu117) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu117) (2022.12.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1+cu117) (1.3.0)\n",
            "\u001b[33mWARNING: Error parsing dependencies of triton: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/triton-3.1.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: triton 3.1.0\n",
            "    Can't uninstall 'triton'. No files were found to uninstall.\n",
            "  Attempting uninstall: torch\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: torch 2.5.1\n",
            "    Can't uninstall 'torch'. No files were found to uninstall.\n",
            "  Attempting uninstall: torchvision\n",
            "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: torchvision 0.20.1\n",
            "    Can't uninstall 'torchvision'. No files were found to uninstall.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "video-transformers 0.0.9 requires huggingface-hub<0.12.0,>=0.11.0, but you have huggingface-hub 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1+cu117 torchvision-0.15.2+cu117 triton-2.0.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 --index-url https://download.pytorch.org/whl/cu117\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebea191a-0850-430f-ab4e-a3d669305edb",
      "metadata": {
        "id": "ebea191a-0850-430f-ab4e-a3d669305edb",
        "outputId": "3b2d8637-4f9b-4fde-9fb6-30aaf1a614fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "CUDA Available: True\n",
            "CUDA Version: 12.4\n",
            "cuDNN Version: 90100\n",
            "PyTorch Version: 2.5.1+cu124\n",
            "TorchVision Version: 0.20.1+cu124\n",
            "Error moving model to device: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "import torchvision\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING for detailed error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Verify CUDA availability and versions\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"CUDA Version:\", torch.version.cuda)\n",
        "print(\"cuDNN Version:\", torch.backends.cudnn.version())\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"TorchVision Version:\", torchvision.__version__)\n",
        "\n",
        "# Initialize models\n",
        "try:\n",
        "    # Initialize the video model with pretrained weights (after installing compatible versions)\n",
        "    video_model = models.video.r3d_18(pretrained=True)\n",
        "    video_model.fc = torch.nn.Linear(video_model.fc.in_features, 256)\n",
        "    video_model = video_model.to(device)\n",
        "    print(\"Model moved to device successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Error moving model to device:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "633afbbf-b909-4bcc-ae97-68f71c4048bb",
      "metadata": {
        "id": "633afbbf-b909-4bcc-ae97-68f71c4048bb",
        "outputId": "aebe10da-3e63-49e7-863e-e4cd0bce610d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 342MB/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Modify the final layer to output 256 features\u001b[39;00m\n\u001b[1;32m     45\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(cnn_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Tokenizers and models\u001b[39;00m\n\u001b[1;32m     49\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdmis-lab/biobert-base-cased-v1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "# from huggingface_hub import login, HfApi  # Uncomment if needed\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths to class folders and images\n",
        "class_paths = {\n",
        "    \"acute\": \"acute\",\n",
        "    \"normal\": \"normal\",\n",
        "    \"chronic\": \"chronic\",\n",
        "    \"lacunar\": \"lacunar\",\n",
        "}\n",
        "\n",
        "# Define transforms for images\n",
        "image_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),  # Updated size for ResNet\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Initialize models\n",
        "# Using a pre-trained ResNet model\n",
        "cnn_model = models.resnet50(pretrained=True)\n",
        "# Modify the final layer to output 256 features\n",
        "cnn_model.fc = torch.nn.Linear(cnn_model.fc.in_features, 256)\n",
        "cnn_model = cnn_model.to(device)\n",
        "\n",
        "# Tokenizers and models\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "text_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(device)\n",
        "\n",
        "report_generator_tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-v2-base\")\n",
        "report_generator = AutoModelForSeq2SeqLM.from_pretrained(\"GanjinZero/biobart-v2-base\").to(device)\n",
        "\n",
        "# Custom model that combines image, text, classification, and report generation\n",
        "class CombinedModel(torch.nn.Module):\n",
        "    def __init__(self, cnn_model, text_model, report_generator, num_classes):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.text_model = text_model\n",
        "        self.report_generator = report_generator\n",
        "        self.classifier = torch.nn.Linear(\n",
        "            256 + 768, num_classes\n",
        "        )  # Adjust input size based on output features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images,  # Shape: (Batch, Frames, Channels, Height, Width)\n",
        "        text_input_ids,\n",
        "        text_attention_mask,\n",
        "        gen_input_ids=None,\n",
        "        gen_attention_mask=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        batch_size, num_frames, C, H, W = images.shape\n",
        "        images = images.view(-1, C, H, W)  # Reshape to (Batch * Frames, C, H, W)\n",
        "\n",
        "        # Process images through CNN\n",
        "        cnn_features = self.cnn_model(images)  # Shape: (Batch * Frames, 256)\n",
        "        cnn_features = cnn_features.view(batch_size, num_frames, -1)  # (Batch, Frames, 256)\n",
        "\n",
        "        # Aggregate features across frames (e.g., by averaging)\n",
        "        video_embeddings = torch.mean(cnn_features, dim=1)  # (Batch, 256)\n",
        "\n",
        "        # Process text input with text_model\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=text_input_ids, attention_mask=text_attention_mask\n",
        "        )\n",
        "        text_embeddings = text_outputs.last_hidden_state.mean(dim=1)  # (Batch, 768)\n",
        "\n",
        "        # Combine embeddings and classify\n",
        "        combined_embeddings = torch.cat((video_embeddings, text_embeddings), dim=1)  # (Batch, 1024)\n",
        "        class_outputs = self.classifier(combined_embeddings)  # (Batch, num_classes)\n",
        "\n",
        "        # Generate report using the report generator\n",
        "        if labels is not None:\n",
        "            outputs = self.report_generator(\n",
        "                input_ids=gen_input_ids,\n",
        "                attention_mask=gen_attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "            gen_loss = outputs.loss\n",
        "            generated_report = None  # Do not generate report during training\n",
        "        else:\n",
        "            gen_loss = None\n",
        "            generated_report_ids = self.report_generator.generate(\n",
        "                input_ids=gen_input_ids,\n",
        "                attention_mask=gen_attention_mask,\n",
        "                max_length=100,\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            generated_report = report_generator_tokenizer.batch_decode(\n",
        "                generated_report_ids, skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "        return class_outputs, generated_report, gen_loss\n",
        "\n",
        "# Initialize the combined model\n",
        "num_classes = len(class_paths)\n",
        "combined_model = CombinedModel(\n",
        "    cnn_model, text_model, report_generator, num_classes\n",
        ").to(device)\n",
        "\n",
        "# Dataset class with fixed number of frames\n",
        "class MultiModalPatientDataset(Dataset):\n",
        "    def __init__(self, class_paths, image_transform, n_frames=16):\n",
        "        self.image_transform = image_transform\n",
        "        self.data = self._load_data(class_paths)\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def _load_data(self, class_paths):\n",
        "        data = []\n",
        "        for class_label, class_path in class_paths.items():\n",
        "            for patient_folder in os.listdir(class_path):\n",
        "                patient_path = os.path.join(class_path, patient_folder)\n",
        "                image_paths = [\n",
        "                    os.path.join(patient_path, f)\n",
        "                    for f in os.listdir(patient_path)\n",
        "                    if f.endswith(\".png\")\n",
        "                ]\n",
        "                report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "                if os.path.exists(report_path):\n",
        "                    with open(report_path, \"r\") as file:\n",
        "                        report = file.read().strip()\n",
        "                    data.append(\n",
        "                        {\n",
        "                            \"images\": image_paths,\n",
        "                            \"report\": report,\n",
        "                            \"label\": class_label,\n",
        "                            \"raw_report\": report,\n",
        "                        }\n",
        "                    )\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_paths = item[\"images\"]\n",
        "        n_images = len(image_paths)\n",
        "\n",
        "        # Sample or pad images to fixed number of frames\n",
        "        if n_images >= self.n_frames:\n",
        "            # Evenly sample n_frames images\n",
        "            indices = np.linspace(0, n_images - 1, self.n_frames, dtype=int)\n",
        "            image_paths_sampled = [image_paths[i] for i in indices]\n",
        "        else:\n",
        "            # Pad the image list by repeating the last image\n",
        "            pad_count = self.n_frames - n_images\n",
        "            image_paths_sampled = image_paths + [image_paths[-1]] * pad_count\n",
        "\n",
        "        images = [\n",
        "            self.image_transform(Image.open(img_path).convert(\"RGB\"))\n",
        "            for img_path in image_paths_sampled\n",
        "        ]\n",
        "        images = torch.stack(images)  # Shape: (Frames, Channels, Height, Width)\n",
        "\n",
        "        # Tokenize report for text_model (BioBERT)\n",
        "        text_inputs = tokenizer(\n",
        "            item[\"report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        text_input_ids = text_inputs[\"input_ids\"].squeeze(0)\n",
        "        text_attention_mask = text_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Tokenize report for report_generator (BioBART)\n",
        "        gen_inputs = report_generator_tokenizer(\n",
        "            item[\"report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        gen_input_ids = gen_inputs[\"input_ids\"].squeeze(0)\n",
        "        gen_attention_mask = gen_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Tokenize target report for labels (BioBART)\n",
        "        target_inputs = report_generator_tokenizer(\n",
        "            item[\"raw_report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        labels = target_inputs[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        label = torch.tensor(list(class_paths.keys()).index(item[\"label\"]))\n",
        "\n",
        "        return {\n",
        "            \"images\": images,  # Shape: (Frames, Channels, Height, Width)\n",
        "            \"text_input_ids\": text_input_ids,\n",
        "            \"text_attention_mask\": text_attention_mask,\n",
        "            \"gen_input_ids\": gen_input_ids,\n",
        "            \"gen_attention_mask\": gen_attention_mask,\n",
        "            \"label\": label,\n",
        "            \"labels\": labels,  # For Seq2Seq model\n",
        "            \"raw_report\": item[\"raw_report\"],\n",
        "        }\n",
        "\n",
        "# Initialize dataset and split\n",
        "dataset = MultiModalPatientDataset(\n",
        "    class_paths=class_paths, image_transform=image_transform, n_frames=16\n",
        ")\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [item[\"images\"] for item in batch]\n",
        "    text_input_ids = [item[\"text_input_ids\"] for item in batch]\n",
        "    text_attention_masks = [item[\"text_attention_mask\"] for item in batch]\n",
        "    gen_input_ids = [item[\"gen_input_ids\"] for item in batch]\n",
        "    gen_attention_masks = [item[\"gen_attention_mask\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "    class_labels = torch.stack([item[\"label\"] for item in batch])\n",
        "    raw_reports = [item[\"raw_report\"] for item in batch]\n",
        "\n",
        "    # Pad sequences to the maximum length in the batch\n",
        "    text_input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        text_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "    )\n",
        "    text_attention_masks_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        text_attention_masks, batch_first=True, padding_value=0\n",
        "    )\n",
        "\n",
        "    gen_input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        gen_input_ids, batch_first=True, padding_value=report_generator_tokenizer.pad_token_id\n",
        "    )\n",
        "    gen_attention_masks_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        gen_attention_masks, batch_first=True, padding_value=0\n",
        "    )\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        labels, batch_first=True, padding_value=-100  # For ignored tokens in loss computation\n",
        "    )\n",
        "\n",
        "    # Stack images\n",
        "    images_stacked = torch.stack(images)  # Shape: (Batch, Frames, C, H, W)\n",
        "\n",
        "    return {\n",
        "        \"images\": images_stacked,\n",
        "        \"text_input_ids\": text_input_ids_padded,\n",
        "        \"text_attention_mask\": text_attention_masks_padded,\n",
        "        \"gen_input_ids\": gen_input_ids_padded,\n",
        "        \"gen_attention_mask\": gen_attention_masks_padded,\n",
        "        \"label\": class_labels,\n",
        "        \"labels\": labels_padded,\n",
        "        \"raw_report\": raw_reports,\n",
        "    }\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-5\n",
        "optimizer = AdamW(combined_model.parameters(), lr=learning_rate)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Metrics for evaluation\n",
        "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "precision_metric = Precision(\n",
        "    task=\"multiclass\", average=\"macro\", num_classes=num_classes\n",
        ").to(device)\n",
        "recall_metric = Recall(task=\"multiclass\", average=\"macro\", num_classes=num_classes).to(device)\n",
        "f1_metric = F1Score(task=\"multiclass\", average=\"macro\", num_classes=num_classes).to(device)\n",
        "\n",
        "# Training loop with BioBART fine-tuning\n",
        "for epoch in range(num_epochs):\n",
        "    combined_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = batch[\"images\"].to(device)  # Shape: (Batch, Frames, C, H, W)\n",
        "        text_input_ids = batch[\"text_input_ids\"].to(device)\n",
        "        text_attention_mask = batch[\"text_attention_mask\"].to(device)\n",
        "        gen_input_ids = batch[\"gen_input_ids\"].to(device)\n",
        "        gen_attention_mask = batch[\"gen_attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        decoder_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        class_outputs, _, gen_loss = combined_model(\n",
        "            images,\n",
        "            text_input_ids,\n",
        "            text_attention_mask,\n",
        "            gen_input_ids=gen_input_ids,\n",
        "            gen_attention_mask=gen_attention_mask,\n",
        "            labels=decoder_labels,\n",
        "        )\n",
        "        class_loss = torch.nn.CrossEntropyLoss()(class_outputs, labels)\n",
        "\n",
        "        # Total loss (classification + report generation)\n",
        "        total_batch_loss = class_loss + gen_loss\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss += total_batch_loss.item()\n",
        "        scaler.scale(total_batch_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Validation and Test\n",
        "for loader, split in zip([val_loader, test_loader], [\"Validation\", \"Test\"]):\n",
        "    total_accuracy, total_precision, total_recall, total_f1, total_bleu = (\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "    )\n",
        "    combined_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader):\n",
        "            images = batch[\"images\"].to(device)  # Shape: (Batch, Frames, C, H, W)\n",
        "            text_input_ids = batch[\"text_input_ids\"].to(device)\n",
        "            text_attention_mask = batch[\"text_attention_mask\"].to(device)\n",
        "            gen_input_ids = batch[\"gen_input_ids\"].to(device)\n",
        "            gen_attention_mask = batch[\"gen_attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            raw_reports = batch[\"raw_report\"]\n",
        "\n",
        "            outputs, generated_reports, _ = combined_model(\n",
        "                images,\n",
        "                text_input_ids,\n",
        "                text_attention_mask,\n",
        "                gen_input_ids=gen_input_ids,\n",
        "                gen_attention_mask=gen_attention_mask,\n",
        "            )\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            total_accuracy += accuracy_metric(preds, labels)\n",
        "            total_precision += precision_metric(preds, labels)\n",
        "            total_recall += recall_metric(preds, labels)\n",
        "            total_f1 += f1_metric(preds, labels)\n",
        "\n",
        "            # Evaluate the generated report using BLEU score\n",
        "            for ref_report, gen_report in zip(raw_reports, generated_reports):\n",
        "                total_bleu += sentence_bleu(\n",
        "                    [ref_report.split()], gen_report.split()\n",
        "                )\n",
        "\n",
        "    num_batches = len(loader)\n",
        "    print(f\"{split} Accuracy: {total_accuracy / num_batches:.4f}\")\n",
        "    print(f\"{split} Precision: {total_precision / num_batches:.4f}\")\n",
        "    print(f\"{split} Recall: {total_recall / num_batches:.4f}\")\n",
        "    print(f\"{split} F1 Score: {total_f1 / num_batches:.4f}\")\n",
        "    print(f\"{split} BLEU Score (Generated Report): {total_bleu / num_batches:.4f}\")\n",
        "\n",
        "# Uncomment the following lines to save and upload the model to Hugging Face\n",
        "# login(\"YOUR_HUGGING_FACE_API_TOKEN\")\n",
        "# combined_model.save_pretrained(\"combined_multimodal_model\")\n",
        "# tokenizer.save_pretrained(\"combined_multimodal_model\")\n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path=\"combined_multimodal_model\",\n",
        "#     repo_id=\"username/combined_multimodal_model_name\",\n",
        "#     repo_type=\"model\",\n",
        "#     use_auth_token=True\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb8136a-f412-49cf-a2cb-ddc932b9b310",
      "metadata": {
        "id": "efb8136a-f412-49cf-a2cb-ddc932b9b310",
        "outputId": "a93ce437-3ff7-47ff-e14d-185c5dcde69c"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m video_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mr3d_18(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m video_model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(video_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m256\u001b[39m)  \u001b[38;5;66;03m# Embedding layer\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m video_model \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Tokenizers and models\u001b[39;00m\n\u001b[1;32m     48\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdmis-lab/biobert-base-cased-v1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "# from huggingface_hub import login, HfApi  # Uncomment if needed\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths to class folders and images\n",
        "class_paths = {\n",
        "    \"acute\": \"acute\",\n",
        "    \"normal\": \"normal\",\n",
        "    \"chronic\": \"chronic\",\n",
        "    \"lacunar\": \"lacunar\",\n",
        "}\n",
        "\n",
        "# Define transforms for images\n",
        "image_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((112, 112)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Initialize models\n",
        "video_model = models.video.r3d_18(pretrained=True)\n",
        "\n",
        "video_model.fc = torch.nn.Linear(video_model.fc.in_features, 256)  # Embedding layer\n",
        "video_model = video_model.to(device)\n",
        "\n",
        "# Tokenizers and models\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "text_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(device)\n",
        "\n",
        "report_generator_tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/biobart-v2-base\")\n",
        "report_generator = AutoModelForSeq2SeqLM.from_pretrained(\"GanjinZero/biobart-v2-base\").to(device)\n",
        "\n",
        "# Custom model that combines video, text, classification, and report generation\n",
        "class CombinedModel(torch.nn.Module):\n",
        "    def __init__(self, video_model, text_model, report_generator, num_classes):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.video_model = video_model\n",
        "        self.text_model = text_model\n",
        "        self.report_generator = report_generator\n",
        "        self.classifier = torch.nn.Linear(\n",
        "            256 + 768, num_classes\n",
        "        )  # Adjust input size based on output features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images,\n",
        "        text_input_ids,\n",
        "        text_attention_mask,\n",
        "        gen_input_ids=None,\n",
        "        gen_attention_mask=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        # Process video input\n",
        "        video_embeddings = self.video_model(images)\n",
        "\n",
        "        # Process text input with text_model\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=text_input_ids, attention_mask=text_attention_mask\n",
        "        )\n",
        "        text_embeddings = text_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Combine embeddings and classify\n",
        "        combined_embeddings = torch.cat((video_embeddings, text_embeddings), dim=1)\n",
        "        class_outputs = self.classifier(combined_embeddings)\n",
        "\n",
        "        # Generate report using the report generator\n",
        "        if labels is not None:\n",
        "            outputs = self.report_generator(\n",
        "                input_ids=gen_input_ids,\n",
        "                attention_mask=gen_attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "            gen_loss = outputs.loss\n",
        "            generated_report = None  # Do not generate report during training\n",
        "        else:\n",
        "            gen_loss = None\n",
        "            generated_report_ids = self.report_generator.generate(\n",
        "                input_ids=gen_input_ids,\n",
        "                attention_mask=gen_attention_mask,\n",
        "                max_length=100,\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            generated_report = report_generator_tokenizer.batch_decode(\n",
        "                generated_report_ids, skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "        return class_outputs, generated_report, gen_loss\n",
        "\n",
        "# Initialize the combined model\n",
        "num_classes = len(class_paths)\n",
        "combined_model = CombinedModel(\n",
        "    video_model, text_model, report_generator, num_classes\n",
        ").to(device)\n",
        "\n",
        "# Dataset class with summarization and fixed number of frames\n",
        "class MultiModalPatientDataset(Dataset):\n",
        "    def __init__(self, class_paths, image_transform, n_frames=128):\n",
        "        self.image_transform = image_transform\n",
        "        self.data = self._load_data(class_paths)\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def _load_data(self, class_paths):\n",
        "        data = []\n",
        "        for class_label, class_path in class_paths.items():\n",
        "            for patient_folder in os.listdir(class_path):\n",
        "                patient_path = os.path.join(class_path, patient_folder)\n",
        "                image_paths = [\n",
        "                    os.path.join(patient_path, f)\n",
        "                    for f in os.listdir(patient_path)\n",
        "                    if f.endswith(\".png\")\n",
        "                ]\n",
        "                report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "                if os.path.exists(report_path):\n",
        "                    with open(report_path, \"r\") as file:\n",
        "                        report = file.read().strip()\n",
        "                    # For simplicity, not summarizing the report\n",
        "                    summarized_report = report\n",
        "                    data.append(\n",
        "                        {\n",
        "                            \"images\": image_paths,\n",
        "                            \"report\": summarized_report,\n",
        "                            \"label\": class_label,\n",
        "                            \"raw_report\": report,  # Keep the original report for BLEU score\n",
        "                        }\n",
        "                    )\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_paths = item[\"images\"]\n",
        "        n_images = len(image_paths)\n",
        "\n",
        "        # Sample or pad images to fixed number of frames\n",
        "        if n_images >= self.n_frames:\n",
        "            # Evenly sample n_frames images\n",
        "            indices = np.linspace(0, n_images - 1, self.n_frames, dtype=int)\n",
        "            image_paths_sampled = [image_paths[i] for i in indices]\n",
        "        else:\n",
        "            # Pad the image list by repeating the last image\n",
        "            pad_count = self.n_frames - n_images\n",
        "            image_paths_sampled = image_paths + [image_paths[-1]] * pad_count\n",
        "\n",
        "        images = [\n",
        "            self.image_transform(Image.open(img_path).convert(\"RGB\"))\n",
        "            for img_path in image_paths_sampled\n",
        "        ]\n",
        "        images = torch.stack(images).permute(\n",
        "            1, 0, 2, 3\n",
        "        )  # Shape: (Channels, Frames, Height, Width)\n",
        "\n",
        "        # Tokenize report for text_model (BioBERT)\n",
        "        text_inputs = tokenizer(\n",
        "            item[\"report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        text_input_ids = text_inputs[\"input_ids\"].squeeze(0)\n",
        "        text_attention_mask = text_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Tokenize report for report_generator (BioBART)\n",
        "        gen_inputs = report_generator_tokenizer(\n",
        "            item[\"report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        gen_input_ids = gen_inputs[\"input_ids\"].squeeze(0)\n",
        "        gen_attention_mask = gen_inputs[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Tokenize target report for labels (BioBART)\n",
        "        target_inputs = report_generator_tokenizer(\n",
        "            item[\"raw_report\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "        labels = target_inputs[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        label = torch.tensor(list(class_paths.keys()).index(item[\"label\"]), dtype=torch.long)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"images\": images,\n",
        "            \"text_input_ids\": text_input_ids,\n",
        "            \"text_attention_mask\": text_attention_mask,\n",
        "            \"gen_input_ids\": gen_input_ids,\n",
        "            \"gen_attention_mask\": gen_attention_mask,\n",
        "            \"label\": label,\n",
        "            \"labels\": labels,  # For Seq2Seq model, labels are tokenized target sequences\n",
        "            \"raw_report\": item[\"raw_report\"],  # Include raw report for BLEU score\n",
        "        }\n",
        "\n",
        "# Initialize dataset and split\n",
        "dataset = MultiModalPatientDataset(\n",
        "    class_paths=class_paths, image_transform=image_transform, n_frames=128\n",
        ")\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [item[\"images\"] for item in batch]\n",
        "    text_input_ids = [item[\"text_input_ids\"] for item in batch]\n",
        "    text_attention_masks = [item[\"text_attention_mask\"] for item in batch]\n",
        "    gen_input_ids = [item[\"gen_input_ids\"] for item in batch]\n",
        "    gen_attention_masks = [item[\"gen_attention_mask\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "    class_labels = torch.stack([item[\"label\"] for item in batch])\n",
        "    raw_reports = [item[\"raw_report\"] for item in batch]\n",
        "\n",
        "    # Pad sequences to the maximum length in the batch\n",
        "    text_input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        text_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "    )\n",
        "    text_attention_masks_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        text_attention_masks, batch_first=True, padding_value=0\n",
        "    )\n",
        "\n",
        "    gen_input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        gen_input_ids, batch_first=True, padding_value=report_generator_tokenizer.pad_token_id\n",
        "    )\n",
        "    gen_attention_masks_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        gen_attention_masks, batch_first=True, padding_value=0\n",
        "    )\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        labels, batch_first=True, padding_value=-100  # Use -100 for ignored tokens in loss computation\n",
        "    )\n",
        "\n",
        "    # Stack images\n",
        "    images_stacked = torch.stack(images)\n",
        "\n",
        "    return {\n",
        "        \"images\": images_stacked,\n",
        "        \"text_input_ids\": text_input_ids_padded,\n",
        "        \"text_attention_mask\": text_attention_masks_padded,\n",
        "        \"gen_input_ids\": gen_input_ids_padded,\n",
        "        \"gen_attention_mask\": gen_attention_masks_padded,\n",
        "        \"label\": class_labels,\n",
        "        \"labels\": labels_padded,\n",
        "        \"raw_report\": raw_reports,\n",
        "    }\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-5\n",
        "optimizer = AdamW(combined_model.parameters(), lr=learning_rate)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Metrics for evaluation\n",
        "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "precision_metric = Precision(\n",
        "    task=\"multiclass\", average=\"macro\", num_classes=num_classes\n",
        ").to(device)\n",
        "recall_metric = Recall(task=\"multiclass\", average=\"macro\", num_classes=num_classes).to(device)\n",
        "f1_metric = F1Score(task=\"multiclass\", average=\"macro\", num_classes=num_classes).to(device)\n",
        "\n",
        "# Training loop with BioBART fine-tuning\n",
        "for epoch in range(num_epochs):\n",
        "    combined_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = batch[\"images\"].to(device)\n",
        "        text_input_ids = batch[\"text_input_ids\"].to(device)\n",
        "        text_attention_mask = batch[\"text_attention_mask\"].to(device)\n",
        "        gen_input_ids = batch[\"gen_input_ids\"].to(device)\n",
        "        gen_attention_mask = batch[\"gen_attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        decoder_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        class_outputs, _, gen_loss = combined_model(\n",
        "            images,\n",
        "            text_input_ids,\n",
        "            text_attention_mask,\n",
        "            gen_input_ids=gen_input_ids,\n",
        "            gen_attention_mask=gen_attention_mask,\n",
        "            labels=decoder_labels,\n",
        "        )\n",
        "        class_loss = torch.nn.CrossEntropyLoss()(class_outputs, labels)\n",
        "\n",
        "        # Total loss (classification + report generation)\n",
        "        total_batch_loss = class_loss + gen_loss\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss += total_batch_loss.item()\n",
        "        scaler.scale(total_batch_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Validation and Test\n",
        "for loader, split in zip([val_loader, test_loader], [\"Validation\", \"Test\"]):\n",
        "    total_accuracy, total_precision, total_recall, total_f1, total_bleu = (\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "    )\n",
        "    combined_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader):\n",
        "            images = batch[\"images\"].to(device)\n",
        "            text_input_ids = batch[\"text_input_ids\"].to(device)\n",
        "            text_attention_mask = batch[\"text_attention_mask\"].to(device)\n",
        "            gen_input_ids = batch[\"gen_input_ids\"].to(device)\n",
        "            gen_attention_mask = batch[\"gen_attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            raw_reports = batch[\"raw_report\"]\n",
        "\n",
        "            outputs, generated_reports, _ = combined_model(\n",
        "                images,\n",
        "                text_input_ids,\n",
        "                text_attention_mask,\n",
        "                gen_input_ids=gen_input_ids,\n",
        "                gen_attention_mask=gen_attention_mask,\n",
        "            )\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            total_accuracy += accuracy_metric(preds, labels)\n",
        "            total_precision += precision_metric(preds, labels)\n",
        "            total_recall += recall_metric(preds, labels)\n",
        "            total_f1 += f1_metric(preds, labels)\n",
        "\n",
        "            # Evaluate the generated report using BLEU score\n",
        "            for ref_report, gen_report in zip(raw_reports, generated_reports):\n",
        "                total_bleu += sentence_bleu(\n",
        "                    [ref_report.split()], gen_report.split()\n",
        "                )\n",
        "\n",
        "    num_batches = len(loader)\n",
        "    print(f\"{split} Accuracy: {total_accuracy / num_batches:.4f}\")\n",
        "    print(f\"{split} Precision: {total_precision / num_batches:.4f}\")\n",
        "    print(f\"{split} Recall: {total_recall / num_batches:.4f}\")\n",
        "    print(f\"{split} F1 Score: {total_f1 / num_batches:.4f}\")\n",
        "    print(f\"{split} BLEU Score (Generated Report): {total_bleu / num_batches:.4f}\")\n",
        "\n",
        "# Uncomment the following lines to save and upload the model to Hugging Face\n",
        "# login(\"YOUR_HUGGING_FACE_API_TOKEN\")\n",
        "# combined_model.save_pretrained(\"combined_multimodal_model\")\n",
        "# tokenizer.save_pretrained(\"combined_multimodal_model\")\n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path=\"combined_multimodal_model\",\n",
        "#     repo_id=\"username/combined_multimodal_model_name\",\n",
        "#     repo_type=\"model\",\n",
        "#     use_auth_token=True\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f5ed9e-5c3c-4a43-b5f3-9bf58894e4a5",
      "metadata": {
        "id": "92f5ed9e-5c3c-4a43-b5f3-9bf58894e4a5",
        "outputId": "75b0ebc6-a87c-4355-d4f6-02fc75d0df5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c991bd5e-4c71-4afd-a398-1fe570009ac3",
      "metadata": {
        "id": "c991bd5e-4c71-4afd-a398-1fe570009ac3",
        "outputId": "896da039-8aac-4d4e-cff8-e084d0566ba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA tensor operations failed: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "Failed to move simple model to CUDA: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "try:\n",
        "    # Test tensor operation\n",
        "    x = torch.randn(1).to(device)\n",
        "    y = x * 2\n",
        "    print(\"CUDA tensor operations work.\")\n",
        "except Exception as e:\n",
        "    print(\"CUDA tensor operations failed:\", e)\n",
        "\n",
        "# Test moving a simple model to CUDA\n",
        "try:\n",
        "    model = torch.nn.Linear(10, 10).to(device)\n",
        "    print(\"Simple model moved to CUDA successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to move simple model to CUDA:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7f85cc-9a63-42a4-ba56-52fcddf5c0f4",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f4642212fe204b1d881fb96e7789b58f",
            "589840b7960b4b2e9216db51e02ecd70"
          ]
        },
        "id": "dc7f85cc-9a63-42a4-ba56-52fcddf5c0f4",
        "outputId": "7cf90fec-4a7e-4692-cdd2-6a80a03e7fd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/132 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4642212fe204b1d881fb96e7789b58f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m class_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(class_outputs, labels)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Report generation task\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m report_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreport\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    138\u001b[0m generated_report \u001b[38;5;241m=\u001b[39m report_generator\u001b[38;5;241m.\u001b[39mgenerate(combined_embeddings\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    139\u001b[0m report_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(generated_report\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, report_generator\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size), report_inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3021\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3081\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3078\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 3081\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3082\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3084\u001b[0m     )\n\u001b[1;32m   3086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   3087\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3088\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3090\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "589840b7960b4b2e9216db51e02ecd70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from torchvision import models, transforms\n",
        "# from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from PIL import Image\n",
        "# from torch.optim import AdamW\n",
        "# from tqdm import tqdm\n",
        "# from torch.cuda.amp import autocast, GradScaler\n",
        "# from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "# from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Paths to class folders and images\n",
        "# class_paths = {\n",
        "#     \"acute\": \"acute\",\n",
        "#     \"normal\": \"normal\",\n",
        "#     \"chronic\": \"chronic\",\n",
        "#     \"lacunar\": \"lacunar\"\n",
        "# }\n",
        "\n",
        "# # Define transforms for images\n",
        "# image_transform = transforms.Compose([\n",
        "#     transforms.Resize((112, 112)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# # Initialize video model and BioBERT\n",
        "# video_model = models.video.r3d_18(pretrained=True)\n",
        "# video_model.fc = torch.nn.Linear(video_model.fc.in_features, 256)  # Embedding layer\n",
        "# video_model = video_model.to(device)\n",
        "\n",
        "# # BioBERT for report text embedding\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "# text_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(device)\n",
        "\n",
        "# # BioGPT for report generation\n",
        "# report_generator = AutoModelForCausalLM.from_pretrained(\"microsoft/biogpt\").to(device)\n",
        "\n",
        "# # Dataset class\n",
        "# class MultiModalPatientDataset(Dataset):\n",
        "#     def __init__(self, class_paths, image_transform, max_frames=16):\n",
        "#         self.image_transform = image_transform\n",
        "#         self.max_frames = max_frames\n",
        "#         self.data = self._load_data(class_paths)\n",
        "\n",
        "#     def _load_data(self, class_paths):\n",
        "#         data = []\n",
        "#         for class_label, class_path in class_paths.items():\n",
        "#             for patient_folder in os.listdir(class_path):\n",
        "#                 patient_path = os.path.join(class_path, patient_folder)\n",
        "#                 image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "#                 report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "#                 if os.path.exists(report_path):\n",
        "#                     with open(report_path, 'r') as file:\n",
        "#                         report = file.read().strip()\n",
        "#                     data.append({\n",
        "#                         \"images\": image_paths[:self.max_frames],\n",
        "#                         \"report\": report,\n",
        "#                         \"label\": class_label\n",
        "#                     })\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "#         images = [self.image_transform(Image.open(img_path).convert(\"RGB\")) for img_path in item[\"images\"]]\n",
        "#         images = torch.stack(images).permute(1, 0, 2, 3)\n",
        "\n",
        "#         text_inputs = tokenizer(item[\"report\"], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
        "#         text_inputs = {k: v.squeeze(0).to(device) for k, v in text_inputs.items()}\n",
        "\n",
        "#         label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "#         return {\"images\": images.to(device), \"report\": text_inputs, \"label\": label}\n",
        "\n",
        "# # Initialize dataset and split\n",
        "# dataset = MultiModalPatientDataset(class_paths=class_paths, image_transform=image_transform)\n",
        "# train_size = int(0.7 * len(dataset))\n",
        "# val_size = int(0.1 * len(dataset))\n",
        "# test_size = len(dataset) - train_size - val_size\n",
        "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# # Initialize data loaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# # Training parameters\n",
        "# num_epochs = 50\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = AdamW(list(video_model.parameters()) + list(text_model.parameters()) + list(report_generator.parameters()), lr=learning_rate)\n",
        "# scaler = GradScaler()\n",
        "\n",
        "# # Initialize classifier and metrics\n",
        "# classifier = torch.nn.Linear(512, len(class_paths)).to(device)\n",
        "\n",
        "# accuracy = Accuracy(task=\"multiclass\", num_classes=len(class_paths)).to(device)\n",
        "# precision = Precision(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "# recall = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "# f1 = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     video_model.train()\n",
        "#     text_model.train()\n",
        "#     report_generator.train()\n",
        "#     classifier.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch in tqdm(train_loader):\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         images = batch[\"images\"]\n",
        "#         labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "#         with autocast():\n",
        "#             video_outputs = video_model(images)\n",
        "#             text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "#             combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "\n",
        "#             # Adjusting the classifier's input size\n",
        "#             classifier_input = torch.nn.Linear(combined_embeddings.size(1), 512).to(device)\n",
        "#             combined_embeddings = classifier_input(combined_embeddings)\n",
        "\n",
        "#             # Classification task\n",
        "#             class_outputs = classifier(combined_embeddings)\n",
        "#             class_loss = torch.nn.CrossEntropyLoss()(class_outputs, labels)\n",
        "\n",
        "#             # Report generation task\n",
        "#             report_inputs = tokenizer(batch[\"report\"], return_tensors=\"pt\").input_ids.to(device)\n",
        "#             generated_report = report_generator.generate(combined_embeddings.unsqueeze(0), max_length=50)\n",
        "#             report_loss = torch.nn.CrossEntropyLoss()(generated_report.view(-1, report_generator.config.vocab_size), report_inputs.view(-1))\n",
        "\n",
        "#             loss = class_loss + report_loss\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(optimizer)\n",
        "#         scaler.update()\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "#     avg_loss = total_loss / len(train_loader)\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# # Validation and Test\n",
        "# for loader, split in zip([val_loader, test_loader], [\"Validation\", \"Test\"]):\n",
        "#     total_accuracy, total_precision, total_recall, total_f1, total_bleu = 0, 0, 0, 0, 0\n",
        "#     video_model.eval()\n",
        "#     text_model.eval()\n",
        "#     report_generator.eval()\n",
        "#     classifier.eval()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in tqdm(loader):\n",
        "#             images = batch[\"images\"]\n",
        "#             labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "#             video_outputs = video_model(images)\n",
        "#             text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "#             combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "\n",
        "#             classifier_input = torch.nn.Linear(combined_embeddings.size(1), 512).to(device)\n",
        "#             combined_embeddings = classifier_input(combined_embeddings)\n",
        "\n",
        "#             # Classification\n",
        "#             class_outputs = classifier(combined_embeddings)\n",
        "#             preds = torch.argmax(class_outputs, dim=1)\n",
        "\n",
        "#             total_accuracy += accuracy(preds, labels)\n",
        "#             total_precision += precision(preds, labels)\n",
        "#             total_recall += recall(preds, labels)\n",
        "#             total_f1 += f1(preds, labels)\n",
        "\n",
        "#             # Report Generation\n",
        "#             generated_report = report_generator.generate(combined_embeddings.unsqueeze(0), max_length=50)\n",
        "#             generated_text = tokenizer.decode(generated_report[0], skip_special_tokens=True)\n",
        "#             reference_report = batch[\"report\"].decode('utf-8')\n",
        "#             total_bleu += sentence_bleu([reference_report.split()], generated_text.split())\n",
        "\n",
        "#     num_batches = len(loader)\n",
        "#     print(f\"{split} Accuracy: {total_accuracy / num_batches:.4f}\")\n",
        "#     print(f\"{split} Precision: {total_precision / num_batches:.4f}\")\n",
        "#     print(f\"{split} Recall: {total_recall / num_batches:.4f}\")\n",
        "#     print(f\"{split} F1 Score: {total_f1 / num_batches:.4f}\")\n",
        "#     print(f\"{split} BLEU Score (Report): {total_bleu / num_batches:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e79afe-ddda-442a-b170-81f66a98d834",
      "metadata": {
        "id": "22e79afe-ddda-442a-b170-81f66a98d834"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from torchvision import models, transforms\n",
        "# from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from PIL import Image\n",
        "# from torch.optim import AdamW\n",
        "# from tqdm import tqdm\n",
        "# from torch.cuda.amp import autocast, GradScaler\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "# from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "\n",
        "# # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Paths to class folders and images\n",
        "# class_paths = {\n",
        "#     \"acute\": \"acute\",  # Replace with actual path\n",
        "#     \"normal\": \"normal\",\n",
        "#     \"chronic\": \"chronic\",\n",
        "#     \"lacunar\": \"lacunar\"\n",
        "# }\n",
        "\n",
        "# # Define transforms for images\n",
        "# image_transform = transforms.Compose([\n",
        "#     transforms.Resize((112, 112)),  # Resizing frames to 112x112\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# # Initialize video and BioBERT models\n",
        "# video_model = models.video.r3d_18(pretrained=True)\n",
        "# video_model.fc = torch.nn.Linear(video_model.fc.in_features, 256)  # Embedding layer\n",
        "# video_model = video_model.to(device)\n",
        "\n",
        "# # BioBERT for report text embedding\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "# text_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(device)\n",
        "\n",
        "# # Dataset class with multi-frame inputs and report text\n",
        "# class MultiModalPatientDataset(Dataset):\n",
        "#     def __init__(self, class_paths, image_transform, max_frames=16):\n",
        "#         self.image_transform = image_transform\n",
        "#         self.max_frames = max_frames  # Set max_frames first\n",
        "#         self.data = self._load_data(class_paths)\n",
        "\n",
        "#     def _load_data(self, class_paths):\n",
        "#         data = []\n",
        "#         for class_label, class_path in class_paths.items():\n",
        "#             for patient_folder in os.listdir(class_path):\n",
        "#                 patient_path = os.path.join(class_path, patient_folder)\n",
        "#                 image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "#                 report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "#                 # Ensure report exists\n",
        "#                 if os.path.exists(report_path):\n",
        "#                     with open(report_path, 'r') as file:\n",
        "#                         report = file.read().strip()\n",
        "\n",
        "#                     # Append image paths, report text, and class label\n",
        "#                     data.append({\n",
        "#                         \"images\": image_paths[:self.max_frames],  # Limit to max_frames if needed\n",
        "#                         \"report\": report,\n",
        "#                         \"label\": class_label\n",
        "#                     })\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "#         images = [self.image_transform(Image.open(img_path).convert(\"RGB\")) for img_path in item[\"images\"]]\n",
        "#         images = torch.stack(images).permute(1, 0, 2, 3)  # Shape: (C, T, H, W)\n",
        "\n",
        "#         text_inputs = tokenizer(item[\"report\"], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
        "#         text_inputs = {k: v.squeeze(0).to(device) for k, v in text_inputs.items()}\n",
        "\n",
        "#         label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "#         return {\"images\": images.to(device), \"report\": text_inputs, \"label\": label}\n",
        "\n",
        "# # Initialize dataset and split\n",
        "# dataset = MultiModalPatientDataset(class_paths=class_paths, image_transform=image_transform)\n",
        "# train_size = int(0.7 * len(dataset))\n",
        "# val_size = int(0.1 * len(dataset))\n",
        "# test_size = len(dataset) - train_size - val_size\n",
        "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# # Initialize data loaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# # Training parameters\n",
        "# num_epochs = 100\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = AdamW(list(video_model.parameters()) + list(text_model.parameters()), lr=learning_rate)\n",
        "# scaler = GradScaler()\n",
        "\n",
        "# # Initialize classifier\n",
        "# classifier = torch.nn.Linear(512, len(class_paths)).to(device)  # Adjusted for combined embeddings of size 512\n",
        "\n",
        "# # Initialize metrics with the task argument\n",
        "# accuracy = Accuracy(task=\"multiclass\", num_classes=len(class_paths)).to(device)\n",
        "# precision = Precision(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "# recall = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "# f1 = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     video_model.train()\n",
        "#     text_model.train()\n",
        "#     classifier.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch in tqdm(train_loader):\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         images = batch[\"images\"]\n",
        "#         labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "#         with autocast():\n",
        "#             video_outputs = video_model(images)\n",
        "#             text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "#             combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "#             outputs = classifier(combined_embeddings)\n",
        "\n",
        "#             loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(optimizer)\n",
        "#         scaler.update()\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "#     avg_loss = total_loss / len(train_loader)\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# # Validation on the validation set\n",
        "# video_model.eval()\n",
        "# text_model.eval()\n",
        "# classifier.eval()\n",
        "# val_accuracy, val_precision, val_recall, val_f1 = 0, 0, 0, 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in tqdm(val_loader):\n",
        "#         images = batch[\"images\"]\n",
        "#         labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "#         video_outputs = video_model(images)\n",
        "#         text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "#         combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "#         outputs = classifier(combined_embeddings)\n",
        "\n",
        "#         preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "#         val_accuracy += accuracy(preds, labels)\n",
        "#         val_precision += precision(preds, labels)\n",
        "#         val_recall += recall(preds, labels)\n",
        "#         val_f1 += f1(preds, labels)\n",
        "\n",
        "# # Average metrics over the validation set\n",
        "# num_batches_val = len(val_loader)\n",
        "# print(f\"Validation Accuracy: {val_accuracy / num_batches_val:.4f}\")\n",
        "# print(f\"Validation Precision: {val_precision / num_batches_val:.4f}\")\n",
        "# print(f\"Validation Recall: {val_recall / num_batches_val:.4f}\")\n",
        "# print(f\"Validation F1 Score: {val_f1 / num_batches_val:.4f}\")\n",
        "\n",
        "# # Evaluation on the test set\n",
        "# total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in tqdm(test_loader):\n",
        "#         images = batch[\"images\"]\n",
        "#         labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "#         video_outputs = video_model(images)\n",
        "#         text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "#         combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "#         outputs = classifier(combined_embeddings)\n",
        "\n",
        "#         preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "#         total_accuracy += accuracy(preds, labels)\n",
        "#         total_precision += precision(preds, labels)\n",
        "#         total_recall += recall(preds, labels)\n",
        "#         total_f1 += f1(preds, labels)\n",
        "\n",
        "# # Average metrics over the test set\n",
        "# num_batches_test = len(test_loader)\n",
        "# print(f\"Test Accuracy: {total_accuracy / num_batches_test:.4f}\")\n",
        "# print(f\"Test Precision: {total_precision / num_batches_test:.4f}\")\n",
        "# print(f\"Test Recall: {total_recall / num_batches_test:.4f}\")\n",
        "# print(f\"Test F1 Score: {total_f1 / num_batches_test:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ffbdd9-28fd-49a9-939d-fa8d770cf8cb",
      "metadata": {
        "id": "48ffbdd9-28fd-49a9-939d-fa8d770cf8cb"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from torchvision import models, transforms\n",
        "# from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from PIL import Image\n",
        "# from torch.optim import AdamW\n",
        "# from tqdm import tqdm\n",
        "# from torch.cuda.amp import autocast, GradScaler\n",
        "# from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "# from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Paths to class folders and images\n",
        "# class_paths = {\n",
        "#     \"acute\": \"acute\",\n",
        "#     \"normal\": \"normal\",\n",
        "#     \"chronic\": \"chronic\",\n",
        "#     \"lacunar\": \"lacunar\"\n",
        "# }\n",
        "\n",
        "# # Define transforms for images\n",
        "# image_transform = transforms.Compose([\n",
        "#     transforms.Resize((112, 112)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# # Initialize video and BioBERT models\n",
        "# video_model = models.video.r3d_18(pretrained=True)\n",
        "# video_model.fc = torch.nn.Linear(video_model.fc.in_features, 256)  # Embedding layer\n",
        "# video_model = video_model.to(device)\n",
        "\n",
        "# # BioBERT for report text embedding\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "# text_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(device)\n",
        "\n",
        "# # Report generation model (using a language model with causal head for report generation)\n",
        "# report_generator = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "# # Dataset class\n",
        "# class MultiModalPatientDataset(Dataset):\n",
        "#     def __init__(self, class_paths, image_transform, max_frames=16):\n",
        "#         self.image_transform = image_transform\n",
        "#         self.max_frames = max_frames\n",
        "#         self.data = self._load_data(class_paths)\n",
        "\n",
        "#     def _load_data(self, class_paths):\n",
        "#         data = []\n",
        "#         for class_label, class_path in class_paths.items():\n",
        "#             for patient_folder in os.listdir(class_path):\n",
        "#                 patient_path = os.path.join(class_path, patient_folder)\n",
        "#                 image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "#                 report_path = os.path.join(patient_path, \"report.txt\")\n",
        "\n",
        "#                 if os.path.exists(report_path):\n",
        "#                     with open(report_path, 'r') as file:\n",
        "#                         report = file.read().strip()\n",
        "#                     data.append({\n",
        "#                         \"images\": image_paths[:self.max_frames],\n",
        "#                         \"report\": report,\n",
        "#                         \"label\": class_label\n",
        "#                     })\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "#         images = [self.image_transform(Image.open(img_path).convert(\"RGB\")) for img_path in item[\"images\"]]\n",
        "#         images = torch.stack(images).permute(1, 0, 2, 3)\n",
        "\n",
        "#         text_inputs = tokenizer(item[\"report\"], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
        "#         text_inputs = {k: v.squeeze(0).to(device) for k, v in text_inputs.items()}\n",
        "\n",
        "#         label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "#         return {\"images\": images.to(device), \"report\": text_inputs, \"label\": label}\n",
        "\n",
        "# # Initialize dataset and split\n",
        "# dataset = MultiModalPatientDataset(class_paths=class_paths, image_transform=image_transform)\n",
        "# train_size = int(0.7 * len(dataset))\n",
        "# val_size = int(0.1 * len(dataset))\n",
        "# test_size = len(dataset) - train_size - val_size\n",
        "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# # Initialize data loaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# # Training parameters\n",
        "# num_epochs = 50\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = AdamW(list(video_model.parameters()) + list(text_model.parameters()) + list(report_generator.parameters()), lr=learning_rate)\n",
        "# scaler = GradScaler()\n",
        "\n",
        "# # Initialize classifier and metrics\n",
        "# classifier = torch.nn.Linear(512, len(class_paths)).to(device)\n",
        "\n",
        "# accuracy = Accuracy(task=\"multiclass\", num_classes=len(class_paths)).to(device)\n",
        "# precision = Precision(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "# recall = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "# f1 = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(class_paths)).to(device)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     video_model.train()\n",
        "#     text_model.train()\n",
        "#     report_generator.train()\n",
        "#     classifier.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch in tqdm(train_loader):\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         images = batch[\"images\"]\n",
        "#         labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "#         with autocast():\n",
        "#             video_outputs = video_model(images)\n",
        "#             text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "#             combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "\n",
        "#             # Classification task\n",
        "#             class_outputs = classifier(combined_embeddings)\n",
        "#             class_loss = torch.nn.CrossEntropyLoss()(class_outputs, labels)\n",
        "\n",
        "#             # Report generation task\n",
        "#             report_inputs = tokenizer(batch[\"report\"], return_tensors=\"pt\").input_ids.to(device)\n",
        "#             generated_report = report_generator.generate(combined_embeddings.unsqueeze(0), max_length=50)\n",
        "#             report_loss = torch.nn.CrossEntropyLoss()(generated_report.view(-1, report_generator.config.vocab_size), report_inputs.view(-1))\n",
        "\n",
        "#             loss = class_loss + report_loss\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(optimizer)\n",
        "#         scaler.update()\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "#     avg_loss = total_loss / len(train_loader)\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# # Validation and Test\n",
        "# for loader, split in zip([val_loader, test_loader], [\"Validation\", \"Test\"]):\n",
        "#     total_accuracy, total_precision, total_recall, total_f1, total_bleu = 0, 0, 0, 0, 0\n",
        "#     video_model.eval()\n",
        "#     text_model.eval()\n",
        "#     report_generator.eval()\n",
        "#     classifier.eval()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in tqdm(loader):\n",
        "#             images = batch[\"images\"]\n",
        "#             labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "\n",
        "#             video_outputs = video_model(images)\n",
        "#             text_outputs = text_model(**batch[\"report\"]).last_hidden_state.mean(dim=1)\n",
        "#             combined_embeddings = torch.cat((video_outputs, text_outputs), dim=1)\n",
        "\n",
        "#             # Classification\n",
        "#             class_outputs = classifier(combined_embeddings)\n",
        "#             preds = torch.argmax(class_outputs, dim=1)\n",
        "\n",
        "#             total_accuracy += accuracy(preds, labels)\n",
        "#             total_precision += precision(preds, labels)\n",
        "#             total_recall += recall(preds, labels)\n",
        "#             total_f1 += f1(preds, labels)\n",
        "\n",
        "#             # Report Generation\n",
        "#             generated_report = report_generator.generate(combined_embeddings.unsqueeze(0), max_length=50)\n",
        "#             generated_text = tokenizer.decode(generated_report[0], skip_special_tokens=True)\n",
        "#             reference_report = batch[\"report\"].decode('utf-8')\n",
        "#             total_bleu += sentence_bleu([reference_report.split()], generated_text.split())\n",
        "\n",
        "#     num_batches = len(loader)\n",
        "#     print(f\"{split} Accuracy: {total_accuracy / num_batches:.4f}\")\n",
        "#     print(f\"{split} Precision: {total_precision / num_batches:.4f}\")\n",
        "#     print(f\"{split} Recall: {total_recall / num_batches:.4f}\")\n",
        "#     print(f\"{split} F1 Score: {total_f1 / num_batches:.4f}\")\n",
        "#     print(f\"{split} BLEU Score (Report): {total_bleu / num_batches:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68428ca3-0eab-406a-8afa-f1d3e8d4769e",
      "metadata": {
        "id": "68428ca3-0eab-406a-8afa-f1d3e8d4769e",
        "outputId": "0520ccfc-b5c2-4a6e-9037-e5881e92e093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: open_flamingo in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from open_flamingo) (0.8.0)\n",
            "Requirement already satisfied: einops-exts in /usr/local/lib/python3.10/dist-packages (from open_flamingo) (0.0.4)\n",
            "Requirement already satisfied: transformers>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from open_flamingo) (4.46.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from open_flamingo) (2.0.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from open_flamingo) (9.3.0)\n",
            "Requirement already satisfied: open-clip-torch>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from open_flamingo) (2.29.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.98 in /usr/local/lib/python3.10/dist-packages (from open_flamingo) (0.1.98)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (4.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->open_flamingo) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->open_flamingo) (68.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->open_flamingo) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->open_flamingo) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->open_flamingo) (18.1.8)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.16.0->open_flamingo) (0.15.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.16.0->open_flamingo) (2024.9.11)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.16.0->open_flamingo) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.16.0->open_flamingo) (4.66.6)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.16.0->open_flamingo) (0.23.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.16.0->open_flamingo) (0.4.5)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.16.0->open_flamingo) (1.0.11)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->open_flamingo) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->open_flamingo) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->open_flamingo) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->open_flamingo) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->open_flamingo) (0.20.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch>=2.16.0->open_flamingo) (2024.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->open-clip-torch>=2.16.0->open_flamingo) (0.2.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->open_flamingo) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.1->open_flamingo) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.1->open_flamingo) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.1->open_flamingo) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.1->open_flamingo) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->open_flamingo) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip  install open_flamingo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "758daf71-b10c-4b59-85c5-11abba79eb7d",
      "metadata": {
        "id": "758daf71-b10c-4b59-85c5-11abba79eb7d",
        "outputId": "7d7fae45-6d50-4077-84a8-f537690ae208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
            "Flamingo model initialized with 1046992944 trainable parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/388 [00:15<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Only single frame supported",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: [1, 1, C, H, W] to match model's expected input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Single image in the expected input shape\u001b[39;49;00m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlang_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Calculate loss (assuming the model supports loss calculation)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m series_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1517\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1520\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m result\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_flamingo/src/flamingo.py:108\u001b[0m, in \u001b[0;36mFlamingo.forward\u001b[0;34m(self, vision_x, lang_x, attention_mask, labels, clear_conditioned_layers, past_key_values, use_cache)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_encoder\u001b[38;5;241m.\u001b[39mis_conditioned()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Case: do not use caching (i.e. this is a standard forward pass);\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_vision_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition_media_locations(input_ids\u001b[38;5;241m=\u001b[39mlang_x)\n\u001b[1;32m    111\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_encoder(\n\u001b[1;32m    112\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mlang_x,\n\u001b[1;32m    113\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    117\u001b[0m )\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_flamingo/src/flamingo.py:191\u001b[0m, in \u001b[0;36mFlamingo._encode_vision_x\u001b[0;34m(self, vision_x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m vision_x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision_x should be of shape (b, T_img, F, C, H, W)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m b, T, F \u001b[38;5;241m=\u001b[39m vision_x\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m F \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly single frame supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m vision_x \u001b[38;5;241m=\u001b[39m rearrange(vision_x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb T F c h w -> (b T F) c h w\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "\u001b[0;31mAssertionError\u001b[0m: Only single frame supported"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from open_flamingo import create_model_and_transforms\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# from torch.optim import AdamW\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Define paths for each class\n",
        "# class_paths = {\n",
        "#     \"acute\": \"normal_extracted\",  # Replace with actual path\n",
        "#     \"normal\": \"normal_extracted\",\n",
        "#     \"chronic\": \"normal_extracted\",\n",
        "#     \"lacunar\": \"normal_extracted\"\n",
        "# }\n",
        "\n",
        "# # Initialize OpenFlamingo model and processors\n",
        "# model, image_processor, tokenizer = create_model_and_transforms(\n",
        "#     clip_vision_encoder_path=\"ViT-L-14\",\n",
        "#     clip_vision_encoder_pretrained=\"openai\",\n",
        "#     lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
        "#     tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
        "#     cross_attn_every_n_layers=1\n",
        "# )\n",
        "# model = model.to(device)\n",
        "\n",
        "# # Dataset class\n",
        "# class MultiImagePatientDataset(Dataset):\n",
        "#     def __init__(self, class_paths, image_processor, tokenizer, max_length=64):\n",
        "#         self.image_processor = image_processor\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.data = self._load_data(class_paths)\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def _load_data(self, class_paths):\n",
        "#         data = []\n",
        "#         for class_label, class_path in class_paths.items():\n",
        "#             for patient_folder in os.listdir(class_path):\n",
        "#                 patient_path = os.path.join(class_path, patient_folder)\n",
        "\n",
        "#                 # Collect image paths in the patient folder\n",
        "#                 image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "\n",
        "#                 # Read the text report\n",
        "#                 text_file = os.path.join(patient_path, 'report.txt')\n",
        "#                 if os.path.exists(text_file):\n",
        "#                     with open(text_file, 'r') as file:\n",
        "#                         caption = file.read().strip()\n",
        "\n",
        "#                     # Append data entry with multiple images per patient\n",
        "#                     data.append({\n",
        "#                         \"images\": image_paths,\n",
        "#                         \"caption\": caption,\n",
        "#                         \"label\": class_label\n",
        "#                     })\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "\n",
        "#         # Load and process each image in the series\n",
        "#         images = [self.image_processor(Image.open(img_path).convert(\"RGB\")).unsqueeze(0) for img_path in item[\"images\"]]\n",
        "#         images = torch.cat(images, dim=0).to(device)  # Stack images to create a tensor of shape [num_images, C, H, W]\n",
        "\n",
        "#         # Tokenize caption text\n",
        "#         caption_encoding = self.tokenizer(\n",
        "#             item[\"caption\"],\n",
        "#             max_length=self.max_length,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_tensors=\"pt\"\n",
        "#         ).to(device)\n",
        "\n",
        "#         # Convert class label to index\n",
        "#         label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "#         return {\n",
        "#             \"images\": images,  # All images as a batch\n",
        "#             \"caption_input_ids\": caption_encoding.input_ids.squeeze(0),\n",
        "#             \"caption_attention_mask\": caption_encoding.attention_mask.squeeze(0),\n",
        "#             \"label\": label\n",
        "#         }\n",
        "\n",
        "# # Initialize DataLoader\n",
        "# batch_size = 1\n",
        "# dataset = MultiImagePatientDataset(class_paths=class_paths, image_processor=image_processor, tokenizer=tokenizer)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Training parameters\n",
        "# num_epochs = 3\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training Loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch in tqdm(dataloader):\n",
        "#         images = batch[\"images\"]  # Shape: [num_images, C, H, W] for each series\n",
        "#         input_ids = batch[\"caption_input_ids\"]\n",
        "#         attention_mask = batch[\"caption_attention_mask\"]\n",
        "\n",
        "#         # Iterate over each image in the series as a separate entry\n",
        "#         series_loss = 0\n",
        "#         for img in images:\n",
        "#             img = img.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, C, H, W] to match model's expected input\n",
        "\n",
        "#             # Forward pass\n",
        "#             outputs = model(\n",
        "#                 vision_x=img,  # Single image in the expected input shape\n",
        "#                 lang_x=input_ids,\n",
        "#                 attention_mask=attention_mask\n",
        "#             )\n",
        "\n",
        "#             # Calculate loss (assuming the model supports loss calculation)\n",
        "#             series_loss += outputs.loss\n",
        "\n",
        "#         # Average loss over the series\n",
        "#         loss = series_loss / images.size(0)\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     avg_loss = total_loss / len(dataloader)\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# # Save the trained model\n",
        "# model.save_pretrained(\"flamingo_finetuned_model\")\n",
        "# tokenizer.save_pretrained(\"flamingo_finetuned_processor\")\n",
        "# print(\"Training complete. Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ed23e8-5255-4712-9be4-c2205209a22e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "744abaa75fe54712b046e955ba2b20e0",
            "cd6835f8f5e84c27a171b1d55429a681",
            "f36e4e0b894d41ddb57388998427356d",
            "a8f91c87bc404614ac78d5bf9bf8fd58",
            "1b503a0443f74141b390e19575b405c2",
            "b6e8f66cc29f475487b607c78a7e478b",
            "6fb8ad6a2301488394853d03762b63c1",
            "0d6bf17bde34430a97b0ddf4ec2623a6",
            "cec84ca8ca984fa2a2cf649e84afb92e",
            "fc6c7ac2b667457ca145f776ec84526d",
            "ee9f365b2d664741b3b35b51d5ed375f",
            "0426bd3d0e654c8e8e21b1bbea71114a",
            "2a393025bedb42b4b1fdd8020a6b2623"
          ]
        },
        "id": "20ed23e8-5255-4712-9be4-c2205209a22e",
        "outputId": "1b335ed8-5290-468d-a1d8-a19c374536a5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "744abaa75fe54712b046e955ba2b20e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/open_clip/factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd6835f8f5e84c27a171b1d55429a681",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f36e4e0b894d41ddb57388998427356d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8f91c87bc404614ac78d5bf9bf8fd58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b503a0443f74141b390e19575b405c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6e8f66cc29f475487b607c78a7e478b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_mosaic_gpt.py:   0%|          | 0.00/8.87k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b:\n",
            "- configuration_mosaic_gpt.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fb8ad6a2301488394853d03762b63c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "mosaic_gpt.py:   0%|          | 0.00/20.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d6bf17bde34430a97b0ddf4ec2623a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "param_init_fns.py:   0%|          | 0.00/15.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b:\n",
            "- param_init_fns.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cec84ca8ca984fa2a2cf649e84afb92e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "low_precision_layernorm.py:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b:\n",
            "- low_precision_layernorm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc6c7ac2b667457ca145f776ec84526d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gpt_blocks.py:   0%|          | 0.00/3.11k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee9f365b2d664741b3b35b51d5ed375f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "attention.py:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b:\n",
            "- attention.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b:\n",
            "- gpt_blocks.py\n",
            "- attention.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b:\n",
            "- mosaic_gpt.py\n",
            "- param_init_fns.py\n",
            "- low_precision_layernorm.py\n",
            "- gpt_blocks.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0426bd3d0e654c8e8e21b1bbea71114a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/5.25G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/root/.cache/huggingface/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b/50d6bc94e17812873f39c36c5f815263fa71fb80/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a393025bedb42b4b1fdd8020a6b2623",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flamingo model initialized with 1046992944 trainable parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/157 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "vision_x should be of shape (b, T_img, F, C, H, W)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlang_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n\u001b[1;32m    122\u001b[0m series_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_flamingo/src/flamingo.py:108\u001b[0m, in \u001b[0;36mFlamingo.forward\u001b[0;34m(self, vision_x, lang_x, attention_mask, labels, clear_conditioned_layers, past_key_values, use_cache)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_encoder\u001b[38;5;241m.\u001b[39mis_conditioned()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Case: do not use caching (i.e. this is a standard forward pass);\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_vision_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition_media_locations(input_ids\u001b[38;5;241m=\u001b[39mlang_x)\n\u001b[1;32m    111\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_encoder(\n\u001b[1;32m    112\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mlang_x,\n\u001b[1;32m    113\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    117\u001b[0m )\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_flamingo/src/flamingo.py:189\u001b[0m, in \u001b[0;36mFlamingo._encode_vision_x\u001b[0;34m(self, vision_x)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_vision_x\u001b[39m(\u001b[38;5;28mself\u001b[39m, vision_x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    Compute media tokens from vision input by passing it through vision encoder and conditioning language model.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    rearrange code based on https://github.com/dhansmair/flamingo-mini\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m vision_x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision_x should be of shape (b, T_img, F, C, H, W)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m     b, T, F \u001b[38;5;241m=\u001b[39m vision_x\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m F \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly single frame supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: vision_x should be of shape (b, T_img, F, C, H, W)"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from open_flamingo import create_model_and_transforms\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# from torch.optim import AdamW\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Define paths for each class\n",
        "# class_paths = {\n",
        "#     \"acute\": \"acute\",  # Replace with actual path\n",
        "#     \"normal\": \"normal\",\n",
        "#     \"chronic\": \"chronic\",\n",
        "#     \"lacunar\": \"lacunar\"\n",
        "# }\n",
        "\n",
        "# # Initialize OpenFlamingo model and processors\n",
        "# model, image_processor, tokenizer = create_model_and_transforms(\n",
        "#     clip_vision_encoder_path=\"ViT-L-14\",\n",
        "#     clip_vision_encoder_pretrained=\"openai\",\n",
        "#     lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
        "#     tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
        "#     cross_attn_every_n_layers=1\n",
        "# )\n",
        "# model = model.to(device)\n",
        "\n",
        "# # Dataset class\n",
        "# class MultiImagePatientDataset(Dataset):\n",
        "#     def __init__(self, class_paths, image_processor, tokenizer, max_length=64):\n",
        "#         self.image_processor = image_processor\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.data = self._load_data(class_paths)\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def _load_data(self, class_paths):\n",
        "#         data = []\n",
        "#         for class_label, class_path in class_paths.items():\n",
        "#             for patient_folder in os.listdir(class_path):\n",
        "#                 patient_path = os.path.join(class_path, patient_folder)\n",
        "\n",
        "#                 # Collect image paths in the patient folder\n",
        "#                 image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "\n",
        "#                 # Read the text report\n",
        "#                 text_file = os.path.join(patient_path, 'report.txt')\n",
        "#                 if os.path.exists(text_file):\n",
        "#                     with open(text_file, 'r') as file:\n",
        "#                         caption = file.read().strip()\n",
        "\n",
        "#                     # Append data entry with multiple images per patient\n",
        "#                     data.append({\n",
        "#                         \"images\": image_paths,\n",
        "#                         \"caption\": caption,\n",
        "#                         \"label\": class_label\n",
        "#                     })\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "\n",
        "#         # Load each image in the series\n",
        "#         images = [self.image_processor(Image.open(img_path).convert(\"RGB\")).unsqueeze(0) for img_path in item[\"images\"]]\n",
        "\n",
        "#         # Tokenize caption text\n",
        "#         caption_encoding = self.tokenizer(\n",
        "#             item[\"caption\"],\n",
        "#             max_length=self.max_length,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_tensors=\"pt\"\n",
        "#         ).to(device)\n",
        "\n",
        "#         # Convert class label to index\n",
        "#         label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "#         return {\n",
        "#             \"images\": images,  # List of images\n",
        "#             \"caption_input_ids\": caption_encoding.input_ids.squeeze(0),\n",
        "#             \"caption_attention_mask\": caption_encoding.attention_mask.squeeze(0),\n",
        "#             \"label\": label\n",
        "#         }\n",
        "\n",
        "# # Initialize DataLoader\n",
        "# batch_size = 1\n",
        "# dataset = MultiImagePatientDataset(class_paths=class_paths, image_processor=image_processor, tokenizer=tokenizer)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Training parameters\n",
        "# num_epochs = 50\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training Loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch in tqdm(dataloader):\n",
        "#         # Iterate over each image in the series as a separate frame\n",
        "#         series_loss = 0\n",
        "#         for img in batch[\"images\"]:\n",
        "#             img = img.to(device).unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, C, H, W]\n",
        "\n",
        "#             # Prepare text inputs\n",
        "#             input_ids = batch[\"caption_input_ids\"]\n",
        "#             attention_mask = batch[\"caption_attention_mask\"]\n",
        "\n",
        "#             # Forward pass\n",
        "#             outputs = model(\n",
        "#                 vision_x=img,\n",
        "#                 lang_x=input_ids,\n",
        "#                 attention_mask=attention_mask\n",
        "#             )\n",
        "\n",
        "#             # Accumulate loss\n",
        "#             series_loss += outputs.loss\n",
        "\n",
        "#         # Average the loss over all images in the series\n",
        "#         loss = series_loss / len(batch[\"images\"])\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     avg_loss = total_loss / len(dataloader)\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# # Save the trained model\n",
        "# model.save_pretrained(\"flamingo_finetuned_model\")\n",
        "# tokenizer.save_pretrained(\"flamingo_finetuned_processor\")\n",
        "# print(\"Training complete. Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e0a21c-8a1d-4058-bec1-23fb8392ab33",
      "metadata": {
        "id": "e9e0a21c-8a1d-4058-bec1-23fb8392ab33"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82b5246-74b8-4d4b-a720-8cdb17284be5",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6451ea5e180449b3bb5c2eb8cf457185"
          ]
        },
        "id": "c82b5246-74b8-4d4b-a720-8cdb17284be5",
        "outputId": "913e0528-a2d4-4b43-dd51-f56e74bb1c1a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6451ea5e180449b3bb5c2eb8cf457185",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.25G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
            "Flamingo model initialized with 1046992944 trainable parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/189 [00:10<?, ?it/s]\u001b[A\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "vision_x should be of shape (b, T_img, F, C, H, W)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# Mixed precision\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlang_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    124\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_flamingo/src/flamingo.py:108\u001b[0m, in \u001b[0;36mFlamingo.forward\u001b[0;34m(self, vision_x, lang_x, attention_mask, labels, clear_conditioned_layers, past_key_values, use_cache)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_encoder\u001b[38;5;241m.\u001b[39mis_conditioned()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Case: do not use caching (i.e. this is a standard forward pass);\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_vision_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition_media_locations(input_ids\u001b[38;5;241m=\u001b[39mlang_x)\n\u001b[1;32m    111\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_encoder(\n\u001b[1;32m    112\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mlang_x,\n\u001b[1;32m    113\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    117\u001b[0m )\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_flamingo/src/flamingo.py:189\u001b[0m, in \u001b[0;36mFlamingo._encode_vision_x\u001b[0;34m(self, vision_x)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_vision_x\u001b[39m(\u001b[38;5;28mself\u001b[39m, vision_x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    Compute media tokens from vision input by passing it through vision encoder and conditioning language model.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    rearrange code based on https://github.com/dhansmair/flamingo-mini\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m vision_x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision_x should be of shape (b, T_img, F, C, H, W)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m     b, T, F \u001b[38;5;241m=\u001b[39m vision_x\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m F \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly single frame supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: vision_x should be of shape (b, T_img, F, C, H, W)"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from open_flamingo import create_model_and_transforms\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# from torch.optim import AdamW\n",
        "# from tqdm import tqdm\n",
        "# from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Define paths for each class\n",
        "# class_paths = {\n",
        "#     \"acute\": \"acute\",  # Replace with actual path\n",
        "#     \"normal\": \"normal\",\n",
        "#     \"chronic\": \"chronic\",\n",
        "#     \"lacunar\": \"lacunar\"\n",
        "# }\n",
        "\n",
        "# # Initialize OpenFlamingo model and processors\n",
        "# model, image_processor, tokenizer = create_model_and_transforms(\n",
        "#     clip_vision_encoder_path=\"ViT-L-14\",\n",
        "#     clip_vision_encoder_pretrained=\"openai\",\n",
        "#     lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
        "#     tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
        "#     cross_attn_every_n_layers=1\n",
        "# )\n",
        "# model = model.to(device)\n",
        "\n",
        "# # Dataset class\n",
        "# class MultiImagePatientDataset(Dataset):\n",
        "#     def __init__(self, class_paths, image_processor, tokenizer, max_length=64):\n",
        "#         self.image_processor = image_processor\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.data = self._load_data(class_paths)\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def _load_data(self, class_paths):\n",
        "#         data = []\n",
        "#         for class_label, class_path in class_paths.items():\n",
        "#             for patient_folder in os.listdir(class_path):\n",
        "#                 patient_path = os.path.join(class_path, patient_folder)\n",
        "\n",
        "#                 # Collect image paths in the patient folder\n",
        "#                 image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "\n",
        "#                 # Read the text report\n",
        "#                 text_file = os.path.join(patient_path, 'report.txt')\n",
        "#                 if os.path.exists(text_file):\n",
        "#                     with open(text_file, 'r') as file:\n",
        "#                         caption = file.read().strip()\n",
        "\n",
        "#                     # Append data entry with multiple images per patient\n",
        "#                     data.append({\n",
        "#                         \"images\": image_paths,\n",
        "#                         \"caption\": caption,\n",
        "#                         \"label\": class_label\n",
        "#                     })\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "\n",
        "#         # Load each image in the series\n",
        "#         images = [self.image_processor(Image.open(img_path).convert(\"RGB\")).unsqueeze(0) for img_path in item[\"images\"]]\n",
        "#         images = torch.stack(images).unsqueeze(2).to(device)  # Shape: [T_img, 1, C, H, W]\n",
        "\n",
        "#         # Tokenize caption text\n",
        "#         caption_encoding = self.tokenizer(\n",
        "#             item[\"caption\"],\n",
        "#             max_length=self.max_length,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_tensors=\"pt\"\n",
        "#         ).to(device)\n",
        "\n",
        "#         # Convert class label to index\n",
        "#         label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "#         return {\n",
        "#             \"images\": images,  # Tensor of images\n",
        "#             \"caption_input_ids\": caption_encoding.input_ids.squeeze(0),\n",
        "#             \"caption_attention_mask\": caption_encoding.attention_mask.squeeze(0),\n",
        "#             \"label\": label\n",
        "#         }\n",
        "\n",
        "# # Initialize DataLoader\n",
        "# batch_size = 1\n",
        "# dataset = MultiImagePatientDataset(class_paths=class_paths, image_processor=image_processor, tokenizer=tokenizer)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Training parameters\n",
        "# num_epochs = 3\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Mixed precision scaler\n",
        "# scaler = GradScaler()\n",
        "\n",
        "# # Training Loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch in tqdm(dataloader):\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Prepare images for Flamingo by ensuring the shape is (1, T_img, 1, C, H, W)\n",
        "#         images = batch[\"images\"].unsqueeze(0)  # Shape: (1, T_img, 1, C, H, W)\n",
        "#         input_ids = batch[\"caption_input_ids\"]\n",
        "#         attention_mask = batch[\"caption_attention_mask\"]\n",
        "\n",
        "#         with autocast():  # Mixed precision\n",
        "#             outputs = model(\n",
        "#                 vision_x=images,\n",
        "#                 lang_x=input_ids,\n",
        "#                 attention_mask=attention_mask\n",
        "#             )\n",
        "#             loss = outputs.loss\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#         # Mixed-precision backward pass and optimization\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(optimizer)\n",
        "#         scaler.update()\n",
        "\n",
        "#         # Clear cache to free up unused memory\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "#     avg_loss = total_loss / len(dataloader)\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# # Save the trained model\n",
        "# model.save_pretrained(\"flamingo_finetuned_model\")\n",
        "# tokenizer.save_pretrained(\"flamingo_finetuned_processor\")\n",
        "# print(\"Training complete. Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8726b5-8cfb-4b6f-9813-c38c3b9078fd",
      "metadata": {
        "id": "bb8726b5-8cfb-4b6f-9813-c38c3b9078fd"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from open_flamingo import create_model_and_transforms\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# from torch.optim import AdamW\n",
        "# from tqdm import tqdm\n",
        "# from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Define paths for each class\n",
        "# class_paths = {\n",
        "#     \"acute\": \"normal_extracted\",  # Replace with actual path\n",
        "#     \"normal\": \"normal_extracted\",\n",
        "#     \"chronic\": \"normal_extracted\",\n",
        "#     \"lacunar\": \"normal_extracted\"\n",
        "# }\n",
        "\n",
        "# # Initialize OpenFlamingo model and processors\n",
        "# model, image_processor, tokenizer = create_model_and_transforms(\n",
        "#     clip_vision_encoder_path=\"ViT-L-14\",\n",
        "#     clip_vision_encoder_pretrained=\"openai\",\n",
        "#     lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
        "#     tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
        "#     cross_attn_every_n_layers=1\n",
        "# )\n",
        "# model = model.to(device)\n",
        "\n",
        "# # Dataset class\n",
        "# class MultiImagePatientDataset(Dataset):\n",
        "#     def __init__(self, class_paths, image_processor, tokenizer, max_length=64):\n",
        "#         self.image_processor = image_processor\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.data = self._load_data(class_paths)\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def _load_data(self, class_paths):\n",
        "#         data = []\n",
        "#         for class_label, class_path in class_paths.items():\n",
        "#             for patient_folder in os.listdir(class_path):\n",
        "#                 patient_path = os.path.join(class_path, patient_folder)\n",
        "\n",
        "#                 # Collect image paths in the patient folder\n",
        "#                 image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "\n",
        "#                 # Read the text report\n",
        "#                 text_file = os.path.join(patient_path, 'report.txt')\n",
        "#                 if os.path.exists(text_file):\n",
        "#                     with open(text_file, 'r') as file:\n",
        "#                         caption = file.read().strip()\n",
        "\n",
        "#                     # Append data entry with multiple images per patient\n",
        "#                     data.append({\n",
        "#                         \"images\": image_paths,\n",
        "#                         \"caption\": caption,\n",
        "#                         \"label\": class_label\n",
        "#                     })\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "\n",
        "#         # Load each image in the series\n",
        "#         images = [self.image_processor(Image.open(img_path).convert(\"RGB\")).unsqueeze(0) for img_path in item[\"images\"]]\n",
        "\n",
        "#         # Tokenize caption text\n",
        "#         caption_encoding = self.tokenizer(\n",
        "#             item[\"caption\"],\n",
        "#             max_length=self.max_length,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_tensors=\"pt\"\n",
        "#         ).to(device)\n",
        "\n",
        "#         # Convert class label to index\n",
        "#         label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "#         return {\n",
        "#             \"images\": images,  # List of images\n",
        "#             \"caption_input_ids\": caption_encoding.input_ids.squeeze(0),\n",
        "#             \"caption_attention_mask\": caption_encoding.attention_mask.squeeze(0),\n",
        "#             \"label\": label\n",
        "#         }\n",
        "\n",
        "# # Initialize DataLoader\n",
        "# batch_size = 1\n",
        "# dataset = MultiImagePatientDataset(class_paths=class_paths, image_processor=image_processor, tokenizer=tokenizer)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Training parameters\n",
        "# num_epochs = 3\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Mixed precision scaler\n",
        "# scaler = GradScaler()\n",
        "\n",
        "# # Training Loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch in tqdm(dataloader):\n",
        "#         series_loss = 0\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Iterate over each image in the series as a separate frame\n",
        "#         for img in batch[\"images\"]:\n",
        "#             img = img.to(device).unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, C, H, W]\n",
        "\n",
        "#             # Prepare text inputs\n",
        "#             input_ids = batch[\"caption_input_ids\"]\n",
        "#             attention_mask = batch[\"caption_attention_mask\"]\n",
        "\n",
        "#             with autocast():  # Mixed precision\n",
        "#                 outputs = model(\n",
        "#                     vision_x=img,\n",
        "#                     lang_x=input_ids,\n",
        "#                     attention_mask=attention_mask\n",
        "#                 )\n",
        "#                 series_loss += outputs.loss\n",
        "\n",
        "#         # Average the loss over all images in the series\n",
        "#         loss = series_loss / len(batch[\"images\"])\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Mixed-precision backward pass and optimization\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(optimizer)\n",
        "#         scaler.update()\n",
        "\n",
        "#         # Clear cache to free up unused memory\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "#     avg_loss = total_loss / len(dataloader)\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# # Save the trained model\n",
        "# model.save_pretrained(\"flamingo_finetuned_model\")\n",
        "# tokenizer.save_pretrained(\"flamingo_finetuned_processor\")\n",
        "# print(\"Training complete. Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62feb486-2367-4bfa-8702-8f72b4541cb8",
      "metadata": {
        "id": "62feb486-2367-4bfa-8702-8f72b4541cb8",
        "outputId": "5c340608-3c53-4cad-c310-4fada0080f24"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'FlamingoProcessor' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlamingoProcessor, FlamingoForConditionalGeneration\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'FlamingoProcessor' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from transformers import FlamingoProcessor, FlamingoForConditionalGeneration\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# from pycocoevalcap.rouge.rouge import Rouge\n",
        "# from pycocoevalcap.meteor.meteor import Meteor\n",
        "# from pycocoevalcap.cider.cider import Cider\n",
        "# from tqdm import tqdm\n",
        " # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Define paths for each class\n",
        "# class_paths = {\n",
        "#     \"actue\": \"workspace/normal_extracted\",  # Replace with actual path\n",
        "#     \"normal\": \"workspace/normal_extracted\",\n",
        "#     \"chronic\": \"workspace/normal_extracted\",\n",
        "#     \"lacunar\": \"workspace/normal_extracted\"\n",
        "# }\n",
        "\n",
        "# # Initialize model and processor\n",
        "# processor = FlamingoProcessor.from_pretrained(\"openai/flamingo\")\n",
        "# model = FlamingoForConditionalGeneration.from_pretrained(\"openai/flamingo\").to(device)\n",
        "\n",
        "# # Dataset class\n",
        "# class MultiClassPatientDataset(Dataset):\n",
        "#     def __init__(self, class_paths, processor):\n",
        "#         self.processor = processor\n",
        "#         self.data = self._load_data(class_paths)\n",
        "\n",
        "#     def _load_data(self, class_paths):\n",
        "#         data = []\n",
        "#         for class_label, class_path in class_paths.items():\n",
        "#             for patient_folder in os.listdir(class_path):\n",
        "#                 patient_path = os.path.join(class_path, patient_folder)\n",
        "\n",
        "#                 # Collect image paths in the patient folder\n",
        "#                 image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "\n",
        "#                 # Read the text report\n",
        "#                 text_file = os.path.join(patient_path, 'report.txt')\n",
        "#                 if os.path.exists(text_file):\n",
        "#                     with open(text_file, 'r') as file:\n",
        "#                         caption = file.read().strip()\n",
        "\n",
        "#                     # Append data entry\n",
        "#                     data.append({\n",
        "#                         \"images\": image_paths,\n",
        "#                         \"caption\": caption,\n",
        "#                         \"label\": class_label\n",
        "#                     })\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.data[idx]\n",
        "\n",
        "#         # Load and process images\n",
        "#         images = [self.processor(Image.open(img_path).convert(\"RGB\"), return_tensors=\"pt\").pixel_values.squeeze(0) for img_path in item[\"images\"]]\n",
        "\n",
        "#         # Map class label to integer\n",
        "#         label = list(class_paths.keys()).index(item[\"label\"])\n",
        "\n",
        "#         # Get the caption\n",
        "#         caption = item[\"caption\"]\n",
        "\n",
        "#         return {\"images\": torch.stack(images), \"caption\": caption, \"label\": label}\n",
        "\n",
        "# # Initialize DataLoader\n",
        "# dataset = MultiClassPatientDataset(class_paths=class_paths, processor=processor)\n",
        "# dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# # Training parameters\n",
        "# num_epochs = 3\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Evaluation metrics\n",
        "# rouge = Rouge()\n",
        "# meteor = Meteor()\n",
        "# cider = Cider()\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     epoch_loss = 0\n",
        "#     for batch in tqdm(dataloader):\n",
        "#         images = batch[\"images\"][0].to(device)  # Images batch\n",
        "#         captions = batch[\"caption\"]\n",
        "#         labels = batch[\"label\"].to(device)  # Classification labels\n",
        "\n",
        "#         # Prepare inputs\n",
        "#         inputs = processor(text=[captions], images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "#         # Forward pass for captioning\n",
        "#         outputs = model(**inputs, labels=inputs.input_ids)\n",
        "#         caption_loss = outputs.loss\n",
        "\n",
        "#         # Forward pass for classification\n",
        "#         logits = outputs.logits.mean(dim=1)  # Simple aggregation over images\n",
        "#         classification_loss = torch.nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "#         # Combine losses\n",
        "#         total_loss = caption_loss + classification_loss\n",
        "#         epoch_loss += total_loss.item()\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         total_loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# # Evaluation\n",
        "# model.eval()\n",
        "# bleu_scores, rouge_scores, meteor_scores, cider_scores, accuracy_scores = [], [], [], [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in tqdm(dataloader):\n",
        "#         images = batch[\"images\"][0].to(device)\n",
        "#         reference_caption = batch[\"caption\"]\n",
        "#         labels = batch[\"label\"].to(device)\n",
        "\n",
        "#         # Generate captions\n",
        "#         inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "#         generated_ids = model.generate(**inputs)\n",
        "#         generated_caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "#         # Classification\n",
        "#         logits = model(**inputs).logits.mean(dim=1)\n",
        "#         predicted_class = logits.argmax(dim=-1)\n",
        "#         accuracy_scores.append(accuracy_score(labels.cpu(), predicted_class.cpu()))\n",
        "\n",
        "#         # Calculate BLEU score\n",
        "#         reference_tokens = [reference_caption.split()]\n",
        "#         generated_tokens = generated_caption.split()\n",
        "#         bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
        "#         bleu_scores.append(bleu_score)\n",
        "\n",
        "#         # Calculate ROUGE, METEOR, CIDEr scores\n",
        "#         rouge_score = rouge.compute_score({0: [reference_caption]}, {0: [generated_caption]})[0][1]\n",
        "#         meteor_score = meteor.compute_score({0: [reference_caption]}, {0: [generated_caption]})[0]\n",
        "#         cider_score = cider.compute_score({0: [reference_caption]}, {0: [generated_caption]})[0]\n",
        "\n",
        "#         rouge_scores.append(rouge_score)\n",
        "#         meteor_scores.append(meteor_score)\n",
        "#         cider_scores.append(cider_score)\n",
        "\n",
        "# # Aggregate evaluation results\n",
        "# avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "# avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "# avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "# avg_cider = sum(cider_scores) / len(cider_scores)\n",
        "# avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "\n",
        "# print(\"Evaluation Results:\")\n",
        "# print(f\"BLEU Score: {avg_bleu:.4f}\")\n",
        "# print(f\"ROUGE Score: {avg_rouge:.4f}\")\n",
        "# print(f\"METEOR Score: {avg_meteor:.4f}\")\n",
        "# print(f\"CIDEr Score: {avg_cider:.4f}\")\n",
        "# print(f\"Classification Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "# # Save model\n",
        "# model.save_pretrained(\"flamingo_finetuned_model\")\n",
        "# processor.save_pretrained(\"flamingo_finetuned_processor\")\n",
        "# print(\"Model training and evaluation complete. Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db56655-9073-420b-8e87-a083d23b52b3",
      "metadata": {
        "id": "5db56655-9073-420b-8e87-a083d23b52b3"
      },
      "outputs": [],
      "source": [
        "# # Training parameters\n",
        "# num_epochs = 3\n",
        "# learning_rate = 1e-5\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Evaluation metrics\n",
        "# rouge = Rouge()\n",
        "# meteor = Meteor()\n",
        "# cider = Cider()\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     epoch_loss = 0\n",
        "#     for batch in tqdm(dataloader):\n",
        "#         images = batch[\"images\"][0].to(device)  # Images batch\n",
        "#         captions = batch[\"caption\"]\n",
        "#         labels = batch[\"label\"].to(device)  # Classification labels\n",
        "\n",
        "#         # Prepare inputs\n",
        "#         inputs = processor(text=[captions], images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "#         # Forward pass for captioning\n",
        "#         outputs = model(**inputs, labels=inputs.input_ids)\n",
        "#         caption_loss = outputs.loss\n",
        "\n",
        "#         # Forward pass for classification\n",
        "#         logits = outputs.logits.mean(dim=1)  # Simple aggregation over images\n",
        "#         classification_loss = torch.nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "#         # Combine losses\n",
        "#         total_loss = caption_loss + classification_loss\n",
        "#         epoch_loss += total_loss.item()\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         total_loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# # Evaluation\n",
        "# model.eval()\n",
        "# bleu_scores, rouge_scores, meteor_scores, cider_scores, accuracy_scores = [], [], [], [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in tqdm(dataloader):\n",
        "#         images = batch[\"images\"][0].to(device)\n",
        "#         reference_caption = batch[\"caption\"]\n",
        "#         labels = batch[\"label\"].to(device)\n",
        "\n",
        "#         # Generate captions\n",
        "#         inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "#         generated_ids = model.generate(**inputs)\n",
        "#         generated_caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "#         # Classification\n",
        "#         logits = model(**inputs).logits.mean(dim=1)\n",
        "#         predicted_class = logits.argmax(dim=-1)\n",
        "#         accuracy_scores.append(accuracy_score(labels.cpu(), predicted_class.cpu()))\n",
        "\n",
        "#         # Calculate BLEU score\n",
        "#         reference_tokens = [reference_caption.split()]\n",
        "#         generated_tokens = generated_caption.split()\n",
        "#         bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
        "#         bleu_scores.append(bleu_score)\n",
        "\n",
        "#         # Calculate ROUGE, METEOR, CIDEr scores\n",
        "#         rouge_score = rouge.compute_score({0: [reference_caption]}, {0: [generated_caption]})[0][1]\n",
        "#         meteor_score = meteor.compute_score({0: [reference_caption]}, {0: [generated_caption]})[0]\n",
        "#         cider_score = cider.compute_score({0: [reference_caption]}, {0: [generated_caption]})[0]\n",
        "\n",
        "#         rouge_scores.append(rouge_score)\n",
        "#         meteor_scores.append(meteor_score)\n",
        "#         cider_scores.append(cider_score)\n",
        "\n",
        "# # Aggregate evaluation results\n",
        "# avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "# avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "# avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "# avg_cider = sum(cider_scores) / len(cider_scores)\n",
        "# avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "\n",
        "# print(\"Evaluation Results:\")\n",
        "# print(f\"BLEU Score: {avg_bleu:.4f}\")\n",
        "# print(f\"ROUGE Score: {avg_rouge:.4f}\")\n",
        "# print(f\"METEOR Score: {avg_meteor:.4f}\")\n",
        "# print(f\"CIDEr Score: {avg_cider:.4f}\")\n",
        "# print(f\"Classification Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "# # Save model\n",
        "# model.save_pretrained(\"flamingo_finetuned_model\")\n",
        "# processor.save_pretrained(\"flamingo_finetuned_processor\")\n",
        "# print(\"Model training and evaluation complete. Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be8adf3-10ab-48f4-8401-ff57e5c741a5",
      "metadata": {
        "id": "5be8adf3-10ab-48f4-8401-ff57e5c741a5",
        "outputId": "02f101e2-4f95-434a-997c-26adb996fbdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: The file is not a valid archive or is corrupted.\n"
          ]
        }
      ],
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# # Define the path to the archive file and the extraction directory\n",
        "# zip_file_path = \"workspace/normal.zip\"\n",
        "# extract_to = 'normal_extracted'\n",
        "\n",
        "# # Create the extraction directory if it doesn't exist\n",
        "# os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "# # Extract the archive using shutil\n",
        "# try:\n",
        "#     shutil.unpack_archive(zip_file_path, extract_to)\n",
        "#     print(f\"Extraction complete! Files are extracted to: {extract_to}\")\n",
        "# except shutil.ReadError:\n",
        "#     print(\"Error: The file is not a valid archive or is corrupted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be31646-7c8c-4cc6-97a7-6e487462940c",
      "metadata": {
        "id": "4be31646-7c8c-4cc6-97a7-6e487462940c",
        "outputId": "4cb4e055-e5c8-4c50-889e-71e778cd1d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting video-transformers\n",
            "  Downloading video_transformers-0.0.9-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting accelerate<0.15.0,>=0.14.0 (from video-transformers)\n",
            "  Downloading accelerate-0.14.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting evaluate<0.4.0,>=0.3.0 (from video-transformers)\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: transformers>=4.25.0 in /usr/local/lib/python3.10/dist-packages (from video-transformers) (4.46.1)\n",
            "Collecting timm<0.7.0,>=0.6.12 (from video-transformers)\n",
            "  Downloading timm-0.6.13-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting click==8.0.4 (from video-transformers)\n",
            "  Downloading click-8.0.4-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting balanced-loss (from video-transformers)\n",
            "  Downloading balanced_loss-0.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from video-transformers) (1.5.2)\n",
            "Collecting tensorboard (from video-transformers)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opencv-python (from video-transformers)\n",
            "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-5.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting huggingface-hub<0.12.0,>=0.11.0 (from video-transformers)\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.15.0,>=0.14.0->video-transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.15.0,>=0.14.0->video-transformers) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<0.15.0,>=0.14.0->video-transformers) (5.9.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.15.0,>=0.14.0->video-transformers) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.15.0,>=0.14.0->video-transformers) (2.0.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (2.20.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate<0.4.0,>=0.3.0->video-transformers) (2024.5.0)\n",
            "Collecting responses<0.19 (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (4.0.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "INFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-5.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-5.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==1.4.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-5.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-5.0.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-5.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "INFO: pip is still looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading gradio-4.43.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting fastapi<0.113.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading fastapi-0.112.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.42.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.41.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.40.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==1.2.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-1.2.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==1.1.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.38.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting altair<6.0,>=5.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting gradio-client==1.1.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-1.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.38.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==1.0.2 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.37.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.36.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==1.0.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-1.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.36.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.35.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.33.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.17.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.17.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.32.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.32.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.32.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.31.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.16.4 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.16.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.31.4-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.31.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.16.3 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.16.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.31.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.31.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.31.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.16.2 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.16.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.29.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.16.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.16.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.28.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.16.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.16.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.28.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.28.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.28.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.27.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.15.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.15.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.26.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.25.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.15.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.15.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.24.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.14.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.14.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.23.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.13.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.13.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.21.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.12.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.12.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.20.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.11.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.11.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.20.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.19.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.10.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.19.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.10.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.19.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.18.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.17.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.9.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.16.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.8.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.8.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.15.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.14.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting gradio-client==0.8.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.13.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-4.11.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.7.3 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.10.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.9.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.9.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.7.2 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.7.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.8.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.7.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.7.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.7.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.4.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.1.2-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.1.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.0.2-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.0.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-4.0.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.6.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-3.50.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.50.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.49.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.48.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.47.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.6.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.6.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-3.47.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.46.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.5.3 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.5.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-3.46.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.45.2-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.45.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.5.2 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-3.45.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.44.4-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.5.1 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-3.44.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio-client==0.5.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-3.44.2-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.44.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.44.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.43.2-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.43.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.43.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.42.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.41.2-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.41.1-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.41.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.40.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (3.10.10)\n",
            "  Downloading gradio-3.40.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.39.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.38.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.37.0-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading gradio-3.36.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-3.36.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-3.35.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-3.35.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-3.35.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-3.34.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-3.33.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-3.33.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading gradio-3.32.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.31.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.30.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.29.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.28.3-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.28.2-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.28.1-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.28.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.27.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.26.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting gradio-client==0.1.2 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading gradio_client-0.1.2-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting gradio>=3.1.6 (from video-transformers)\n",
            "  Downloading gradio-3.25.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.24.1-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.24.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.23.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.22.1-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.22.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.21.0-py3-none-any.whl.metadata (14 kB)\n",
            "  Downloading gradio-3.20.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (3.1.2)\n",
            "Collecting markdown-it-py>=2.0.0 (from markdown-it-py[linkify]>=2.0.0->gradio>=3.1.6->video-transformers)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (3.9.2)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting orjson (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading orjson-3.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (9.3.0)\n",
            "Collecting pycryptodome (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pydantic (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "Collecting pydub (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading python_multipart-0.0.17-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (4.4.0)\n",
            "Collecting uvicorn (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting websockets>=10.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.12.0,>=0.11.0->video-transformers) (3.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm<0.7.0,>=0.6.12->video-transformers) (0.15.2)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers>=4.25.0 (from video-transformers)\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
            "  Downloading transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
            "  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
            "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "  Downloading transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
            "  Downloading transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "  Downloading transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "  Downloading transformers-4.35.1-py3-none-any.whl.metadata (123 kB)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl.metadata (119 kB)\n",
            "  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
            "  Downloading transformers-4.30.1-py3-none-any.whl.metadata (113 kB)\n",
            "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl.metadata (112 kB)\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl.metadata (112 kB)\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl.metadata (111 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.0->video-transformers) (2024.9.11)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.0->video-transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->video-transformers) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->video-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->video-transformers) (3.5.0)\n",
            "Collecting absl-py>=0.4 (from tensorboard->video-transformers)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard->video-transformers)\n",
            "  Downloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard->video-transformers)\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (3.20.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (68.2.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard->video-transformers) (1.16.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->video-transformers)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard->video-transformers)\n",
            "  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio>=3.1.6->video-transformers) (4.19.2)\n",
            "Collecting narwhals>=1.5.2 (from altair<6.0,>=5.0->gradio>=3.1.6->video-transformers)\n",
            "  Downloading narwhals-1.12.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-extensions (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate<0.4.0,>=0.3.0->video-transformers) (18.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate<0.4.0,>=0.3.0->video-transformers) (0.6)\n",
            "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting dill (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.3-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.13.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting dill (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio>=3.1.6->video-transformers) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio>=3.1.6->video-transformers) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio>=3.1.6->video-transformers) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio>=3.1.6->video-transformers) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio>=3.1.6->video-transformers) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio>=3.1.6->video-transformers) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio>=3.1.6->video-transformers) (4.0.3)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio>=3.1.6->video-transformers)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio>=3.1.6->video-transformers)\n",
            "  Downloading linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio>=3.1.6->video-transformers)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate<0.4.0,>=0.3.0->video-transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate<0.4.0,>=0.3.0->video-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate<0.4.0,>=0.3.0->video-transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate<0.4.0,>=0.3.0->video-transformers) (2022.12.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (2.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (18.1.8)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1.0,>=0.115.2->gradio>=3.1.6->video-transformers)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic->gradio>=3.1.6->video-transformers)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic->gradio>=3.1.6->video-transformers)\n",
            "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=3.1.6->video-transformers)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=3.1.6->video-transformers) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio>=3.1.6->video-transformers)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from evaluate<0.4.0,>=0.3.0->video-transformers)\n",
            "  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate<0.4.0,>=0.3.0->video-transformers) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate<0.4.0,>=0.3.0->video-transformers) (2024.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=3.1.6->video-transformers) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=3.1.6->video-transformers) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=3.1.6->video-transformers) (0.12.0)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio>=3.1.6->video-transformers)\n",
            "  Downloading uc_micro_py-1.0.3-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=3.1.6->video-transformers) (1.1.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp~=3.0->gradio>=3.1.6->video-transformers) (0.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (1.3.0)\n",
            "Downloading video_transformers-0.0.9-py3-none-any.whl (46 kB)\n",
            "Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "Downloading accelerate-0.14.0-py3-none-any.whl (175 kB)\n",
            "Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "Downloading gradio-3.20.1-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading balanced_loss-0.1.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m168.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m158.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Downloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.13.2-py3-none-any.whl (512 kB)\n",
            "Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "Downloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m145.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "Downloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "Downloading orjson-3.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m135.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading python_multipart-0.0.17-py3-none-any.whl (24 kB)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Downloading linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading narwhals-1.12.1-py3-none-any.whl (195 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "Downloading uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: tokenizers, pydub, werkzeug, websockets, uc-micro-py, typing-extensions, tensorboard-data-server, python-multipart, pycryptodome, orjson, opencv-python, narwhals, mdurl, markdown, h11, grpcio, ffmpy, dill, click, annotated-types, aiofiles, absl-py, uvicorn, tensorboard, starlette, responses, pydantic-core, multiprocess, markdown-it-py, linkify-it-py, huggingface-hub, httpcore, transformers, pydantic, mdit-py-plugins, httpx, fastapi, altair, gradio, datasets, evaluate, timm, balanced-loss, accelerate, video-transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.1\n",
            "    Uninstalling tokenizers-0.20.1:\n",
            "      Successfully uninstalled tokenizers-0.20.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.23.4\n",
            "    Uninstalling huggingface-hub-0.23.4:\n",
            "      Successfully uninstalled huggingface-hub-0.23.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.1\n",
            "    Uninstalling transformers-4.46.1:\n",
            "      Successfully uninstalled transformers-4.46.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.20.0\n",
            "    Uninstalling datasets-2.20.0:\n",
            "      Successfully uninstalled datasets-2.20.0\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.11\n",
            "    Uninstalling timm-1.0.11:\n",
            "      Successfully uninstalled timm-1.0.11\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.34.2\n",
            "    Uninstalling accelerate-0.34.2:\n",
            "      Successfully uninstalled accelerate-0.34.2\n",
            "Successfully installed absl-py-2.1.0 accelerate-0.14.0 aiofiles-24.1.0 altair-5.4.1 annotated-types-0.7.0 balanced-loss-0.1.0 click-8.0.4 datasets-2.13.2 dill-0.3.6 evaluate-0.3.0 fastapi-0.115.4 ffmpy-0.4.0 gradio-3.20.1 grpcio-1.67.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.11.1 linkify-it-py-2.0.3 markdown-3.7 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 multiprocess-0.70.14 narwhals-1.12.1 opencv-python-4.10.0.84 orjson-3.10.11 pycryptodome-3.21.0 pydantic-2.9.2 pydantic-core-2.23.4 pydub-0.25.1 python-multipart-0.0.17 responses-0.18.0 starlette-0.41.2 tensorboard-2.18.0 tensorboard-data-server-0.7.2 timm-0.6.13 tokenizers-0.13.3 transformers-4.29.0 typing-extensions-4.12.2 uc-micro-py-1.0.3 uvicorn-0.32.0 video-transformers-0.0.9 websockets-13.1 werkzeug-3.1.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install video-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0979e621-7618-4dce-8cf1-d72627816e44",
      "metadata": {
        "id": "0979e621-7618-4dce-8cf1-d72627816e44",
        "outputId": "4534c3b2-7b1b-4325-bc14-0845c306d8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: video-transformers in /usr/local/lib/python3.10/dist-packages (0.0.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: accelerate<0.15.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from video-transformers) (0.14.0)\n",
            "Requirement already satisfied: evaluate<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from video-transformers) (0.3.0)\n",
            "Requirement already satisfied: transformers>=4.25.0 in /usr/local/lib/python3.10/dist-packages (from video-transformers) (4.29.0)\n",
            "Requirement already satisfied: timm<0.7.0,>=0.6.12 in /usr/local/lib/python3.10/dist-packages (from video-transformers) (0.6.13)\n",
            "Requirement already satisfied: click==8.0.4 in /usr/local/lib/python3.10/dist-packages (from video-transformers) (8.0.4)\n",
            "Requirement already satisfied: balanced-loss in /usr/local/lib/python3.10/dist-packages (from video-transformers) (0.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from video-transformers) (1.5.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from video-transformers) (2.18.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from video-transformers) (4.10.0.84)\n",
            "Requirement already satisfied: gradio>=3.1.6 in /usr/local/lib/python3.10/dist-packages (from video-transformers) (3.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.15.0,>=0.14.0->video-transformers) (1.26.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<0.15.0,>=0.14.0->video-transformers) (5.9.6)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.15.0,>=0.14.0->video-transformers) (2.0.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (2.13.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (2.2.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (0.70.14)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.4.0,>=0.3.0->video-transformers) (0.18.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (24.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (3.10.10)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (5.4.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (0.115.4)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (0.4.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (0.27.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio>=3.1.6->video-transformers) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (3.9.2)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (0.3.3)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (3.10.11)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (9.3.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (3.21.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (0.0.17)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (0.32.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.1.6->video-transformers) (13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm<0.7.0,>=0.6.12->video-transformers) (0.15.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.0->video-transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.0->video-transformers) (0.13.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->video-transformers) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->video-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->video-transformers) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (3.20.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (68.2.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard->video-transformers) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->video-transformers) (3.1.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio>=3.1.6->video-transformers) (4.19.2)\n",
            "Requirement already satisfied: narwhals>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio>=3.1.6->video-transformers) (1.12.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate<0.4.0,>=0.3.0->video-transformers) (18.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio>=3.1.6->video-transformers) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio>=3.1.6->video-transformers) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio>=3.1.6->video-transformers) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio>=3.1.6->video-transformers) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio>=3.1.6->video-transformers) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio>=3.1.6->video-transformers) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio>=3.1.6->video-transformers) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio>=3.1.6->video-transformers) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio>=3.1.6->video-transformers) (2.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (2.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (18.1.8)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio>=3.1.6->video-transformers) (0.41.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->gradio>=3.1.6->video-transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->gradio>=3.1.6->video-transformers) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio>=3.1.6->video-transformers) (4.0.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio>=3.1.6->video-transformers) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio>=3.1.6->video-transformers) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->gradio>=3.1.6->video-transformers) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio>=3.1.6->video-transformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate<0.4.0,>=0.3.0->video-transformers) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate<0.4.0,>=0.3.0->video-transformers) (2024.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio>=3.1.6->video-transformers) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio>=3.1.6->video-transformers) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio>=3.1.6->video-transformers) (0.12.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio>=3.1.6->video-transformers) (1.0.3)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio>=3.1.6->video-transformers) (1.1.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->gradio>=3.1.6->video-transformers) (0.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate<0.15.0,>=0.14.0->video-transformers) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface_hub video-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9232615-4bd3-491b-8b2c-3975d04763cc",
      "metadata": {
        "id": "c9232615-4bd3-491b-8b2c-3975d04763cc",
        "outputId": "a66ed24a-7da8-4870-91c9-739e13b0c82c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting huggingface_hub==0.12.1\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.12.1) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.12.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.12.1) (4.66.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.12.1) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.12.1) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.12.1) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.12.1) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.12.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.12.1) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.12.1) (2022.12.7)\n",
            "Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "Installing collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.11.1\n",
            "    Uninstalling huggingface-hub-0.11.1:\n",
            "      Successfully uninstalled huggingface-hub-0.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "video-transformers 0.0.9 requires huggingface-hub<0.12.0,>=0.11.0, but you have huggingface-hub 0.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.12.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install huggingface_hub==0.12.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee48b53-2f23-42b6-ae54-027aee55402e",
      "metadata": {
        "id": "9ee48b53-2f23-42b6-ae54-027aee55402e",
        "outputId": "6d09040f-c2ab-44b1-af27-63e2a5b21e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorchvideo\n",
            "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting fvcore (from pytorchvideo)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting av (from pytorchvideo)\n",
            "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting parameterized (from pytorchvideo)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting iopath (from pytorchvideo)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.26.3)\n",
            "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.66.6)\n",
            "Collecting termcolor>=1.1 (from fvcore->pytorchvideo)\n",
            "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (9.3.0)\n",
            "Collecting tabulate (from fvcore->pytorchvideo)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.2)\n",
            "Collecting portalocker (from iopath->pytorchvideo)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188686 sha256=ad00844333266b2bc2577a5eb8348cafaa2407ec09aab342fda28ce6a643edc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=e2268f91994f581c9e5d2e1590469560c345ffc9e51cfca1524f85b4b73ec035\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=02bb201008a459d4377577986a6e5d978feb1abf46dcf890006acf81d6b6f4fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: yacs, termcolor, tabulate, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\n",
            "Successfully installed av-13.1.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-2.10.1 pytorchvideo-0.1.5 tabulate-0.9.0 termcolor-2.5.0 yacs-0.1.8\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pytorchvideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60c0b13-9cfe-4eb5-9482-9812b103e3b8",
      "metadata": {
        "id": "b60c0b13-9cfe-4eb5-9482-9812b103e3b8"
      },
      "outputs": [],
      "source": [
        "# Define paths for each class\n",
        "class_paths = {\n",
        "    \"acute\": \"acute\",  # Replace with actual path\n",
        "    \"normal\": \"normal\",\n",
        "    \"chronic\": \"chronic\",\n",
        "    \"lacunar\": \"lacunar\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff1e390-e329-487c-bd52-b49ba6e83dd1",
      "metadata": {
        "id": "fff1e390-e329-487c-bd52-b49ba6e83dd1",
        "outputId": "3a3c7738-a107-43de-dcfa-7ccdebafc741"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/191 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [1, 1, 3, 16, 112, 112]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 76\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(outputs, labels)\n\u001b[1;32m     78\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/video/resnet.py:251\u001b[0m, in \u001b[0;36mVideoResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    254\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [1, 1, 3, 16, 112, 112]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Video Classification model\n",
        "model = models.video.r3d_18(pretrained=True)  # Example: using a 3D ResNet for video classification\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 4)  # Set the number of output classes\n",
        "model = model.to(device)\n",
        "\n",
        "# Image transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),  # Resize frames to 112x112\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Dataset class for multi-frame inputs\n",
        "# Dataset class for multi-frame inputs\n",
        "class MultiFramePatientDataset(Dataset):\n",
        "    def __init__(self, class_paths, transform, max_frames=16):\n",
        "        self.transform = transform\n",
        "        self.max_frames = max_frames  # Move this line before calling _load_data\n",
        "        self.data = self._load_data(class_paths)\n",
        "\n",
        "    def _load_data(self, class_paths):\n",
        "        data = []\n",
        "        for class_label, class_path in class_paths.items():\n",
        "            for patient_folder in os.listdir(class_path):\n",
        "                patient_path = os.path.join(class_path, patient_folder)\n",
        "                image_paths = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.endswith('.png')]\n",
        "                data.append({\n",
        "                    \"images\": image_paths[:self.max_frames],\n",
        "                    \"label\": class_label\n",
        "                })\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        images = [self.transform(Image.open(img_path).convert(\"RGB\")) for img_path in item[\"images\"]]\n",
        "        images = torch.stack(images).permute(1, 0, 2, 3)  # Shape: (C, T, H, W)\n",
        "        label = list(class_paths.keys()).index(item[\"label\"])\n",
        "        return {\"images\": images.to(device), \"label\": label}\n",
        "\n",
        "\n",
        "# Initialize DataLoader\n",
        "batch_size = 1\n",
        "dataset = MultiFramePatientDataset(class_paths=class_paths, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 3\n",
        "learning_rate = 1e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        images = batch[\"images\"].unsqueeze(0)  # Shape: (1, C, T, H, W)\n",
        "        labels = torch.tensor([batch[\"label\"]], device=device)\n",
        "        with autocast():\n",
        "            outputs = model(images)\n",
        "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"video_classification_model.pth\")\n",
        "print(\"Training complete. Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1670d3f-3bdc-4372-aeac-6707571da57c",
      "metadata": {
        "id": "c1670d3f-3bdc-4372-aeac-6707571da57c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4aab476f3cc3459789aff9e7b782d35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9eb498ce7f93411ea79db847e00661c6",
              "IPY_MODEL_eb01d8caed9745c8b8f19f9f219e5ade",
              "IPY_MODEL_3b32465add24494c9daccb8fa368ff06",
              "IPY_MODEL_53e4599e8b8d494da7da6debe5a4b67d"
            ],
            "layout": "IPY_MODEL_34e803869b984e11a15e4e8fbb39003b"
          }
        },
        "b98bb087ba41450d8c2b55e2e7c6dba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a5943f7466a4fb99eff195af41214a5",
            "placeholder": "​",
            "style": "IPY_MODEL_e68a2472e36a45d289b46eace8626a8f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "743adea0ea094ecd8cb99e205d722af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_2aeafbce1c0d4ea986dbbb2ee712ae8d",
            "placeholder": "​",
            "style": "IPY_MODEL_c525623b4f16408b8a5e800978fc2632",
            "value": ""
          }
        },
        "01cbbd23b1d747a99d06ba82bbe621e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_52fecc4e20a04aecb67ab6c5dd7e4f7f",
            "style": "IPY_MODEL_37a612ddd86749c2b77e5ab613c7b627",
            "value": true
          }
        },
        "7e6a921009464110a87887c0044e323d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_65149b73c4a24a5d8940bdb6fa62b5ff",
            "style": "IPY_MODEL_2c36978ec8c74bbaa779b2074c0422be",
            "tooltip": ""
          }
        },
        "d8682fe007bf48f6b0d58e849d194fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d96096b5e76749348aacb17fcb159041",
            "placeholder": "​",
            "style": "IPY_MODEL_02d00321720643fbb186fa50c199389f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "34e803869b984e11a15e4e8fbb39003b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7a5943f7466a4fb99eff195af41214a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e68a2472e36a45d289b46eace8626a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2aeafbce1c0d4ea986dbbb2ee712ae8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c525623b4f16408b8a5e800978fc2632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52fecc4e20a04aecb67ab6c5dd7e4f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37a612ddd86749c2b77e5ab613c7b627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65149b73c4a24a5d8940bdb6fa62b5ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c36978ec8c74bbaa779b2074c0422be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d96096b5e76749348aacb17fcb159041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02d00321720643fbb186fa50c199389f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a2d786652994bcba321f6275fefebbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c151a87404e4a6f82a8b728a41dfe93",
            "placeholder": "​",
            "style": "IPY_MODEL_0d2b81ca1d6043adb8952d72a8d1a215",
            "value": "Connecting..."
          }
        },
        "5c151a87404e4a6f82a8b728a41dfe93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d2b81ca1d6043adb8952d72a8d1a215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eb498ce7f93411ea79db847e00661c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2a202af9e0415fb2e1ff26b085aedf",
            "placeholder": "​",
            "style": "IPY_MODEL_8cbf03cca12f416d85d9cf09567f234b",
            "value": "Token is valid (permission: fineGrained)."
          }
        },
        "eb01d8caed9745c8b8f19f9f219e5ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b9f5a89e924897a74880a48a777491",
            "placeholder": "​",
            "style": "IPY_MODEL_9c38d3a176ac488f938282b1de57fcdc",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "3b32465add24494c9daccb8fa368ff06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33342a7ec1f946fa8c55b207f71f54b7",
            "placeholder": "​",
            "style": "IPY_MODEL_8d426284d16f4d8fa9cca0524b0e4637",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "53e4599e8b8d494da7da6debe5a4b67d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818bc7bebd2848619496648303eb8155",
            "placeholder": "​",
            "style": "IPY_MODEL_402d057a99804a6e95bd50c895f1dd84",
            "value": "Login successful"
          }
        },
        "4d2a202af9e0415fb2e1ff26b085aedf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbf03cca12f416d85d9cf09567f234b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24b9f5a89e924897a74880a48a777491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c38d3a176ac488f938282b1de57fcdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33342a7ec1f946fa8c55b207f71f54b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d426284d16f4d8fa9cca0524b0e4637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "818bc7bebd2848619496648303eb8155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "402d057a99804a6e95bd50c895f1dd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b818c2a2ec84fa5b2bcdaeff3e1951f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b923f3e98eaa4b55ab57d3681b16a778",
              "IPY_MODEL_c41114cae39c4749805eae6732908050",
              "IPY_MODEL_7aaee5b635624a7e8a054348147a4dfc"
            ],
            "layout": "IPY_MODEL_0d82a1c6f1d947cda5fd01856cafbf18"
          }
        },
        "b923f3e98eaa4b55ab57d3681b16a778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f22b1be2f844d88b3feb8655598f282",
            "placeholder": "​",
            "style": "IPY_MODEL_514a1a57239e41d0ae5e85f3dfec87a6",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c41114cae39c4749805eae6732908050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2a9f0f8d72468aaa128ef371f05b55",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f81dea79c2864b81a339f8aad14680e4",
            "value": 25
          }
        },
        "7aaee5b635624a7e8a054348147a4dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f09a2f543a7142d3a69d191c89b1cfa6",
            "placeholder": "​",
            "style": "IPY_MODEL_5cff68ba2354493f8345c7892c578266",
            "value": " 25.0/25.0 [00:00&lt;00:00, 2.27kB/s]"
          }
        },
        "0d82a1c6f1d947cda5fd01856cafbf18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f22b1be2f844d88b3feb8655598f282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "514a1a57239e41d0ae5e85f3dfec87a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2a9f0f8d72468aaa128ef371f05b55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81dea79c2864b81a339f8aad14680e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f09a2f543a7142d3a69d191c89b1cfa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cff68ba2354493f8345c7892c578266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2933be84cbb94033a746d9526d114b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_440964b0315f4403a8a965080db0aeb9",
              "IPY_MODEL_b81d0c2ec9da499c8794a61fb80f79f5",
              "IPY_MODEL_edaf51af71054f92a2b5dd9089e2a6ba"
            ],
            "layout": "IPY_MODEL_3394147e203f4b3897f966b08a07682c"
          }
        },
        "440964b0315f4403a8a965080db0aeb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_586aad82bbe742e5ba988356b95c8334",
            "placeholder": "​",
            "style": "IPY_MODEL_b89abddffe9240c3ba9ea2f3a7c6106e",
            "value": "config.json: 100%"
          }
        },
        "b81d0c2ec9da499c8794a61fb80f79f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f710bfb411c493896fbe7b9904f1813",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29eb193602824589906983b3ea2186cc",
            "value": 1208
          }
        },
        "edaf51af71054f92a2b5dd9089e2a6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9628daf58fc746ada4b11fe5b3e089a4",
            "placeholder": "​",
            "style": "IPY_MODEL_54039a27dcc843f1940a3e19ddb7c951",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 108kB/s]"
          }
        },
        "3394147e203f4b3897f966b08a07682c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "586aad82bbe742e5ba988356b95c8334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89abddffe9240c3ba9ea2f3a7c6106e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f710bfb411c493896fbe7b9904f1813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29eb193602824589906983b3ea2186cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9628daf58fc746ada4b11fe5b3e089a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54039a27dcc843f1940a3e19ddb7c951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60411715624948a69a700f9f2b67ac74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b1709ca0e9f4c2a9489848b3675f7e1",
              "IPY_MODEL_18f70a526fa3459dafd23cb83148afba",
              "IPY_MODEL_289d94a583104b98b2b3c89f8aa7034c"
            ],
            "layout": "IPY_MODEL_72f3c1366ca24756815a77dc095ee56a"
          }
        },
        "2b1709ca0e9f4c2a9489848b3675f7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_062d907970c74093bc988e53bd03485e",
            "placeholder": "​",
            "style": "IPY_MODEL_c63cc6f06f9b469e88f413ba323d1627",
            "value": "spiece.model: 100%"
          }
        },
        "18f70a526fa3459dafd23cb83148afba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a46148d9ff1d45809203955eaa15008f",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f68727d59ff34c28a6c705ebfc50fc41",
            "value": 791656
          }
        },
        "289d94a583104b98b2b3c89f8aa7034c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e3e3832f6a34ccfb0cbac057528c862",
            "placeholder": "​",
            "style": "IPY_MODEL_e38aeaa66b7b406991d7940e5e7e9223",
            "value": " 792k/792k [00:00&lt;00:00, 936kB/s]"
          }
        },
        "72f3c1366ca24756815a77dc095ee56a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "062d907970c74093bc988e53bd03485e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c63cc6f06f9b469e88f413ba323d1627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a46148d9ff1d45809203955eaa15008f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f68727d59ff34c28a6c705ebfc50fc41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e3e3832f6a34ccfb0cbac057528c862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38aeaa66b7b406991d7940e5e7e9223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45eb1dd2f5d44304aa8077a739a4893f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90a85571dd0647af8f74d09cfd836de1",
              "IPY_MODEL_c5a28f7771a344c9bdfbd6b78a96dbbb",
              "IPY_MODEL_aaf9e066b5e7466cb8b688140e35bff0"
            ],
            "layout": "IPY_MODEL_e71bef3141974586a3b7a787db4aeb73"
          }
        },
        "90a85571dd0647af8f74d09cfd836de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a86b2416a0224e80a2ca73307d311c11",
            "placeholder": "​",
            "style": "IPY_MODEL_340198907c4b47fc9cd43359d7827069",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "c5a28f7771a344c9bdfbd6b78a96dbbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcc5b9c09219462dbf8fda0bff3b9599",
            "max": 1786,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba9e0b9affd2444fabe8c6a9b0b3b5d0",
            "value": 1786
          }
        },
        "aaf9e066b5e7466cb8b688140e35bff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6991e5aa1cd420480b5e4d1bd657d51",
            "placeholder": "​",
            "style": "IPY_MODEL_5ffc83bac84a452e8ed0dc0e5dfcc763",
            "value": " 1.79k/1.79k [00:00&lt;00:00, 136kB/s]"
          }
        },
        "e71bef3141974586a3b7a787db4aeb73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a86b2416a0224e80a2ca73307d311c11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "340198907c4b47fc9cd43359d7827069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcc5b9c09219462dbf8fda0bff3b9599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba9e0b9affd2444fabe8c6a9b0b3b5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6991e5aa1cd420480b5e4d1bd657d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ffc83bac84a452e8ed0dc0e5dfcc763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b742378c928c405e823a20078f2337eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4084be2ed06241eda0853e1686d7af18",
              "IPY_MODEL_b42ea3cb39bd4333b3121ae4d1843c2f",
              "IPY_MODEL_a6b569fb3bb84e0fbf79f183583864e3"
            ],
            "layout": "IPY_MODEL_7748bdff37b94b8ab64ecaffd6027c86"
          }
        },
        "4084be2ed06241eda0853e1686d7af18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f655f7337cb401ca9f81f18980e64c6",
            "placeholder": "​",
            "style": "IPY_MODEL_4089f0e8fc0e4af4a5471a49ed06e61c",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "b42ea3cb39bd4333b3121ae4d1843c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_922fd44ed2fc4e56ac99ddc1b8d772f1",
            "max": 891691413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86d2b4c060ce4488821d13a7f30f9c11",
            "value": 891691413
          }
        },
        "a6b569fb3bb84e0fbf79f183583864e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96956ebbbd3d4e20a099e5070e1f1475",
            "placeholder": "​",
            "style": "IPY_MODEL_b02ecb311e594ef18ac5f3df03f586f5",
            "value": " 892M/892M [00:36&lt;00:00, 23.1MB/s]"
          }
        },
        "7748bdff37b94b8ab64ecaffd6027c86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f655f7337cb401ca9f81f18980e64c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4089f0e8fc0e4af4a5471a49ed06e61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "922fd44ed2fc4e56ac99ddc1b8d772f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86d2b4c060ce4488821d13a7f30f9c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96956ebbbd3d4e20a099e5070e1f1475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02ecb311e594ef18ac5f3df03f586f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d205de3f9fa4bf89b60b78de378bc48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0e169d9eeb54c32849ff320f613f62f",
              "IPY_MODEL_8c9be25ff6d54d2fb6babc1b29cf7991",
              "IPY_MODEL_c4feb16cac6a4d46861e6fe5fc9f5b4a"
            ],
            "layout": "IPY_MODEL_4a6e06bd764c447eb2b34358b533aef7"
          }
        },
        "b0e169d9eeb54c32849ff320f613f62f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05eff3d2b46048ef8818aeb1ccd32de2",
            "placeholder": "​",
            "style": "IPY_MODEL_02e92fbb3bd84766b0b2ef60516faed1",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8c9be25ff6d54d2fb6babc1b29cf7991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c22eeacdb6141d0bce256b8893fbc84",
            "max": 801452821,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15751a0d866548fcbde34e149b205370",
            "value": 801452821
          }
        },
        "c4feb16cac6a4d46861e6fe5fc9f5b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90fd85015024457aa98f32b3b7b75b2e",
            "placeholder": "​",
            "style": "IPY_MODEL_2b3e005751404b20818f68d721bfdbd6",
            "value": " 801M/801M [00:36&lt;00:00, 35.9MB/s]"
          }
        },
        "4a6e06bd764c447eb2b34358b533aef7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05eff3d2b46048ef8818aeb1ccd32de2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02e92fbb3bd84766b0b2ef60516faed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c22eeacdb6141d0bce256b8893fbc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15751a0d866548fcbde34e149b205370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90fd85015024457aa98f32b3b7b75b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3e005751404b20818f68d721bfdbd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d586ba33fd7d4d96a1fdfb607acf3203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_091650b47cad473d8e1e59d1279ab9e1",
              "IPY_MODEL_3a01a36f7fac4ce6b74aff3f58bc1cbf",
              "IPY_MODEL_829a8ca70c5b4df9b049d33550a2aae5",
              "IPY_MODEL_105e08b46f7141c7aaf78abac3e8ce48"
            ],
            "layout": "IPY_MODEL_59da7e71ee1148cda8691d3aa2cc8d75"
          }
        },
        "c314d8aa493b4985a39c8f1f23c4974f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392011f6c6d147ce8c3cc2c046b9f792",
            "placeholder": "​",
            "style": "IPY_MODEL_28ff4396a7d845658dcba0298f731a06",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "8026fb3ec04a4af882e214fb59074677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8cbb80f263324328a1c72fa3b7702d9f",
            "placeholder": "​",
            "style": "IPY_MODEL_dbf69d1194284d9fbe4dd6788a22253e",
            "value": ""
          }
        },
        "2908a3d1d1ef4d018fb396f18e62e483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_337b7060be854d24999b81419d1ab716",
            "style": "IPY_MODEL_8cd73d0a97344bc2ae87990887390bda",
            "value": true
          }
        },
        "ca5c0d3febaf40938a2733000b971538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_14ab8a6552624ea6be419d28581ae334",
            "style": "IPY_MODEL_024e27ff65f14f1a87da7c66f8cf0add",
            "tooltip": ""
          }
        },
        "e13c8c64c89e4497be871a151444c64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1deacca8769482fbfc647f2423ad3dc",
            "placeholder": "​",
            "style": "IPY_MODEL_328786752d9e479181eb46a23faf111b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "59da7e71ee1148cda8691d3aa2cc8d75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "392011f6c6d147ce8c3cc2c046b9f792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ff4396a7d845658dcba0298f731a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cbb80f263324328a1c72fa3b7702d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf69d1194284d9fbe4dd6788a22253e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "337b7060be854d24999b81419d1ab716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd73d0a97344bc2ae87990887390bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14ab8a6552624ea6be419d28581ae334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "024e27ff65f14f1a87da7c66f8cf0add": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f1deacca8769482fbfc647f2423ad3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328786752d9e479181eb46a23faf111b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f8dbf8d093142f9a8d96190efdb4010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_860f85caa09f4a3b90fe05e09a1119a7",
            "placeholder": "​",
            "style": "IPY_MODEL_f4bc9785023e49ca8c74d60c8f664709",
            "value": "Connecting..."
          }
        },
        "860f85caa09f4a3b90fe05e09a1119a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4bc9785023e49ca8c74d60c8f664709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "091650b47cad473d8e1e59d1279ab9e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3662fd300354dbcb628285a87cfa267",
            "placeholder": "​",
            "style": "IPY_MODEL_ac68b443515142f598e77024c3659da7",
            "value": "Token is valid (permission: fineGrained)."
          }
        },
        "3a01a36f7fac4ce6b74aff3f58bc1cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46bb7bedc1754799a17e830cbd806363",
            "placeholder": "​",
            "style": "IPY_MODEL_9868d2afbc744636adb2c99b19bf1aa3",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "829a8ca70c5b4df9b049d33550a2aae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_845aec13579f43cf8d1ebdde07d7f7ab",
            "placeholder": "​",
            "style": "IPY_MODEL_aa0953318413431f9dc56c6541c33719",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "105e08b46f7141c7aaf78abac3e8ce48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae400cea311f4fa493fea95eb5fa1550",
            "placeholder": "​",
            "style": "IPY_MODEL_b537e6908218460891073576f6c18edf",
            "value": "Login successful"
          }
        },
        "a3662fd300354dbcb628285a87cfa267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac68b443515142f598e77024c3659da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46bb7bedc1754799a17e830cbd806363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9868d2afbc744636adb2c99b19bf1aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "845aec13579f43cf8d1ebdde07d7f7ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa0953318413431f9dc56c6541c33719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae400cea311f4fa493fea95eb5fa1550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b537e6908218460891073576f6c18edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d6cb2fb82d4faab818d950b421a9d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43b7198cb45a44789dfe30f7b7e683b1",
              "IPY_MODEL_ce9bc74d0af0429d8e5c9d629fd23521",
              "IPY_MODEL_e7c534de8f524a61b9191e7573e87257"
            ],
            "layout": "IPY_MODEL_8c19b9bf928c43fc9085805ac441ba17"
          }
        },
        "43b7198cb45a44789dfe30f7b7e683b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea9ade4ffdb4510854fddc081422891",
            "placeholder": "​",
            "style": "IPY_MODEL_541872cb778047b7ac3bff36f3ebe4cc",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ce9bc74d0af0429d8e5c9d629fd23521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd42c588e1a64f4d9376aba32828accb",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f4663812f4b4634851b131792659285",
            "value": 48
          }
        },
        "e7c534de8f524a61b9191e7573e87257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b2821bb30f3418c96b16774c6cee44e",
            "placeholder": "​",
            "style": "IPY_MODEL_78389a422ba848a0a97f82e82c8a1f39",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.96kB/s]"
          }
        },
        "8c19b9bf928c43fc9085805ac441ba17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea9ade4ffdb4510854fddc081422891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541872cb778047b7ac3bff36f3ebe4cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd42c588e1a64f4d9376aba32828accb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4663812f4b4634851b131792659285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b2821bb30f3418c96b16774c6cee44e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78389a422ba848a0a97f82e82c8a1f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8f55a11feab4ca892a791436e31f708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ab49ce4ddff45a2bcb5274cf49823ee",
              "IPY_MODEL_c2e006c0972044ce8cdc88ac4c8e092b",
              "IPY_MODEL_9752c747587f48e0a1eaed1e1fe83304"
            ],
            "layout": "IPY_MODEL_932cbe9e8bcb436783b15c5d6dea1b12"
          }
        },
        "2ab49ce4ddff45a2bcb5274cf49823ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e226cd2aaf2e40c49ab7f6aa377ac663",
            "placeholder": "​",
            "style": "IPY_MODEL_b278e6f70f4d47319f8b75227b7baf67",
            "value": "vocab.txt: 100%"
          }
        },
        "c2e006c0972044ce8cdc88ac4c8e092b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c255f4335f74ee0b42313b67b582e69",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc109b76f8a449f09614c4b8d0a7e4bd",
            "value": 231508
          }
        },
        "9752c747587f48e0a1eaed1e1fe83304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2850aa864bfe4af3afb68f30a2315eb5",
            "placeholder": "​",
            "style": "IPY_MODEL_19947bd2f6f6478fbfb8b400a9097a0d",
            "value": " 232k/232k [00:00&lt;00:00, 793kB/s]"
          }
        },
        "932cbe9e8bcb436783b15c5d6dea1b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e226cd2aaf2e40c49ab7f6aa377ac663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b278e6f70f4d47319f8b75227b7baf67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c255f4335f74ee0b42313b67b582e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc109b76f8a449f09614c4b8d0a7e4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2850aa864bfe4af3afb68f30a2315eb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19947bd2f6f6478fbfb8b400a9097a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5438282fda8e409c8839d0b6eddc81b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c0818a9f21f46178aef53a816b0f8db",
              "IPY_MODEL_4e1381a9d8d040729f9e2c5e1ab5521e",
              "IPY_MODEL_61588f461684424087386a8e0b08bc20"
            ],
            "layout": "IPY_MODEL_57078bc564b0430f8ed66ea54d05d571"
          }
        },
        "7c0818a9f21f46178aef53a816b0f8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1530e611f8e9464a9637d7d7f4df0e90",
            "placeholder": "​",
            "style": "IPY_MODEL_d2391845787a422986af17f1d3086daf",
            "value": "tokenizer.json: 100%"
          }
        },
        "4e1381a9d8d040729f9e2c5e1ab5521e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c2179ef7eb8440a936c6b2bfaf3dfc3",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0a5087a9451480e99eeb84a381d47a9",
            "value": 466062
          }
        },
        "61588f461684424087386a8e0b08bc20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d731a2ed9d3447538f9badf0a49e62c3",
            "placeholder": "​",
            "style": "IPY_MODEL_04629aaf51f54999b58062c594cffbe5",
            "value": " 466k/466k [00:00&lt;00:00, 1.09MB/s]"
          }
        },
        "57078bc564b0430f8ed66ea54d05d571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1530e611f8e9464a9637d7d7f4df0e90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2391845787a422986af17f1d3086daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c2179ef7eb8440a936c6b2bfaf3dfc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a5087a9451480e99eeb84a381d47a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d731a2ed9d3447538f9badf0a49e62c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04629aaf51f54999b58062c594cffbe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "698f4fa745724835a3faae960fa69316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41032926cbb4477592bafdae8dd76a33",
              "IPY_MODEL_6fee3c758f2e4c869b0356869ab509a0",
              "IPY_MODEL_72505579432b4c708287f1b539045ba6"
            ],
            "layout": "IPY_MODEL_23f53386d7cf47d89d1c6643e594e337"
          }
        },
        "41032926cbb4477592bafdae8dd76a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a75a532fc884c228e9340ca13626015",
            "placeholder": "​",
            "style": "IPY_MODEL_089bd6a8d2334bf49d5502c3f62d4d09",
            "value": "config.json: 100%"
          }
        },
        "6fee3c758f2e4c869b0356869ab509a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90749f43e11e42bda519e5bd4638f62d",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4212ac786854d58b78e45713b8e8690",
            "value": 570
          }
        },
        "72505579432b4c708287f1b539045ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e37f5333e6c4b06b5fd4658d9164776",
            "placeholder": "​",
            "style": "IPY_MODEL_9d52bab1378c4d27a370f65fb2e4f507",
            "value": " 570/570 [00:00&lt;00:00, 46.6kB/s]"
          }
        },
        "23f53386d7cf47d89d1c6643e594e337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a75a532fc884c228e9340ca13626015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "089bd6a8d2334bf49d5502c3f62d4d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90749f43e11e42bda519e5bd4638f62d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4212ac786854d58b78e45713b8e8690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e37f5333e6c4b06b5fd4658d9164776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d52bab1378c4d27a370f65fb2e4f507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c48b136cfc5e435cbcf5a4ace1923d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_631621cb0e484da3b1a2497f23d6737b",
              "IPY_MODEL_412902fd22d443d8a2c7885a119ccef6",
              "IPY_MODEL_2662a691f6644e8f9aa3df9d265ddc4b"
            ],
            "layout": "IPY_MODEL_be06450da0484866afca5e2279a3f547"
          }
        },
        "631621cb0e484da3b1a2497f23d6737b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b61b4e61d3744c9fb1698b92705543b7",
            "placeholder": "​",
            "style": "IPY_MODEL_7993ef40e8454741bbcec1ec1d9901bc",
            "value": "model.safetensors: 100%"
          }
        },
        "412902fd22d443d8a2c7885a119ccef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77e54d38cdfd460bb7a36e8a0d38a337",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57cdb2d05f3b4cf0bb29797b90d8e062",
            "value": 440449768
          }
        },
        "2662a691f6644e8f9aa3df9d265ddc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f46b295cf49454aa9e8cabfd6cf2be2",
            "placeholder": "​",
            "style": "IPY_MODEL_bf8c6361c2964f8e8b59c2524c754e08",
            "value": " 440M/440M [00:01&lt;00:00, 218MB/s]"
          }
        },
        "be06450da0484866afca5e2279a3f547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b61b4e61d3744c9fb1698b92705543b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7993ef40e8454741bbcec1ec1d9901bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77e54d38cdfd460bb7a36e8a0d38a337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57cdb2d05f3b4cf0bb29797b90d8e062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f46b295cf49454aa9e8cabfd6cf2be2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf8c6361c2964f8e8b59c2524c754e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}